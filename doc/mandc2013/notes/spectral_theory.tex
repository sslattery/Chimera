%        File: latexdoc.tex
%     Created: Friday March 4 12:24:15 2011
% Last Change: Friday March 4 12:24:20 2011
%
\documentclass[letterpaper,12pt]{article}
\usepackage[top=1.0in,bottom=1.0in,left=1.25in,right=1.25in]{geometry}
\usepackage{verbatim}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[mathcal]{euscript}
\usepackage{tabularx}
\usepackage{cite}
\usepackage{c++}
\usepackage{tmadd,tmath}
\usepackage[usenames]{color}
\usepackage[
naturalnames = true, 
colorlinks = true, 
linkcolor = black,
anchorcolor = black,
citecolor = black,
menucolor = black,
urlcolor = blue
]{hyperref}
\usepackage{listings}
\usepackage{textcomp}
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\lstset{
  backgroundcolor=\color{lbcolor},
  tabsize=4,
  rulecolor=,
  language=c++,
  basicstyle=\scriptsize,
  upquote=true,
  aboveskip={1.5\baselineskip},
  columns=fixed,
  showstringspaces=false,
  extendedchars=true,
  breaklines=true,
  prebreak =
  \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
  frame=single,
  showtabs=false,
  showspaces=false,
  showstringspaces=false,
  identifierstyle=\ttfamily,
  keywordstyle=\color[rgb]{0,0,1},
  commentstyle=\color[rgb]{0.133,0.545,0.133},
  stringstyle=\color[rgb]{0.627,0.126,0.941},
}

%%---------------------------------------------------------------------------%%
\author{Stuart R. Slattery
  \\ \href{mailto:sslattery@wisc.edu}{\texttt{sslattery@wisc.edu}}
}

\date{\today}
\title{A Spectral Analysis of the Domain Decomposed Adjoint
  Neumann-Ulam Method}
\begin{document}
\maketitle

%%---------------------------------------------------------------------------%%
\begin{abstract}

  The domain decomposed behavior of the adjoint Neumann-Ulam method
  for solving linear operator equations is analyzed using the spectral
  properties of the linear operator. Relationships for the average
  length of the adjoint random walks and the fraction of histories
  leaking from a domain are made with respect to the Eigenvalues of
  the linear operator. In addition, a formal definition of the optical
  thickness of a linear operator is presented based in the spectral
  analysis. The one-speed, two-dimensional neutron diffusion equation
  is used as a model problem to test the spectral theory for symmetric
  operators. In general, the spectral theory shows good agreement with
  measured computational results.

\end{abstract}

%%---------------------------------------------------------------------------%%
\section{Introduction}
For very large linear systems, two things are likely to occur: a
single compute node does not contain enough memory to hold the entire
problem or a single compute node takes a prohibitively long time to
solve the system. Domain decomposition methods permit the global
computational domain to be subdivided into smaller regions. These
smaller regions require less memory to store and fewer computational
resources to solve as compared to the global problem. Typically, these
smaller regions are then represented separately in indiviudal compute
nodes. Therefore, in order to solve large scale problems with the
adjoint Neumann-Ulam method, we must explore its behavior in a domain
decomposed evironment.

For reactor physics Monte Carlo simulations, domain decomposition has
been identified as a key principle in moving forward in high
performance computing
\cite{brunner_comparison_2006,siegel_analysis_2012}. To accomplish
this, we recognize from the literature that stochastic histories must
be transported from domain to domain as the simulation progresses and
they transition to states that are not in the local domain. Because we
have chosen a domain decomposition strategy in a parallel environment,
this means that communication of these histories must occur between
compute nodes owning neighboring pieces of the global domain. We wish
to characterize this communication not only because communication is
in general expensive, but also because nearest-neighbor communication
sequences have poor algorithmic strong scaling
\cite{gropp_high-performance_2001}.  We observe this because nearest
neighbor communication patterns scale with the volume and surface
sizes of the local domains. The larger the volume, the more compute
time is spent within the local volume and therefore at some point you
are limited by the latency of the system, or the time it takes to
receive the data to operate on. However, at a certain point, when the
surface of those volumes becomes large enough, the amount of data that
must be communicated grows large enough that it may overcome the
bandwidth limitations of the system, resulting in a decrease in
performance.

For problems where the linear operator is symmetric, a host of
analytic theories exist based on the eigenvalue spectrum of the
operator that characterize their behavior in the context of
deterministic linear solvers. Using past work, these theories are
adapted to the domain decomposed adjoint Neumann-Ulam method using the
one-speed, two-dimensional neutron diffusion equation. In this paper
we describe the adjoint Neumann-Ulam method followed by a presentation
and finite difference discretization of the model problem. Using the
linear system generated by this discretization, we use a spectral
theory to generate analytic relations for the eigenvalues of the
operator. Using the Eigenvalue spectra, we then build relationships to
characterize the transport of random walk histories in a decomposed
domain and compare these analytic results to numerical experiments
conducted with the model problem.

%%---------------------------------------------------------------------------%%
\section{The Adjoint Neumann-Ulam Method}
We seek solutions of the general linear problem in the following form:
\begin{equation}
  \ve{A} \ve{x} = \ve{b}\:,
  \label{eq:linear_problem}
\end{equation}
where $\ve{A} \in \mathbb{R}^{N \times N}$ is a linear operator such
that $\ve{A} : \mathbb{R}^{N} \rightarrow \mathbb{R}^{N}$, $\ve{x} \in
\mathbb{R}^N$ is the solution vector, and $\ve{b} \in \mathbb{R}^N$ is
the forcing term. Choosing the adjoint method Monte Carlo method to
invert the linear operator, we begin by defining the linear system
adjoint to Eq~(\ref{eq:linear_problem}):
\begin{equation}
  \ve{A}^T \ve{y} = \ve{d}\:,
  \label{eq:adjoint_linear_problem}
\end{equation}
where $\ve{y}$ and $\ve{d}$ are the adjoint solution and source
respectively and $\ve{A}^T$ is the adjoint operator.  With this
statement we can then define the split equation:
\begin{equation}
  \ve{y} = \ve{H}^T \ve{y} + \ve{d}\:,
  \label{eq:adjoint_split_system}
\end{equation}
where $\ve{H}$ is defined as the \textit{iteration matrix}.  Using
these definitions, we can derive an estimator from the adjoint method
that will also give the solution vector, $\ve{x}$. We can acquire the
adjoint solution by forming the Neumann series by writing
Eq~(\ref{eq:adjoint_split_system}) as:
\begin{equation}
  \ve{y} = (\ve{I} - \ve{H}^T)^{-1} \ve{d}\:,
  \label{eq:adjoint_split_system_2}
\end{equation}
which in turn yields the Neumann series using the adjoint operator:
\begin{equation}
  \ve{y} = \sum_{k=0}^{\infty} (\ve{H}^T)^k\ve{d}\:.
  \label{eq:adjoint_neumann_series}
\end{equation}
For this series to converge, we require that the spectral radius of
$\ve{H}$ must remain less than 1 as $\ve{H}^T$ contains the same
eigenvalues and therefore has the same spectral radius. We expand this
summation to again yield a series of transitions that can be
approximated by a Monte Carlo random walk sequence, this time forming
the Neumann series in reverse order:
\begin{equation}
  y_i = \sum_{k=0}^{\infty}\sum_{i_1}^{N}\sum_{i_2}^{N}\ldots
  \sum_{i_k}^{N}h_{i_k,i_{k-1}}\ldots h_{i_2,i_1} h_{i_1,i} d_{i_k}\:.
  \label{eq:adjoint_neumann_solution}
\end{equation}
We can readily build an estimator for the adjoint solution from this
series expansion, but we instead desire the direct solution. The
adjoint estimator can be related to the solution by defining the
following inner product equivalence \cite{spanier_monte_1969}:
\begin{equation}
  \langle \ve{A}^T \ve{x}, \ve{y} \rangle = \langle \ve{x}, \ve{A}
  \ve{y} \rangle\:.
  \label{eq:adjoint_operator_product}
\end{equation}
From this definition it follows that:
\begin{equation}
  \langle \ve{x}, \ve{d} \rangle = \langle \ve{y}, \ve{b} \rangle\:.
  \label{eq:adjoint_vector_relation}
\end{equation}
Here we have 2 unknowns, $\ve{y}$ and $\ve{d}$ and therefore we
require two constraints to close the system. We use
Eq~(\ref{eq:adjoint_vector_relation}) as the first constraint and as
a second constraint we select:
\begin{equation}
  \ve{d} = \boldsymbol{\delta}_i\:,
  \label{eq:adjoint_second_constraint}
\end{equation}
where the $k^{th}$ component of the vector $\boldsymbol{\delta}_i$ is
the Dirac delta function $\delta_{i_k,i}$. If we apply
Eq~(\ref{eq:adjoint_second_constraint}) to our first constraint
Eq~(\ref{eq:adjoint_vector_relation}), we get the following convenient
outcome:
\begin{equation}
  \langle \ve{y}, \ve{b} \rangle = \langle \ve{x},
  \boldsymbol{\delta}_i \rangle = x_i \:,
  \label{eq:inner_product_constraint}
\end{equation}
meaning that if we compute the inner product of the direct source and
the adjoint solution using a delta function source, we recover the
direct solution we are looking for

In terms of particle transport, this adjoint method is equivalent to a
traditional forward method. As a result of using the adjoint system,
we modify our probabilities and weights using the \textit{adjoint
  Neumann-Ulam decomposition} of $\ve{H}$:
\begin{equation}
  \ve{H}^{T} = \ve{P} \circ \ve{W}\:,
  \label{eq:adjoint_neumann_ulam}
\end{equation}
where now we are forming the decomposition with respect to the
transpose of $\ve{H}$. We then follow the same procedure as the direct
method for forming the probability and weight matrices in the
decomposition. Using the adjoint form, probabilities should instead be
column-scaled:
\begin{equation}
  p_{ij} = \frac{|h_{ji}|}{\sum_j |h_{ji}|}\:,
  \label{eq:adjoint_probability}
\end{equation}
such that we expect to select a new state $j$ from the current state
in the random walk $j$ by sampling column-wise. Per
Eq~(\ref{eq:adjoint_neumann_ulam}), the transition weight is then
defined as:
\begin{equation}
  w_{ij} = \frac{h_{ji}}{p_{ij}}\:.
  \label{eq:adjoint_weight}
\end{equation}
Using the decomposition we can then define an expectation value for
the adjoint method. Using our result from
Eq~(\ref{eq:inner_product_constraint}) generated by applying the
adjoint constraints, the contribution to the solution in state $i$
from a particular random walk permutation is then:
\begin{equation}
  X_{\nu} = \sum_{m=0}^k W_{m} \delta_{i,i_m}\:,
  \label{eq:adjoint_permutation_contribution}
\end{equation}
where the Kronecker delta indicates that the tally contributes only in
the current state and $b_{i_0}$ will be the sampled source starting
weight. Note here that the estimator in
Eq~(\ref{eq:adjoint_permutation_contribution}) does not have a
dependency on the source state as in
Eq~(\ref{eq:adjoint_permutation_contribution}), providing a remedy for
the situation in the direct method where we must start a random walk
in each source state for every permutation such that we may compute a
contribution for that state. In the adjoint method, we instead tally
in all states and those of lesser importance will not be visited as
frequently by the random walk. Finally, the expectation value using
all permutations is:
\begin{equation}
  E\{X\} = \sum_{\nu} P_{\nu} X_{\nu}\:
  \label{eq:adjoint_expectation_value}
\end{equation}
which, if expanded in the same way as the direct method, directly
recovers the exact solution:
\begin{equation}
  \begin{split}
    E\{X_j\} &=\sum_{k=0}^{\infty}\sum_{i_1}^{N}\sum_{i_2}^{N}\ldots
    \sum_{i_k}^{N} b_{i_0} h_{i_0,i_1}h_{i_1,i_2}\ldots
    h_{i_{k-1},i_k} \delta_{i_k,j} \\ &= x_{j}\:,
  \end{split}
  \label{eq:adjoint_expectation_expansion}
\end{equation}
therefore also providing an unbiased Monte Carlo estimate of the
solution.

We also desire a criteria for random walk termination for problems
where only an approximate solution is necessary. For the adjoint
method, we utilize a \textit{relative weight cutoff}:
\begin{equation}
  W_f = W_c b_{i_0}\:,
  \label{eq:relative_weight_cutoff}
\end{equation}
where $W_c$ is defined as in the direct method. The adjoint random
walk will then be terminated after $m$ steps if $W_m < W_f$ as tally
contributions become increasingly small.

%%---------------------------------------------------------------------------%%
\section{Model Problem}
For our numerical experiments, we choose the one-speed,
two-dimensional neutron diffusion equation as a model problem
\cite{duderstadt_nuclear_1976}:
\begin{equation}
  -\boldsymbol{\nabla} \cdot D \boldsymbol{\nabla} \phi + \Sigma_a
  \phi = S\:,
  \label{eq:diffusion_eq}
\end{equation}
where $\phi$ is the neutron flux, $\Sigma_a$ is the absorption cross
section, and $S$ is the source of neutrons. In addition, $D$ is the
diffusion coefficient defined as:
\begin{equation}
  D = \frac{1}{3 ( \Sigma_t - \bar{\mu}\Sigma_s )}\:,
  \label{eq:diffusion_coeff}
\end{equation}
where $\Sigma_s$ is the scattering cross section, $\Sigma_t = \Sigma_a
+ \Sigma_s$ is the total cross section, and $\bar{\mu}$ is the cosine
of the average scattering angle. For simplicity, we will take
$\bar{\mu} = 0$ for our analysis giving $D=(3 \Sigma_t)^{-1}$. In
addition, to further simplify we will assume a homogenous domain such
that the cross sections remain constant throughout. Doing this permits
us to rewrite Eq~(\ref{eq:diffusion_eq}) as:
\begin{equation}
  -D \boldsymbol{\nabla}^2 \phi + \Sigma_a \phi = S\:.
  \label{eq:diffusion_eq_simple}
\end{equation}

We choose a finite difference scheme on a square Cartesian grid to
discretize the problem. For the Laplacian, we choose the 9-point
stencil shown in Figure~\ref{fig:stencil} over a grid of size $h$
\cite{leveque_finite_2007}:
\begin{multline}
  \nabla^2_9\phi = \frac{1}{6h^2}[4 \phi_{i-1,j} + 4 \phi_{i+1,j}
    + 4 \phi_{i,j-1} + 4 \phi_{i,j+1} + \phi_{i-1,j-1}\\ +
    \phi_{i-1,j+1} + \phi_{i+1,j-1} + \phi_{i+1,j+1} - 20
    \phi_{i,j}]\:.
  \label{eq:nine_point_stencil}
\end{multline}
\begin{figure}[t!]
  \begin{center}
    \scalebox{1.25}{\input{stencil.pdftex_t}}
  \end{center}
  \caption{\textbf{Nine-point Laplacian stencil.}}
  \label{fig:stencil}
\end{figure}
We then have the following linear system to solve:
\begin{multline}
  -\frac{1}{6h^2}[4 \phi_{i-1,j} + 4 \phi_{i+1,j} + 4
    \phi_{i,j-1} + 4 \phi_{i,j+1} + \phi_{i-1,j-1}\\ + \phi_{i-1,j+1}
    + \phi_{i+1,j-1} + \phi_{i+1,j+1} - 20 \phi_{i,j}] + \Sigma_a
  \phi_{i,j} = s_{i,j}\:,
  \label{eq:fd_system}
\end{multline}
and in operator form:
\begin{equation}
  \ve{D}\boldsymbol{\phi}=\ve{s}\:,
  \label{eq:operator_system}
\end{equation}
where $\ve{D}$ is the diffusion operator, $\ve{s}$ is the source in
vector form and $\boldsymbol{\phi}$ is the vector of unkown fluxes. We
note here that the diffusion operator is symmetric.

To close the system, a set of boundary conditions is required. In the
case of a non-reentrant current condition applied to all global
boundaries of the domain, we choose the formulation of Duderstadt by
assuming the flux is zero at some ghost point beyond the
grid. Consider for example the equations on the $i=0$ boundary of the
domain:
\begin{multline}
  -\frac{1}{6h^2}[4 \phi_{-1,j} + 4 \phi_{1,j} + 4 \phi_{0,j-1} +
    4 \phi_{0,j+1} + \phi_{-1,j-1}\\ + \phi_{-1,j+1} + \phi_{1,j-1} +
    \phi_{1,j+1} - 20 \phi_{0,j}] + \Sigma_a \phi_{0,j} = s_{0,j}\:.
  \label{eq:x_min_bnd}
\end{multline}
Here we note some terms where $i=-1$ and therefore are representative
of grid points beyond the boundary of the domain. We set the flux at
these points to be zero, giving a valid set of equations for the $i=0$
boundary:
\begin{multline}
  -\frac{1}{6h^2}[4 \phi_{1,j} + 4 \phi_{0,j-1} + 4 \phi_{0,j+1}
    \\ + \phi_{-1,j+1} + \phi_{1,j-1} + \phi_{1,j+1} - 20 \phi_{0,j}]
  + \Sigma_a \phi_{0,j} = s_{0,j}\:.
  \label{eq:x_min_bnd_2}
\end{multline}
We repeat this procedure for the other boundaries of the domain.

For reflecting boundary conditions, the net current across a boundary
is zero.

%%---------------------------------------------------------------------------%%
\section{Spectral Analysis}
The convergence of the Neumann series in
Eq~(\ref{eq:adjoint_neumann_series}) approximated by the Monte Carlo
solver is dependent on the Eigenvalues of the iteration matrix. In
addition, the Eigenvalues of the linear operator from which the
iteration matrix was derived allow for further analysis of convergence
properties. We will compute the Eigenvalues of these matrices by
assuming Eigenfunctions of the form \cite{leveque_finite_2007}:
\begin{equation}
  \Phi_{p,q}(x,y) = e^{2 \pi \imath p x} e^{2 \pi \imath q y}\:,
  \label{eq:eigenfunction_form}
\end{equation}
where different combinations of $p$ and $q$ represent the different
Eigenmodes of the solution. As these are valid forms of the solution,
then the action of the linear operator on these Eigenfunctions should
give the Eigenvalues of the matrix as they lie on the unit circle in
the complex plane.

\subsection{Diffusion Operator Spectrum}
For the model problem, we first compute the Eigenvalues for the
diffusion operator $\ve{D}$ by applying the operator to the
Eigenfunctions and noting that $x=ih$ and $y=jh$:
\begin{multline}
  \ve{D}\Phi_{p,q}(x,y) = \lambda_{p,q}(\ve{D}) =\\ -\frac{D}{6h^2}[4
    e^{-2 \pi \imath p h} + 4 e^{2 \pi \imath p h} + 4 e^{-2 \pi
      \imath q h} + 4 e^{2 \pi \imath q h} + e^{-2 \pi \imath p h} e^{-2
      \pi \imath q h} \\ + e^{-2 \pi \imath p h} e^{2 \pi \imath q q} +
    e^{2 \pi \imath p h} e^{-2 \pi \imath q h} + e^{2 \pi \imath p h}
    e^{2 \pi \imath q h} - 20] + \Sigma_a \:.
  \label{eq:deriv_diff_1}
\end{multline}
Using Euler's formula, we can collapse the exponentials to
trigonometric functions:
\begin{equation}
  \lambda_{p,q}(\ve{D}) = -\frac{D}{6h^2}[ 8 \cos(\pi p h) + 8
    \cos(\pi q h) + 4 \cos(\pi p h) \cos(\pi q h) - 20] + \Sigma_a\:.
  \label{eq:deriv_diff_2}
\end{equation}

As Eq~(\ref{eq:diffusion_eq}) is diagonally dominant, Jacobi
preconditioning is sufficient to reduce the spectral radius of the
iteration matrix below unity and therefore ensure convergence of the
Neumann series. The preconditioner in this case is then $\ve{M} =
diag(\ve{D})$ such that we are solving the following linear system:
\begin{equation}
  \ve{M}^{-1} \ve{D} \boldsymbol{\phi} = \ve{M}^{-1} \ve{s}\:.
  \label{eq:precond_diffsion}
\end{equation}
The operator $\ve{M}^{-1} \ve{D}$ is merely the original diffusion
operator with each row scaled by the diagonal component. As we have
defined a homogenous domain, the scaling factor is the same for all
rows in the operator. This scaling factor, $\alpha$, is then defined
as the $\phi_{i,j}$ coefficient from Eq~(\ref{eq:fd_system}):
\begin{equation}
  \alpha = [\frac{10 D}{3 h^2} + \Sigma_a]^{-1}\:.
  \label{eq:jacobi_scaling}
\end{equation}
Using this coefficient, we then have the following spectrum of
preconditioned eigenvalues:
\begin{equation}
  \lambda_{p,q}(\ve{M}^{-1} \ve{D}) = \alpha \lambda_{p,q}(\ve{D})\:.
  \label{eq:preconditioned_eigenvalues}
\end{equation}

A good measure of how well-conditioned is a linear system can be
obtained by computing the condition number for the linear operator
defined as:
\begin{equation}
  \kappa(\ve{A}) = ||\ve{A}||\ ||\ve{A}^{-1}||\:,
  \label{eq:condition_number}
\end{equation}
which can be related to the Eigenvalues of the operator as:
\begin{equation}
  \kappa(\ve{A}) =
  \frac{\max_{p,q}\lambda_{p,q}(\ve{A})}{\min_{p,q}\lambda_{p,q}(\ve{A})}\:.
  \label{eq:condition_number_2}
\end{equation}
To compute the condition number for the preconditioned diffusion
system, we must therefore compute the minimum and maximum Eigenvalues
of the diffusion operator. Per Leveque's text
\cite{leveque_finite_2007}, we expect to find these minimum and
maximum eigenvalues when $p=q$. To find these values,
Eq~(\ref{eq:preconditioned_eigenvalues}) is plotted as a function of
$p$ with $p=q$ in Figure~\ref{fig:diffusion_spectrum} for various
values of $\Sigma_a$.
\begin{figure}[t!]
  \begin{center}
    \includegraphics[width=5in,clip]{diffusion_spectrum.png}
  \end{center}
  \caption{\textbf{Eigenvalue spectra for the diffusion equation.}}
  \label{fig:diffusion_spectrum}
\end{figure}
We see a minimum eigenvalue at $p=0$ and a maximum at $p=10$, letting
us compute the condition number for the Jacobi preconditioned
diffusion operator:
\begin{equation}
  \kappa(\ve{M}^{-1} \ve{D}) =
  \frac{\frac{D}{6h^2}\Sigma_a}{\frac{D}{3h^2}[8 \cos(10 \pi h) + 2
      \cos^2(10 \pi h) - 10] + \Sigma_a}\:.
  \label{eq:diffusion_condition}
\end{equation}
Visually we can also see how increasing the absorption in the problem
creates a more well conditioned system as less diffusion is
expected. If we have a problem with only scattering, $\Sigma_a = 0$
and we observe an infinite condition number as $\lambda_{min}
\rightarrow 0$. We note also that the diffusion operator contains
purely positive Eigenvalues and can therefore be classified as
symmetric positive definite (SPD).

\subsection{Iteration Matrix Spectrum}
The spectral radius of the iteration matrix is obtained by seeking its
largest Eigenvalue. As with the diffusion operator, we can use the
same analysis techniques to find the Eigenvalues for the iteration
matrix. We use a few simplifications by noting that if the Jacobi
preconditioned spectral radius is $\ve{H} = \ve{I} -
\ve{M}^{-1}\ve{D}$, the we expect all terms on the diagonal of the
iteration matrix to be zero such that we have the following stencil:
\begin{equation}
  \ve{H}\boldsymbol{\phi} = -\frac{\alpha D}{6h^2}[4 \phi_{i-1,j} + 4
    \phi_{i+1,j} + 4 \phi_{i,j-1} + 4 \phi_{i,j+1} + \phi_{i-1,j-1}\\ +
    \phi_{i-1,j+1} + \phi_{i+1,j-1} + \phi_{i+1,j+1}]\:.
  \label{eq:iteration_stencil}
\end{equation}
Inserting the Eigenfunctions defined by Eq~\ref{eq:eigenfunction_form}
we get:
\begin{multline}
  \lambda_{p,q}(\ve{H}) = -\frac{\alpha D}{6h^2}[4 e^{-2 \pi \imath p
      h} + 4 e^{2 \pi \imath p h} + 4 e^{-2 \pi \imath q h} + 4 e^{2
      \pi \imath q h} + e^{-2 \pi \imath p h} e^{-2 \pi \imath q h}
    \\ + e^{-2 \pi \imath p h} e^{2 \pi \imath q q} + e^{2 \pi \imath
      p h} e^{-2 \pi \imath q h} + e^{2 \pi \imath p h} e^{2 \pi
      \imath q h}]\:,
  \label{eq:iteration_deriv}
\end{multline}
which simplifies to:
\begin{equation}
  \lambda_{p,q}(\ve{H}) = -\frac{\alpha D}{6h^2}[ 8 \cos(\pi p h) + 8
    \cos(\pi q h) + 4 \cos(\pi p h) \cos(\pi q h)]\:,
  \label{eq:iteration_spectrum}
\end{equation}
giving the Eigenvalue spectrum for the Jacobi preconditioned iteration
matrix. As with the linear operator, we expect the maximum Eigenvalue
to exist when $p=q$. In addition, we note that we have the same cosine
form of the spectrum as for the linear operator expect that the
spectra will be shifted by 10 in the $-p$ direction. We therefore
expect the maximum Eigenvalues to be found when $p=q=0$, giving the
following for the spectral radius of the Jacobi precondition iteration
matrix:
\begin{equation}
  \rho(\ve{H}) = \frac{10 \alpha D}{3 h^2}\:.
  \label{eq:iteration_radius}
\end{equation}

\subsection{Neumann Series Convergence}
The Neumann-Ulam method for matrix inversion is effectively an
approximation to a stationary method (the Jacobi method in the case
of the preconditioning above). Stationary methods for linear systems arise from splitting the
operator in Eq~(\ref{eq:linear_problem}):
\begin{equation}
  \ve{A} = \ve{M} - \ve{N}\:,
  \label{eq:split_linear_operator}
\end{equation}
where the choice of $\ve{M}$ and $\ve{N}$ will be dictated by the
particular method chosen. Using this split definition of the operator
we can then write:
\begin{equation}
  \ve{M}\ve{x} - \ve{N}\ve{x} = \ve{b}\:.
  \label{eq:linear_split_equation1}
\end{equation}
By rearranging, we can generate a form more useful for analysis:
\begin{equation}
  \ve{x} = \ve{H}\ve{x} + \ve{c}\:,
  \label{eq:linear_split_equation2}
\end{equation}
where $\ve{H}=\ve{M}^{-1}\ve{N}$ is defined as the \textit{iteration
  matrix} and $\ve{c}=\ve{M}^{-1}\ve{b}$. With the solution vector on
both the left and right hand sides, an iterative method can then be
formed:
\begin{equation}
  \ve{x}^{k+1} = \ve{H}\ve{x}^k + \ve{c}\:,
  \label{eq:linear_iterative_method}
\end{equation}
with $k \in \mathbb{Z}^+$ defined as the \textit{iteration
  index}. Given this, we can then generate a few statements regarding
the convergence of such stationary methods. Defining $\ve{e}^k =
\ve{u}^k - \ve{u}$ as the solution error at the $k^{th}$ iterate, we
can subtract Eq~(\ref{eq:linear_split_equation2}) from
Eq~(\ref{eq:linear_iterative_method}) to arrive at an error form of
the linear problem:
\begin{equation}
  \ve{e}^{k+1} = \ve{H}\ve{e}^k\:. 
  \label{eq:linear_iterative_error}
\end{equation}
Our error after $k$ iterations is then:
\begin{equation}
  \ve{e}^{k} = \ve{H}^k\ve{e}^0\:. 
  \label{eq:linear_k_iter_error}
\end{equation}
In other words, successive application of the iteration matrix is the
mechanism driving down the error in a stationary method. We can then
place restrictions on the iteration matrix by assuming $\ve{H}$ is
diagonalizable \cite{saad_iterative_2003}, we then have:
\begin{equation}
  \ve{e}^{k} =
  \ve{R}\boldsymbol{\Lambda}^k\ve{R}^{-1}\ve{e}^0\:,
  \label{eq:linear_k_iter_error_diag}
\end{equation}
where $\boldsymbol{\Lambda}$ contains the Eigenvalues of $\ve{H}$ on
its diagonal and the columns of $\ve{R}$ contain the Eigenvectors of
$\ve{H}$. Computing the 2-norm of the above form then gives:
\begin{equation}
  ||\ve{e}^{k}||_2 \leq ||\boldsymbol{\Lambda}^k||_2\ 
  ||\ve{R}||_2\ ||\ve{R}^{-1}||_2\ ||\ve{e}^0||_2\:,
  \label{eq:linear_k_iter_norm1}
\end{equation}
which gives:
\begin{equation}
  ||\ve{e}^{k}||_2 \leq \rho(\ve{H})^k \kappa(\ve{R})
  ||\ve{e}^0||_2\:.
  \label{eq:linear_k_iter_norm2}
\end{equation}
For iteration matrices where the Eigenvectors are orthogonal,
$\kappa(\ve{R})=1$ and the error bound reduces to:
\begin{equation}
  ||\ve{e}^{k}||_2 \leq \rho(\ve{H})^k ||\ve{e}^0||_2\:.
  \label{eq:linear_k_iter_norm3}
\end{equation}
We can now restrict $\ve{H}$ by asserting that $\rho(\ve{H}) < 1$
for a stationary method to converge such that $k$ applications of the
iteration matrix will not cause the error to grow in
Eq~(\ref{eq:linear_k_iter_norm3}). 

In the adjoint Neumann-Ulam method, $k$ iterations, equivalent to $k$
applications of the iteration matrix, are approximated by a
random walk of length $k$ to yield the summation in
Eq~(\ref{eq:adjoint_neumann_solution}). This random walk length, or
the number of transitions before the termination of a history (either
by the weight cutoff, absorption, or exiting the domain) is therefore
bound to the number of stationary iterations required to converge to
the specified tolerance. In the case of the adjoint Neumann-Ulam
method, no such tolerance exists, however, we have specified a weight
cutoff $W_c$ under which low-weight histories will be prematurely
terminated as their contributions are deemed minute. After $k$
iterations, a stationary method is terminated as the error has reached
some fraction, $\epsilon$, of the initial error:
\begin{equation}
  ||\ve{e}^{k}||_2 = \epsilon ||\ve{e}^0||_2\:.
  \label{eq:linear_k_iter_norm4}
\end{equation}
Per Eq~(\ref{eq:linear_k_iter_norm3}), we see that this fraction is
equivalent to $\epsilon = \rho(\ve{H})^k$. For the adjoint Neumann-Ulam
method, if we take this fraction to be the weight cutoff, a measure of
how accurately the contributions of a particular history to the
solution are tallied, we then have the following empirical relation
for $k$:
\begin{equation}
  k = \frac{ \log(W_c) }{ \log( \rho(\ve{H}) ) }\:.
  \label{eq:analytic_k}
\end{equation}
This then gives us a means to estimate the length of the random walks
that will be generated from a particular linear operator based on the
Eigenvalues of its iteration matrix (independent of the linear
operator splitting chosen) and based on the weight cutoff parameter
used in the Neumann-Ulam method

\subsection{Wigner Rational Approximation}
In a domain decomposed situtation, not all histories will remain
within the domain they started in and must instead be
communicated. This communication, expected to be expensive, was
analyzed by Siegel and colleagues for idealized, load balanced
situations for full-core Monte Carlo\cite{siegel_analysis_2012}.  To
quantify the number of particles that leak out of the local domain
they define a leakage fraction, $\lambda$, as:
\begin{equation}
  \lambda = \frac{average\ \#\ of\ particles\ leaving\ local\ domain}
          {total\ of\ \#\ of\ particles\ starting\ in\ local\ domain}\:.
  \label{eq:leakage_fraction}
\end{equation}
For their studies, they assumed that the value of $\lambda$ was
dependent on the total cross section of the system via the Wigner
rational approximation. This approximation was developed by Wigner to
compute the neutron leakage probability from as a simple interpolation
between the small and large fuel lump cases, approximately
characterizing the leakage behavior of many common geometries.
\cite{duderstadt_nuclear_1976}. Outlined more thoroughly by Hwang's
chapter in \cite{azmy_nuclear_2010}, we begin the derivation of
Wigner's approximation by defining $l$ to be the average chord length
across the domain, and $\Sigma_t$ to be the total cross section in the
domain. We then bound the behavior for the leakage fraction in terms
of the optical thickness, $\Sigma_t l$, of the local domain. In the
case of a small domain or a small cross section (and therefore small
probabilty of absorption or scatter), $\Sigma_t l \rightarrow 0,
\lambda \rightarrow 1$. For a large domain or large cross section,
$\Sigma_t l \rightarrow \infty, \lambda \rightarrow (\Sigma_t
l)^{-1}$, where $(\Sigma_t l)^{-1}$ is effectively the number of
interactions that will occur over the length of the coord. Using
Wigner's simple interpolation, we then have a simple formula for the
domain leakage, $\lambda = 1/(1+\Sigma_t l)$. This provides a leakage
estimate, valid over the entire range of $\Sigma_t l$, in terms of the
optical thicknes of the system.

In the case of domain decomposed linear operator equations, the chord
length can be interpreted as the number of discrete states spanning a
domain, $n_i$, along the typical length of that domain (e.g. the
number of discrete states in the $i$ direction in a square domain
using our model diffusion problem). We then seek the equivalent of the
total cross section, $\Sigma_t$ in terms of the discrete random walk
behavior. The total cross section can also be interpreted as the
inverse of the mean free path of particles in the system. In the
Neumann-Ulam method, this is analagous to the length of the Markov
chains (and therefore the number of state-to-state transitions)
generated while sampling the Neumann series. In the previous section,
we determined this length with Eq~(\ref{eq:analytic_k}) to be
dependent on the Eigenvalues of the operator. In addition, we must
also consider the number of states that may be traversed in a given
transition, $n_s$, along the path of the typical length of the
system. For our diffusion model problem, $n_s$ would equate to the
number of states in the $i$ (or $j$ as the problem is symmetric)
direction that a history could potentially move and is dependent on
the stencil used for the discretization. We then have the following
relation for the \textit{optical thickness}, $\tau$, of a linear
operator equation domain:
\begin{equation}
  \tau = \frac{n_i n_s}{k}\:,
  \label{eq:matrix_optical_thickness}
\end{equation}
and inserting Eq~(\ref{eq:analytic_k}) gives:
\begin{equation}
  \tau = \frac{n_i n_s \log(\rho(\ve{H}))}{\log(W_c)}\:.
  \label{eq:matrix_optical_thickness_2}
\end{equation}
Using this optical thickness, we can then complete the Wigner rational
approximation by defining the bounds of $\tau \rightarrow 0, \lambda
\rightarrow 1$ and $\tau \rightarrow \infty, \lambda \rightarrow
\tau^{-1}$. We expect this behavior as optically thin domains will
communicate most of their histories while optically thick domains will
leak the fraction of histories that did not interact within. Using
the Wigner approximation, we can then define the leakage fraction out
of a domain for the adjoint Neumann-Ulam method:
\begin{equation}
  \lambda = \frac{1}{1+\tau}\:.
  \label{eq:analytic_domain_leakage}
\end{equation}
Here the fraction is explictly bound to the Eigenvalues of the
iteration matrix, the size of the domain, the content of the
discretization stencil, and the weight cutoff selected to terminate
low weight histories.  We also note here an interesting consequence of
this approximation: no matter how optically thick a domain may be, the
domain is still expected to leak some histories and thus communicate
them. Therefore, no matter how large the domains are made, if they do
not encapsulate the entire global domain they require some
communication in order to implement a domain decomposed adjoint
Neumann-Ulam method.

%%---------------------------------------------------------------------------%%
\section{Numerical Experiments}

\subsection{Random Walk Length}

\subsection{Domain Leakage}

%%---------------------------------------------------------------------------%%
\section{Conclusion}
These relationships will serve as guidelines for
selecting an appropriate parallel algorithm strategy and provide a
basis for performance models for future parallel implementations of
the adjoint Neumann-Ulam method.

%%---------------------------------------------------------------------------%%
\pagebreak
\bibliographystyle{ieeetr}
\bibliography{references}
\end{document}


