\chapter{The Linear Problem}
\label{ch:linear_problem}

The purpose of this chapter is to provide a full background on solving
linear systems as it relates to this work. It may seem like there is a
lot here, but we really do need it to fully define our new methods and
compare both qualitatively and quantitatively to convential
methods. For example, we need a solid explanation of stationary
methods because MCSA is in fact an acceleration of a stationary method
and therefore shares many properties with them. Subspace methods are
the current class of methods most widely used for sparse systems and
are at the core of the Newton-Krylov methods that we will be comparing
the FANM method with. The most rigorous piece of this chapter should
of course be the stochastic solver definitions, also providing me a
place to include some of my work in this area. Finally,
preconditioning is important for this work, MCSA requires it and
Newton-Krylov methods almost always need some type of
preconditioning. Therefore, we must also discuss these aspects.

\section{Preliminaries}
\label{sec:linear_preliminaries}
We seek solutions of the general linear problem in the following form:
\begin{equation}
  \ve{A} \ve{x} = \ve{b}\:,
  \label{eq:linear_problem}
\end{equation}
where $\ve{A} \in \mathbb{R}^{N \times N}$ is a matrix operator such
that $\ve{A} : \mathbb{R}^{N} \rightarrow \mathbb{R}^{N}$, $\ve{x} \in
\mathbb{R}^N$ is the solution vector, and $\ve{b} \in \mathbb{R}^N$ is
the forcing term. The solutions to Eq~(\ref{eq:linear_problem}) will
be generated by inverting $\ve{A}$ either directly or indirectly:
\begin{equation}
  \ve{x} = \ve{A}^{-1} \ve{b}
  \label{eq:linear_problem_solution}\:.
\end{equation}
In addition we can define the residual:
\begin{equation}
  \ve{r} = \ve{b} - \ve{A}\ve{x}\:,
  \label{eq:linear_residual}
\end{equation}
such that an exact solution to $\ve{x}$ has been found when
$\ve{r}=\ve{0}$.  From the statement in
Eq~(\ref{eq:linear_problem_solution}) we can already place a
restriction on $\ve{A}$ by requiring that it be \textit{nonsingular},
meaning that we can in fact compute $\ve{A}^{-1}$. In this work we
will focus our efforts on approximately inverting the operator through
various means.

In a discussion of methods for solving linear systems, several
mathematical tools are useful in characterizing the qualities of the
linear system. Among the most useful are the \textit{Eigenvalues} of
the matrix, $\sigma(\ve{A})$. We find these by solving the Eigenvalue
problem:
\begin{equation}
  \ve{A} \ve{x} = \lambda \ve{x},\ \lambda \in \sigma(\ve{A})\:.
  \label{eq:eigenvalue_problem}
\end{equation}
By writing Eq~(\ref{eq:eigenvalue_problem}) in a different form,
\begin{equation}
  (\ve{A} - \lambda \ve{I})\ve{x} = 0 \:,
  \label{eq:eigenvalue_problem_2}
\end{equation}
and demanding that non-trivial solutions for $\ve{x}$ exist, it is
then required that $|\ve{A} - \lambda \ve{I}| = 0$. Expanding this
determinant yields a characteristic polynomial in terms of $\lambda$
with roots that form the set of Eigenvalues, $\sigma(\ve{A})$. Each
component of $\sigma(\ve{A})$ can then be used to solve
Eq~(\ref{eq:eigenvalue_problem_2}) for a particular permutation of
$\ve{x}$. The set of all permutations for the \textit{Eigenvectors} of
$\ve{A}$. A quantity of particular interest that is computatable from
the eigenvalues of a matrix is the \textit{spectral radius},
$\rho(\ve{A})$ defined by Saad \citep{saad_2003} as:
\begin{equation}
  \rho(\ve{A}) = \max_{\lambda \in \sigma(\ve{A})} |\lambda| \:.
  \label{eq:spectral_radius}
\end{equation}
In addition, for problems that have a large scale over which the
independent variables may exist (e.g. a problem with events on
timescales ranging from nanoseconds to hours), a good measure of this
range is supplied by the \textit{stiffness ratio}:
\begin{equation}
  Stiffness Ratio = \frac{\max_{\lambda \in \sigma(\ve{A})}
    |\lambda|}{\min_{\lambda \in \sigma(\ve{A})} |\lambda|}
\end{equation}
Those problems that have a wide range of scales in their independent
variables, which will then be reflected in the operator, will then
have a large stiffness ratio. We will define such problems as
\textit{stiff}.

General to both matrices and vectors, \textit{norms} are a mechanism
for collapsing objects of many elements to a single value. Per
LeVeque's text \citep{leveque_2007}, the q-norm of a vector is defined
as:
\begin{equation}
  ||\ve{v}||_q = \Bigg[ \sum_{i=1}^N |v_i|^q \Bigg]^{1/q},\ \ve{v} \in
  \mathbb{R}^N\:,\ q \in \mathbb{Z}^+
  \label{eq:q_norm}
\end{equation}
where ${v_i}$ is the $i^{th}$ component of the vector. Depending on
the value chosen for $q$, local or global qualities of the vector may
be obtained. For example, $q=2$ provides the root of a quadrature sum
of all elements in the vector giving a global measure of the vector
while $q=\infty$ gives the maximum value in the vector, a local
quantity that does not give information regarding the other elements
in the vector.

We can also compute the norm of a matrix by inferring from the norm of
the vector on which it is operating. Per LeVeque, we search for a
constant that is equivalent to $||\ve{A}||$:
\begin{equation}
  ||\ve{A}\ve{x}|| \leq C ||\ve{x}||\:,
  \label{eq:matrix_norm_inequality}
\end{equation}
where the minimum value of $C$ that satisfies
Eq~(\ref{eq:matrix_norm_inequality}) is equivalent to $||\ve{A}||$
that is valid $\forall \ve{x} \in \mathbb{R}^N$. The general
definition in Eq~(\ref{eq:matrix_norm_inequality}) can be expanded in
simple terms for common norms including the infinity norm:
\begin{equation}
  ||\ve{A}||_{\infty} = \max_{1 \leq i \leq N} \sum^N_{j=1}|a_{ij}|\:,
  \label{eq:matrix_infinity_norm}
\end{equation}
and the 2-norm:
\begin{equation}
  ||\ve{A}||_{2} = \sqrt{\rho(\ve{A}^T\ve{A})}\:,
  \label{eq:matrix_2_norm}
\end{equation}
where $\rho$ is the spectral radius as defined in
Eq~(\ref{eq:spectral_radius}).

Knowing this, we can then define several useful properties of matrices
including the \textit{condition number} \citep{saad_2003}:
\begin{equation}
  \kappa(\ve{A}) = ||\ve{A}||\ ||\ve{A}^{-1}||\:,
  \label{eq:condition_number}
\end{equation}
which gives as a metric on assessing how close to singular the system
is. This is due to the fact $||\ve{A}^{-1}||$ is large near
singularities (and undefined for a singular matrix) and thus a large
condition number will be generated. We define such matrices as
\textit{ill-conditioned}. 

\section{Iterative Methods for Solving Sparse Linear Systems}
\label{sec:linear_methods}
The discretization of partial differential equations (\textit{PDEs})
through common methods such as finite differences
\citep{leveque_2007}, finite volumes \citep{leveque_2002}, and finite
elements \citep{zienkiewicz_1977} ultimately generate matrix problems
as in Eq~(\ref{eq:linear_problem}).  In addition, these matrices are
typically sparse, meaning that the vast majority of their constituent
elements are zero. This sparsity is due to the fact that the influence
of a particular grid element only expands as far as a few of its
nearest neighbors depending on the order of discretization used, and
therefore coupling among variables in a particular discrete equation
in the system leads to a few non-zero entries. Because of the natural
occurrence of sparse matrices in common numerical methods many
iterative techniques have been developed to solve such systems. We
discuss here conventional stationary and subspace methods to provide
background and then give detail on a stochastic class of methods.

\subsection{Stationary Methods}
\label{subsec:stationary_methods}
Stationary methods arise from splitting the operator in
Eq~(\ref{eq:linear_problem})
\begin{equation}
  \ve{A} = \ve{M} - \ve{N}\:,
  \label{eq:split_linear_operator}
\end{equation}
where the choice of $\ve{M}$ and $\ve{N}$ will be dictated by the
particular method chosen. Using this split definition of the operator
we can then write:
\begin{equation}
  \ve{M}\ve{x} + \ve{N}\ve{x} = \ve{b}\:.
  \label{eq:linear_split_equation1}
\end{equation}
By rearranging, we can generate a form more useful for analysis:
\begin{equation}
  \ve{x} = \ve{H}\ve{x} + \ve{c}\:,
  \label{eq:linear_split_equation2}
\end{equation}
where $\ve{H}=\ve{M}^{-1}\ve{N}$ is defined as the \textit{iteration
  matrix} and $\ve{c}=\ve{M}^{-1}\ve{b}$. With the solution vector on
both the left and right hand sides, an iterative method can then be
formed:
\begin{equation}
    \ve{x}^{k+1} = \ve{H}\ve{x}^k + \ve{c}\:,
  \label{eq:linear_iterative_method}
\end{equation}
with $k \in \mathbb{Z}^+$ defined as the \textit{iteration index}. In
general, we will define methods in the form of
Eq~(\ref{eq:linear_iterative_method}) as \textit{stationary
  methods}. Given this, we can then generate a few statements
regarding the convergence of such stationary methods. Defining
$\ve{e}^k = \ve{u}^k - \ve{u}$ as the solution error at the
$k^{th}$ iterate, we can subtract Eq~(\ref{eq:linear_split_equation2})
from Eq~(\ref{eq:linear_iterative_method}) to arrive at an error form
of the linear problem:
\begin{equation}
  \ve{e}^{k+1} = \ve{H}\ve{e}^k\:. 
  \label{eq:linear_iterative_error}
\end{equation}
Our error after $k$ iterations is then:
\begin{equation}
  \ve{e}^{k} = \ve{H}^k\ve{e}^0\:. 
  \label{eq:linear_k_iter_error}
\end{equation}
In other words, successive application of the iteration matrix is the
mechanism driving down the error in a stationary method. We can then
place restrictions on the iteration matrix by using the tools
developed in \S~(\ref{sec:linear_preliminaries}). By assuming $\ve{H}$
is diagonalizable (although we may generalize this with the Jordan
canonical form of $H$ \citep{saad_2003}), we then have:
\begin{equation}
  \ve{e}^{k} =
  \ve{R}\boldsymbol{\Lambda}^k\ve{R}^{-1}\ve{e}^0\:,
  \label{eq:linear_k_iter_error_diag}
\end{equation}
where $\boldsymbol{\Lambda}$ contains the Eigenvalues of $\ve{H}$ on
its diagonal and the columns of $\ve{R}$ contain the Eigenvectors of
$\ve{H}$. Computing the 2-norm of the above form then gives:
\begin{equation}
  ||\ve{e}^{k}||_2 \leq ||\boldsymbol{\Lambda}^k||_2\ 
  ||\ve{R}||_2\ ||\ve{R}^{-1}||_2\ ||\ve{e}^0||_2\:,
  \label{eq:linear_k_iter_norm1}
\end{equation}
which gives:
\begin{equation}
  ||\ve{e}^{k}||_2 \leq \rho(\ve{H})^k \kappa(\ve{R})
  ||\ve{e}^0||_2\:.
  \label{eq:linear_k_iter_norm2}
\end{equation}
For iteration matrices where the Eigenvectors are orthogonal,
$\kappa(\ve{R})=0$ and the error bound reduces to:
\begin{equation}
  ||\ve{e}^{k}||_2 \leq \rho(\ve{H})^k
  ||\ve{e}^0||_2\:.
  \label{eq:linear_k_iter_norm3}
\end{equation}
We can now resrtict $\ve{H}$ by asserting that $\rho(\ve{H}) < 1$
for a stationary method to converge such that $k$ applications of the
iteration matrix will not cause the error to grow in
Eq~(\ref{eq:linear_k_iter_norm3}). 

\subsection{Projection Methods}
\label{subsec:projection_methods}
The most common iterative methods used in scientific computing today
for sparse systems are of a broad class known as \textit{projection
  methods}. These methods not only provide access to more powerful
means of reaching a solution, but also a powerful means of
encapsulating the majority of common iterative methods including the
stationary methods just discussed. All projection methods are built
around a core framework where the solution to
Eq~(\ref{eq:linear_problem}) is extracted from a \textit{search
  subspace} $\mathcal{K}$ and bound by the constraint subspace
$\mathcal{L}$ that will vary in definition depending on the iterative
method selected. We build the approximate solution $\tilde{\ve{x}}$ by
starting with an initial guess $\ve{x}_0$ and extracting a correction
$\boldsymbol{\delta}$ from $\mathcal{K}$ such that:
\begin{equation}
  \tilde{\ve{x}} = \ve{x}_0 +
  \boldsymbol{\delta},\ \boldsymbol{\delta} \in \mathcal{K}\:.
  \label{eq:linear_projection_step}
\end{equation}
We bound this correction by asserting that the new residual,
$\tilde{\ve{r}}$, be orthogonal to $\mathcal{L}$:
\begin{equation}
  \langle \tilde{\ve{r}},\ve{w} \rangle = 0,\ \forall w \in
  \mathcal{L}\:.
  \label{eq:linear_projection_constraint}
\end{equation}
We can generate a more physical and geometric-based understanding of
these constraints by writing the new residual as $\tilde{\ve{r}} =
\ve{r}_0 - \ve{A}\boldsymbol{\delta}$ and again asserting the residual
must be orthogonal to $\mathcal{L}$. This situation is geometrically
presented in Figure~\ref{fig:projection_constraint}.
\begin{figure}[htpb!]
  \begin{center}
    \scalebox{1.75}{
      \input{chapters/linear_problem/orthogonal_residual.pdftex_t} }
  \end{center}
  \caption{ Orthogonality of the correction residual with respect to
    $\mathcal{L}$.}
  \label{fig:projection_constraint}
\end{figure}


\subsubsection{The Krylov Subspace}
\label{subsubsec:krylov_subspace}

\subsubsection{GMRES}
\label{subsubsec:gmres}

\subsection{Stochastic Methods}
\label{subsec:stochastic_methods}

\subsubsection{Direct Method}
\label{subsubsec:direct_mc}

\subsubsection{Adjoint Method}
\label{subsubsec:adjoint_mc}

\subsubsection{Sequential Monte Carlo}
\label{subsusbsec:sequential_mc}

\subsection{Monte Carlo Synthetic-Acceleration}
\label{subsec:mcsa}

\subsubsection{A Stationary Method Perspective}
\label{subsubsec:mcsa_stationary}

\subsubsection{A Projection Method Perspective}
\label{subsubsec:mcsa_projection}

\section{Preconditioning the Linear System}
\label{sec:linear_preconditioning}

