\chapter{The Linear Problem}
\label{ch:linear_problem}

The purpose of this chapter is to provide a full background on solving
linear systems as it relates to this work. It may seem like there is a
lot here, but we really do need it to fully define our new methods and
compare both qualitatively and quantitatively to convential
methods. For example, we need a solid explanation of stationary
methods because MCSA is in fact an acceleration of a stationary method
and therefore shares many properties with them. Subspace methods are
the current class of methods most widely used for sparse systems and
are at the core of the Newton-Krylov methods that we will be comparing
the FANM method with. The most rigorous piece of this chapter should
of course be the stochastic solver definitions, also providing me a
place to include some of my work in this area. Finally,
preconditioning is important for this work, MCSA requires it and
Newton-Krylov methods almost always need some type of
preconditioning. Therefore, we must also discuss these aspects.

\section{Preliminaries}
\label{sec:linear_system}

We seek solutions of the general linear problem in the following form:
\begin{equation}
  \ve{A} \ve{x} = \ve{b}\:,
  \label{eq:linear_problem}
\end{equation}
where $\ve{A} \in \mathbb{R}^{N \times N}$ is a matrix operator such
that $\ve{A} : \mathbb{R}^{N} \rightarrow \mathbb{R}^{N}$, $\ve{x} \in
\mathbb{R}^N$ is the solution vector, and $\ve{b} \in \mathbb{R}^N$ is
the forcing term. The solutions to Eq~(\ref{eq:linear_problem}) will
be generated by inverting $\ve{A}$ either directly or indirectly:
\begin{equation}
  \ve{x} = \ve{A}^{-1} \ve{b}
  \label{eq:linear_problem_solution}\:.
\end{equation}
From the statement in Eq~(\ref{eq:linear_problem_solution}) we can
already place a restriction on $\ve{A}$ by requiring that it be
\textit{nonsingular}, meaning that we can in fact compute
$\ve{A}^{-1}$. In this work we will focus our efforts on approximately
inverting the operator through various means.

In a discussion of methods for solving linear systems, several
mathematical tools are useful in characterizing the qualities of the
linear system. Among the most useful are the \textit{Eigenvalues} of
the matrix, $\sigma(\ve{A})$. We find these by solving the Eigenvalue
problem:
\begin{equation}
  \ve{A} \ve{x} = \lambda \ve{x},\ \lambda \in \sigma(\ve{A})\:.
  \label{eq:eigenvalue_problem}
\end{equation}
By writing Eq~(\ref{eq:eigenvalue_problem}) in a different form,
\begin{equation}
  (\ve{A} - \lambda \ve{I})\ve{x} = 0 \:,
  \label{eq:eigenvalue_problem_2}
\end{equation}
and demanding that non-trivial solutions for $\ve{x}$ exist, it is
then required that $|\ve{A} - \lambda \ve{I}| = 0$. Expanding this
determinant yields a characteristic polynomial in terms of $\lambda$
with roots that form the set of Eigenvalues, $\sigma(\ve{A})$. A
quantity of particular interest that is computatable from the
eigenvalues of a matrix is the \textit{spectral radius},
$\rho(\ve{A})$ defined by Saad \citep{saad_2003} as:
\begin{equation}
  \rho(\ve{A}) = \max_{\lambda \in \sigma(\ve{A})} |\lambda| \:.
  \label{eq:spectral_radius}
\end{equation}
In addition, for problems that have a large scale over which the
independent variables may exist (e.g. a problem with events on
timescales ranging from nanoseconds to hours), a good measure of this
range is supplied by the \textit{stiffness ratio}:
\begin{equation}
  Stiffness Ratio = \frac{\max_{\lambda \in \sigma(\ve{A})}
    |\lambda|}{\min_{\lambda \in \sigma(\ve{A})} |\lambda|}
\end{equation}
Those problems that have a wide range of scales in their independent
variables, which will then be reflected in the operator, will then
have a large stiffness ratio. We will define such problems as
\textit{stiff}.

General to both matrices and vectors, \textit{norms} are a mechanism
for collapsing objects of many elements to a single value. Per
LeVeque's text \citep{leveque_2007}, the q-norm of a vector is defined
as:
\begin{equation}
  ||\ve{v}||_q = \Bigg[ \sum_{i=1}^N |v_i|^q \Bigg]^{1/q},\ \ve{v} \in
  \mathbb{R}^N\:,\ q \in \mathbb{Z}^+
  \label{eq:q_norm}
\end{equation}
where ${v_i}$ is the $i^{th}$ component of the vector. Depending on
the value chosen for $q$, local or global qualities of the vector may
be obtained. For example, $q=2$ provides the root of a quadrature sum
of all elements in the vector giving a global measure of the vector
while $q=\infty$ gives the maximum value in the vector, a local
quantity that does not give information regarding the other elements
in the vector.

We can also compute the norm of a matrix by inferring from the norm of
the vector on which it is operating. Per LeVeque, we search for a
constant that is equivalent to $||\ve{A}||$:
\begin{equation}
  ||\ve{A}\ve{x}|| \leq C ||\ve{x}||\:,
  \label{eq:matrix_norm_inequality}
\end{equation}
where the minimum value of $C$ that satisfies
Eq~(\ref{eq:matrix_norm_inequality}) is equivalent to $||\ve{A}||$
that is valid $\forall \ve{x} \in \mathbb{R}^N$. The general
definition in Eq~(\ref{eq:matrix_norm_inequality}) can be expanded in
simple terms for common norms including the infinity norm:
\begin{equation}
  ||\ve{A}||_{\infty} = \max_{1 \leq i \leq N} \sum^N_{j=1}|a_{ij}|\:,
  \label{eq:matrix_infinity_norm}
\end{equation}
and the 2-norm:
\begin{equation}
  ||\ve{A}||_{2} = \sqrt{\rho(\ve{A}^T\ve{A})}\:,
  \label{eq:matrix_2_norm}
\end{equation}
where $\rho$ is the spectral radius as defined in
Eq~(\ref{eq:spectral_radius}).

Knowing this, we can then define several useful properties of matrices
including the \textit{condition number} \citep{saad_2003}:
\begin{equation}
  \kappa(\ve{A}) = ||\ve{A}||\ ||\ve{A}^{-1}||\:,
  \label{eq:condition_number}
\end{equation}
which gives as a metric on assessing how close to singular the system
is. This is due to the fact $||\ve{A}^{-1}||$ is large near
singularities (and undefined for a singular matrix) and thus a large
condition number will be generated. We define such matrices as
\textit{ill-conditioned}. 

\section{Iterative Methods for Solving Sparse Linear Systems}
\label{sec:linear_methods}
The discretization of partial differential equations (\textit{PDEs})
through common methods such as finite differences
\citep{leveque_2007}, finite volumes \citep{leveque_2002}, and finite
elements \citep{zienkiewicz_1977} ultimately generate matrix problems
as in Eq~(\ref{eq:linear_problem}).In addition, these matrices are
typically sparse, meaning that the vast majority of their constituent
elements are zero. This sparsity is due to the fact that the influence
of a particular grid element only expands as far as a few of its
nearest neighbors depending on the order of discretization used, and
therefore coupling among variables in a particular discrete equation
in the system leads to a few non-zero entries.

\subsection{Stationary Methods}
\label{subsec:stationary_methods}

\subsection{Subspace Methods}
\label{subsec:subspace_methods}

\subsubsection{Conjugate Gradient}
\label{subsubsec:conjugate_gradient}

\subsubsection{GMRES}
\label{subsubsec:gmres}

\subsection{Stochastic Methods}
\label{subsec:stochastic_methods}

\subsubsection{Direct Method}
\label{subsubsec:direct_mc}

\subsubsection{Adjoint Method}
\label{subsubsec:adjoint_mc}

\subsubsection{Sequential Monte Carlo}
\label{subsusbsec:sequential_mc}

\subsection{Monte Carlo Synthetic-Acceleration}
\label{subsec:mcsa}

\section{Preconditioning the Linear System}
\label{sec:linear_preconditioning}

