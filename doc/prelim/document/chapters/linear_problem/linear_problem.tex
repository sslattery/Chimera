\chapter{Conventional Solution Methods for Linear Problems}
\label{ch:linear_problem}
The discretization of partial differential equations (\textit{PDEs})
through common methods such as finite differences
\citep{leveque_finite_2007}, finite volumes
\citep{leveque_finite_2002}, and finite elements
\citep{zienkiewicz_finite_2005} ultimately generates sets of coupled
equations in the form of matrix problems.  In addition, these matrices
are typically sparse, meaning that the vast majority of their
constituent elements are zero. This sparsity is due to the fact that
the influence of a particular grid element only expands as far as a
few of its nearest neighbors depending on the order of discretization
used and therefore coupling among variables in a particular discrete
equation in the system leads to a few non-zero entries. Because of the
natural occurrence of sparse matrices in common numerical methods many
iterative techniques have been developed to solve such systems. We
discuss here conventional stationary and projection methods for
solving sparse systems to provide the necessary background for the
remainder of this work. Details on the parallelization of conventional
methods are discussed.\footnote{The contents of this chapter,
  particularly those sections relating to projection methods and
  matrix analysis, are heavily based on Saad's text
  \citep{saad_iterative_2003}.}

\section{Preliminaries}
\label{sec:linear_preliminaries}
We seek solutions of the general linear problem in the following form:
\begin{equation}
  \ve{A} \ve{x} = \ve{b}\:,
  \label{eq:linear_problem}
\end{equation}
where $\ve{A} \in \mathbb{R}^{N \times N}$ is a matrix operator such
that $\ve{A} : \mathbb{R}^{N} \rightarrow \mathbb{R}^{N}$, $\ve{x} \in
\mathbb{R}^N$ is the solution vector, and $\ve{b} \in \mathbb{R}^N$ is
the forcing term. The solutions to Eq~(\ref{eq:linear_problem}) will
be generated by inverting $\ve{A}$ either directly or indirectly:
\begin{equation}
  \ve{x} = \ve{A}^{-1} \ve{b}
  \label{eq:linear_problem_solution}\:.
\end{equation}
In addition we can define the residual:
\begin{equation}
  \ve{r} = \ve{b} - \ve{A}\ve{x}\:,
  \label{eq:linear_residual}
\end{equation}
such that an exact solution $\ve{x}$ has been found when
$\ve{r}=\ve{0}$.  From the statement in
Eq~(\ref{eq:linear_problem_solution}) we can already place a
restriction on $\ve{A}$ by requiring that it be \textit{nonsingular},
meaning that we can in fact compute $\ve{A}^{-1}$. In this work we
will focus our efforts on approximately inverting the operator through
various means.

In a discussion of methods for solving linear systems, several
mathematical tools are useful in characterizing the qualities of the
linear system. Among the most useful are the \textit{Eigenvalues} of
the matrix, $\sigma(\ve{A})$. We find these by solving the Eigenvalue
problem:
\begin{equation}
  \ve{A} \ve{x} = \lambda \ve{x},\ \lambda \in \sigma(\ve{A})\:.
  \label{eq:eigenvalue_problem}
\end{equation}
By writing Eq~(\ref{eq:eigenvalue_problem}) in a different form,
\begin{equation}
  (\ve{A} - \lambda \ve{I})\ve{x} = 0 \:,
  \label{eq:eigenvalue_problem_2}
\end{equation}
and demanding that non-trivial solutions for $\ve{x}$ exist, it is
then required that $|\ve{A} - \lambda \ve{I}| = 0$. Expanding this
determinant yields a characteristic polynomial in terms of $\lambda$
with roots that form the set of Eigenvalues, $\sigma(\ve{A})$. Each
component of $\sigma(\ve{A})$ can then be used to solve
Eq~(\ref{eq:eigenvalue_problem_2}) for a particular permutation of
$\ve{x}$. The set of all permutations form the \textit{Eigenvectors}
of $\ve{A}$. A quantity of particular interest that is computatable
from the eigenvalues of a matrix $\ve{A}$ is the \textit{spectral
  radius}, $\rho(\ve{A})$, defined by Saad \citep{saad_iterative_2003}
as:
\begin{equation}
  \rho(\ve{A}) = \max_{\lambda \in \sigma(\ve{A})} |\lambda| \:.
  \label{eq:spectral_radius}
\end{equation}
In addition, for problems that have a large scale over which the
independent variables may exist (e.g. a problem with events on
timescales ranging from nanoseconds to hours), a good measure of this
range is supplied by the \textit{stiffness ratio}:
\begin{equation}
  Stiffness Ratio = \frac{\max_{\lambda \in \sigma(\ve{A})}
    |\lambda|}{\min_{\lambda \in \sigma(\ve{A})} |\lambda|}
\end{equation}
Those problems that have a wide range of scales in their independent
variables, which will then be reflected in the operator, will then
have a large stiffness ratio. We will define such problems with large
stiffness ratios as \textit{stiff}.

General to both matrices and vectors, \textit{norms} are a mechanism
for collapsing objects of many elements to a single value. Per
LeVeque's text \citep{leveque_finite_2007}, the q-norm of a vector is defined
as:
\begin{equation}
  ||\ve{v}||_q = \Bigg[ \sum_{i=1}^N |v_i|^q \Bigg]^{1/q},\ \ve{v} \in
  \mathbb{R}^N\:,\ q \in \mathbb{Z}^+
  \label{eq:q_norm}
\end{equation}
where ${v_i}$ is the $i^{th}$ component of the vector. Depending on
the value chosen for $q$, local or global qualities of the vector may
be obtained. For example, $q=2$ provides the root of a quadrature sum
of all elements in the vector giving a global measure of the vector
while $q=\infty$ gives the maximum value in the vector, a local
quantity that does not give information regarding the other elements
in the vector.

We can also compute the norm of a matrix by inferring from the norm of
the vector on which it is operating. Per LeVeque, we search for a
constant that is equivalent to $||\ve{A}||$:
\begin{equation}
  ||\ve{A}\ve{x}|| \leq C ||\ve{x}||\:,
  \label{eq:matrix_norm_inequality}
\end{equation}
where the minimum value of $C$ that satisfies
Eq~(\ref{eq:matrix_norm_inequality}) is equivalent to $||\ve{A}||$
and is valid $\forall \ve{x} \in \mathbb{R}^N$. The general
definition in Eq~(\ref{eq:matrix_norm_inequality}) can be expanded in
simple terms for common norms including the infinity norm:
\begin{equation}
  ||\ve{A}||_{\infty} = \max_{1 \leq i \leq N} \sum^N_{j=1}|a_{ij}|\:,
  \label{eq:matrix_infinity_norm}
\end{equation}
and the 2-norm:
\begin{equation}
p  ||\ve{A}||_{2} = \sqrt{\rho(\ve{A}^T\ve{A})}\:,
  \label{eq:matrix_2_norm}
\end{equation}
where $\rho$ is the spectral radius as defined in
Eq~(\ref{eq:spectral_radius}).

Knowing this, we can then define several useful properties of matrices
including the \textit{condition number}:
\begin{equation}
  \kappa(\ve{A}) = ||\ve{A}||\ ||\ve{A}^{-1}||\:,
  \label{eq:condition_number}
\end{equation}
which gives as a metric on assessing how close to singular the system
is. This is due to the fact $||\ve{A}^{-1}||$ is large near
singularities (and undefined for a singular matrix) and thus a large
condition number will be generated. We define such matrices as
\textit{ill-conditioned}. 

\section{Stationary Methods}
\label{sec:stationary_methods}
Stationary methods arise from splitting the operator in
Eq~(\ref{eq:linear_problem})
\begin{equation}
  \ve{A} = \ve{M} - \ve{N}\:,
  \label{eq:split_linear_operator}
\end{equation}
where the choice of $\ve{M}$ and $\ve{N}$ will be dictated by the
particular method chosen. Using this split definition of the operator
we can then write:
\begin{equation}
  \ve{M}\ve{x} + \ve{N}\ve{x} = \ve{b}\:.
  \label{eq:linear_split_equation1}
\end{equation}
By rearranging, we can generate a form more useful for analysis:
\begin{equation}
  \ve{x} = \ve{H}\ve{x} + \ve{c}\:,
  \label{eq:linear_split_equation2}
\end{equation}
where $\ve{H}=\ve{M}^{-1}\ve{N}$ is defined as the \textit{iteration
  matrix} and $\ve{c}=\ve{M}^{-1}\ve{b}$. With the solution vector on
both the left and right hand sides, an iterative method can then be
formed:
\begin{equation}
    \ve{x}^{k+1} = \ve{H}\ve{x}^k + \ve{c}\:,
  \label{eq:linear_iterative_method}
\end{equation}
with $k \in \mathbb{Z}^+$ defined as the \textit{iteration index}. In
general, we will define methods in the form of
Eq~(\ref{eq:linear_iterative_method}) as \textit{stationary
  methods}. Given this, we can then generate a few statements
regarding the convergence of such stationary methods. Defining
$\ve{e}^k = \ve{u}^k - \ve{u}$ as the solution error at the
$k^{th}$ iterate, we can subtract Eq~(\ref{eq:linear_split_equation2})
from Eq~(\ref{eq:linear_iterative_method}) to arrive at an error form
of the linear problem:
\begin{equation}
  \ve{e}^{k+1} = \ve{H}\ve{e}^k\:. 
  \label{eq:linear_iterative_error}
\end{equation}
Our error after $k$ iterations is then:
\begin{equation}
  \ve{e}^{k} = \ve{H}^k\ve{e}^0\:. 
  \label{eq:linear_k_iter_error}
\end{equation}
In other words, successive application of the iteration matrix is the
mechanism driving down the error in a stationary method. We can then
place restrictions on the iteration matrix by using the tools
developed in \S~(\ref{sec:linear_preliminaries}). By assuming $\ve{H}$
is diagonalizable\footnote{We may generalize this to
  non-diagonalizable matrices with the Jordan canonical form of
  $\ve{H}$.} \citep{saad_iterative_2003}, we then have:
\begin{equation}
  \ve{e}^{k} =
  \ve{R}\boldsymbol{\Lambda}^k\ve{R}^{-1}\ve{e}^0\:,
  \label{eq:linear_k_iter_error_diag}
\end{equation}
where $\boldsymbol{\Lambda}$ contains the Eigenvalues of $\ve{H}$ on
its diagonal and the columns of $\ve{R}$ contain the Eigenvectors of
$\ve{H}$. Computing the 2-norm of the above form then gives:
\begin{equation}
  ||\ve{e}^{k}||_2 \leq ||\boldsymbol{\Lambda}^k||_2\ 
  ||\ve{R}||_2\ ||\ve{R}^{-1}||_2\ ||\ve{e}^0||_2\:,
  \label{eq:linear_k_iter_norm1}
\end{equation}
which gives:
\begin{equation}
  ||\ve{e}^{k}||_2 \leq \rho(\ve{H})^k \kappa(\ve{R})
  ||\ve{e}^0||_2\:.
  \label{eq:linear_k_iter_norm2}
\end{equation}
For iteration matrices where the Eigenvectors are orthogonal,
$\kappa(\ve{R})=1$ and the error bound reduces to:
\begin{equation}
  ||\ve{e}^{k}||_2 \leq \rho(\ve{H})^k
  ||\ve{e}^0||_2\:.
  \label{eq:linear_k_iter_norm3}
\end{equation}
We can now resrtict $\ve{H}$ by asserting that $\rho(\ve{H}) < 1$
for a stationary method to converge such that $k$ applications of the
iteration matrix will not cause the error to grow in
Eq~(\ref{eq:linear_k_iter_norm3}). 

\section{Projection Methods}
\label{sec:projection_methods}
Among the most common iterative methods used in scientific computing
today for sparse systems are of a broad class known as
\textit{projection methods}. These methods not only provide access to
more powerful means of reaching a solution, but also a powerful means
of encapsulating the majority of common iterative methods including
the stationary methods just discussed in a common mathematical
framework. All projection methods are built around a core structure
where the solution to Eq~(\ref{eq:linear_problem}) is extracted from a
\textit{search subspace} $\mathcal{K}$ and bound by a
\textit{constraint subspace} $\mathcal{L}$ that will vary in
definition depending on the iterative method selected. We build the
approximate solution $\tilde{\ve{x}}$ by starting with an initial
guess $\ve{x}_0$ and extracting a correction $\boldsymbol{\delta}$
from $\mathcal{K}$ such that:
\begin{equation}
  \tilde{\ve{x}} = \ve{x}_0 +
  \boldsymbol{\delta},\ \boldsymbol{\delta} \in \mathcal{K}\:.
  \label{eq:linear_projection_step}
\end{equation}
We bound this correction by asserting that the new residual,
$\tilde{\ve{r}}$, be orthogonal to $\mathcal{L}$:
\begin{equation}
  \langle \tilde{\ve{r}},\ve{w} \rangle = 0,\ \forall \ve{w} \in
  \mathcal{L}\:.
  \label{eq:linear_projection_constraint}
\end{equation}

We can generate a more physical and geometric-based understanding of
these constraints by writing the new residual as $\tilde{\ve{r}} =
\ve{r}_0 - \ve{A}\boldsymbol{\delta}$ and again asserting the residual
must be orthogonal to $\mathcal{L}$. If $\tilde{\ve{r}}$ is to be
orthogonal to $\mathcal{L}$, then $\ve{A}\boldsymbol{\delta}$ must be
the projection of $\ve{r}_0$ onto the subspace $\mathcal{L}$ that
eliminates the components of the residual that exist in
$\mathcal{L}$. This situation is geometrically presented in
Figure~\ref{fig:linear_projection_constraint}.

\begin{figure}[htpb!]
  \begin{center}
    \scalebox{1.75}{
      \input{chapters/linear_problem/orthogonal_residual.pdftex_t} }
  \end{center}
  \caption{\textbf{Orthogonality constraint of the new residual with
      respect to $\mathcal{L}$.} \textit{By projecting $\ve{r}_0$ onto
      the constraint subspace, we minimize the new residual by
      removing those components.}}
  \label{fig:linear_projection_constraint}
\end{figure}

From Figure~\ref{fig:linear_projection_constraint} we then note that
the following geometric condition must hold:
\begin{equation}
  ||\tilde{\ve{r}}||_2 \leq ||\ve{r}_0||_2,\ \forall \ve{r}_0 \in
  \mathbb{R}^N\:,
  \label{eq:linear_min_res}
\end{equation}
meaning that the residual of the system will always be
\textit{minimized} with respect to the constraints.

Given this minimization condition for the residual, we can form the
outline of an iterative projection method. Consider a matrix $\ve{V}$
to form a basis of $\mathcal{K}$ and a matrix $\ve{W}$ to form a basis
of $\mathcal{L}$. As $\boldsymbol{\delta} \in \mathcal{K}$ by
definition in Eq~(\ref{eq:linear_projection_step}), then
$\boldsymbol{\delta}$ can instead be rewritten as:
\begin{equation}
  \boldsymbol{\delta} = \ve{V}\ve{y},\ \forall \ve{y} \in \mathbb{R}^N:\,
  \label{eq:linear_delta_projection}
\end{equation}
where $\ve{V}$ \textit{projects} $\ve{y}$ onto $\mathcal{K}$. From the
orthogonality constraint in Eq~(\ref{eq:linear_projection_constraint})
it then follows that:
\begin{equation}
  \ve{y} = (\ve{W}^T\ve{A}\ve{V})^{-1}\ve{W}^T\ve{r}_0\:,
  \label{eq:linear_constraint_projection}
\end{equation}
where here the projection onto $\mathcal{K}$ is constrained by the
projection onto $\mathcal{L}$. Knowing this, we can then outline the
following iteration scheme for a projection method:
\begin{subequations}
  \begin{gather}
    \ve{r}^k = \ve{b} - \ve{A}\ve{x}^k\:,\\  
    \ve{y}^k = (\ve{W}^T\ve{A}\ve{V})^{-1}\ve{W}^T\ve{r}_0\:,\\
    \ve{x}^{k+1} = \ve{x}^k + \ve{V}\ve{y}^k\:,
  \end{gather}
  \label{eq:linear_projection_iteration}
\end{subequations}
where $\ve{V}$ and $\ve{W}$ are generated from the definitions of
$\mathcal{K}$ and $\mathcal{L}$ which are defined prior to each
iteration.

From an iteration standpoint, as we choose $\boldsymbol{\delta}$ from
$\mathcal{K}$ and constrain it with $\mathcal{L}$, each iteration
performs a projection that systematically anihilates the components of
the residual that exists in $\mathcal{L}$. This then means that if our
convergence criteria for an iterative method is bound to the residual
of the system, then Eq~(\ref{eq:linear_min_res}) tells us that each
projection step gaurantees us that the norm of the new residual will
never be worse than that of the previous step and will typically move
us towards convergence. Depending on the qualities of the system in
Eq~(\ref{eq:linear_problem}), the selection of the subspaces
$\mathcal{K}$ and $\mathcal{L}$ can serve to both gaurantee
convergence and optimize the rate at which the residual is decreased.

\subsection{Krylov Subspace Methods}
\label{subsec:krylov_methods}
Among the most common projection techniques used in practice are a
class of methods known as \textit{Krylov subspace methods}. Here, the
search subspace is defined as the \textit{Krylov subspace}:
\begin{equation}
  \mathcal{K}_m(\ve{A},\ve{r}_0) = span\{\ve{r}_0, \ve{A}\ve{r}_0,
  \ve{A}^2\ve{r}_0, \dots, \ve{A}^{m-1}\ve{r}_0\}\:,
  \label{eq:krylov_subspace}
\end{equation}
where $m$ denotes the dimensionality of the subspace. In order to
accomodate a more general structure for the operator in
Eq~(\ref{eq:linear_problem}), we often choose an \textit{oblique}
projection method where $\mathcal{K} \neq \mathcal{L}$. If we choose
$\mathcal{L} = \ve{A} \mathcal{K}_m(\ve{A},\ve{r}_0)$, then we are
ultimately solving the normal system $\ve{A}^T\ve{A}\ve{x} =
\ve{A}^T\ve{b}$ where $\ve{A}^T\ve{A}$ will be symmetric positive
definite if $\ve{A}$ is nonsingular, thereby expanding the range of
operators over which these methods are valid. This choice of
constraint subspace also then gives us the result via
Eq~(\ref{eq:linear_projection_constraint}) that the residual is
minimized for all $\boldsymbol{\delta} \in \mathcal{K}$, forming the
basis for the \textit{generalized minimum residual method} (GMRES)
\citep{saad_gmres:_1986}.

Choosing GMRES as our model Krylov method, we are first tasked with
finding a projector onto the subspace. We seek an orthonormal basis
for $\mathcal{K}_m(\ve{A},\ve{r}_0)$ by an orthogonalization procedure
that is commonly based on, but not limited to, the \textit{Arnoldi}
recurrence relation. The Arnoldi procedure will generate an
orthonormal basis, $\ve{V}_m \in \mathbb{R}^{N \times m}$, via a
variant of the Gram-Schmidt procedure that re-applies the operator for
each consecutive vector, thus forming a basis that spans the subspace
in Eq~(\ref{eq:krylov_subspace}). Due to its equivalent
dimensionality, $m$, to that of the subspace, we will refer to such
recurrence relations as \textit{long recurrence relations}. Those
orthogonal projection procedures that have a dimensionality less than
$m$ will be refferred to as \textit{short recurrence relations}.  Once
$\ve{V}_m$ is found, per the constraint subspace definition it then
follows that its basis is defined as $\ve{W}_m = \ve{A}
\ve{V}_m$. Knowing the projections onto the search and constraint
subspaces, the GMRES iteration may be formulated as follows:
\begin{algorithm}[htpb!]
  \caption{GMRES Iteration}
  \label{alg:gmres}
  \begin{algorithmic}
    \State $\ve{r}_0 := \ve{b}-\ve{A}\ve{x}_0$
    \State $\beta := ||\ve{r}_0||_2$
    \State $\ve{v}_1 := \ve{r}_0 / \beta$
    \Comment{Create the orthonormal basis for the Krylov subspace}
    \For{$j = 1, 2, \cdots, m$}
    \State $h_{ij} \leftarrow \langle w_j,v_j \rangle$
    \State $w_j \leftarrow w_j - h_{ij}v_i$
    \EndFor
    \State $h_{j+1,j} \leftarrow ||w_j||_2$
    \State $v_{j+1} \leftarrow w_j / h_{j+1,j}$
    \Comment{Apply the orthogonality constraints}
    \State $\ve{y}_m \leftarrow argmin_y ||\beta \ve{e}_1 - \ve{H}_m\
    \ve{y}||_2 $
    \State $\ve{x}_m \leftarrow \ve{x}_0 + \ve{V_m} \ve{y}_m$
  \end{algorithmic}
\end{algorithm}

We note here several properties of this formulation and how they may
facilitate or hinder the solution of large-scale, sparse linear
problems, also noting that these properties are common among many
Krylov methods. First, from a memory perspective GMRES is efficient in
that the operator $\ve{A}$ need not be explicitly stored. Rather, only
the ability to compute the action of that operator on a vector of
valid size is required. However, these savings in memory are balanced
by the fact that the long recurrence relations used in the Arnoldi
procedure require all vectors that span the Krylov space to be
stored. If the size of these vectors becomes prohibitive, the Arnoldi
procedure can be restarted at the cost of losing information in the
orthogonalization process, creating the potentitial to generate new
search directions that are not orthogonal to all previous search
directions (and therefore less than optimal). From an implementation
perspective, because the operator is not required to be formed, GMRES
is significantly more flexible in its usage in that there are many
instances where various processes serve to provide the action of that
operator (e.g. radiation transport sweeps \citep{evans_denovo:_2010})
that normally may not be amenable to its full construction. In
addition, the minimization problem is a straight-forward least-squares
problem where $\ve{H}$ is an upper-Hessenberg matrix.

\section{Parallel Projection Methods}
\label{sec:parallel_krylov_methods}
Modern parallel implementations of projection methods on distributed
memory architectures rely heavily on capabilities provided by general
linear algebra frameworks. For methods like GMRES, this arises from
the fact that Krylov methods require only a handful of operation types
in their implementation that can be efficiently programmed on these
architectures. Per Saad's text \citep{saad_iterative_2003} and as
noted in Algorithm~\ref{alg:gmres}, these operations are
preconditioning, matrix-vector multiplications, vector updates, and
inner products. For the last three items, linear algebra libraries
such as PETSc \citep{gropp_scalable_1993} and Trilinos
\citep{heroux_overview_2005} provide efficient parallel
implementations for these operations. Depending on the type of
preconditioning used, efficient parallel implementations may also be
available for those operations. Due to their prevalence in modern
numerical methods, parallel formulations these operations have
warranted intense study \citep{tuminaro_parallel_1998}. In all cases, a series
of scatter/gather operations are required such that global
communication operations must occur. Although the relative performance
of such operations is bound to the implementation, asymptotically
perfomance should be the same across all implementations.

We will look at the three primary parallel matrix/vector operations as
preconditioning is not an immediate requirement for implementing the
algorithms. Furthermore, variants are available that reduce the number
of global communications required (consider
\cite{sosonkina_scalable_1998} as an example of reducing global
operation counts using a different orthogonalization procedure than
Arnoldi), however, we will only consider the basic algorithms here as
this handful operations can be generalized to fit more complicated
algorithms. In all of these cases, we assume a general matrix/vector
formulation that is distributed in parallel such that both local and
global knowledge of their decomposition is available on
request. Furthermore, it is assumed that these objects are partitioned
in such a way that the parallel formulation of the operator and
vectors in Eq~(\ref{eq:linear_problem}) will be such that each
parallel process contains only a subset of the global problem and that
subset forms a local set of complete equations. The form of this
partitioning is problem dependent and often has a geometric or
graph-based aspect to its construction in order to optimize
communication patterns. Libraries such as Zoltan
\citep{devine_zoltan_2002}, provide an implementations of such
algorithms.

\subsection{Parallel Vector Update}
\label{subsec:parallel_vec_update}
Parallel vector update operations arise from the construction of the
orthonormal basis and the application of the correction generated by
the constraints to the solution vector. Vector update operations are
embarassingly parallel in that they require no communication
operations to be successfully completed; all data operated on is
local. These operations are globally of the form:
\begin{equation}
  \ve{y}[n] \leftarrow \ve{y}[n] + a * \ve{x}[n],\ \forall n \in [1,N_g]
  \:,
  \label{eq:global_vector_update}
\end{equation}
and locally of the form:
\begin{equation}
  \ve{y}[n] \leftarrow \ve{y}[n] + a * \ve{x}[n],\ \forall n \in [1,N_l]
  \:,
  \label{eq:local_vector_update}
\end{equation}
where $\ve{y}$ and $\ve{x}$ are vectors of global size $N_g$, local
size $N_l$, and $a \in \mathbb{R}^N$. In order to avoid communcation,
the vectors $\ve{y}$ and $\ve{x}$ must have the same parallel
decomposition where each parallel process owns the same pieces of each
vector. 

\subsection{Parallel Vector Product}
\label{subsec:parallel_vector_product}
Vector product operations are used in several instances during a
Krylov iteration including vector norm computations and the
orthogonalization procedure. By definition, the vector product is a
global operation that effectively collapses two vectors to a single
value. Therefore, we cannot eliminate all global
communications. Instead, vector product operations are formulated as
\textit{global reduction operations} that are efficiently supported by
modern message passing libraries. For the dot product of two vectors
$\ve{y}$ and $\ve{x}$, a single reduction is required such that:
\begin{equation}
  d_l = \ve{y}_l \cdot \ve{x}_l,\ d_g = \sum_p d_l \:,  
  \label{eq:parallel_dot_product}
\end{equation}
where the $l$ subscript denotes a local quantity, $d_l$ is the local
vector dot product, and $d_g$ is the global dot product generated by
summing the local dot products over all $p$ processes. Parallel norm
operations can be conducted with the same single reduction. Consider
the infinity norm operation:
\begin{subequations}
  \begin{gather}
    ||x||_{\infty,l} = \max_n \ve{y}[n],\ \forall n \in [1,N_l]\:\\
    ||x||_{\infty,g} = \max_p ||x||_{\infty,l}\:.
  \end{gather}
  \label{eq:parallel_infinity_norm}
\end{subequations}
In this form, the local infinity norm is computed over the local piece
of the vector. The reduction operation is then formed over all $p$
processes such that the global max of the vector is computed and
distributed to all processes.

\subsection{Parallel Matrix-Vector Multiplications}
\label{subsec:parallel_mat_vec_mutliply}
We finally consider parallel matrix-vector multiplication operations
using sparse matrices in a compressed storage format by considering
Saad's outline as well as the more formal work of Tuminaro
\citep{tuminaro_parallel_1998}. For these operations, more complex
communication patterns will be required given that the entire global
vector is required in order to compute a single element of the local
product vector. Fortunately, the vast majority of the global vector
components will be multiplied by zero due to the sparsity of the
matrix and therefore much of the vector can be neglected. Instead we
only require data from a handful of other processes that can be
acquired through asynchronous/synchronous communications. Consider the
sparse matrix-vector multiply in
Figure~\ref{fig:partitioned_matvec_multiply} that is partitioned on 3
processors.
\begin{figure}[htpb!]
  \begin{center}
    \scalebox{1.5}{
      \input{chapters/linear_problem/partitioned_matrix.pdftex_t} }
  \end{center}
  \caption{\textbf{Sparse matrix-vector multiply $\ve{A}\ve{x}=\ve{y}$
      operation partitioned on 3 processors.} \textit{Each process
      owns a set of equations that correlates to its physical
      domain.}}
  \label{fig:partitioned_matvec_multiply}
\end{figure}
Each process owns a set of equations that correlate to the physical
domain of which it has ownership. We can break down the equations
owned by each process in order to devise an efficient scheme for the
multiplication. Consider the portion of the matrix-vector multiply
problem owned by process 1 in
Figure~\ref{fig:partitioned_matvec_multiply}. As shown in
Figure~\ref{fig:matvec_proc_1}, the components of the matrix will be
multiplied by pieces of the vector that are owned by all
processors. 
\begin{figure}[htpb!]
  \begin{center}
    \scalebox{1.5}{
      \input{chapters/linear_problem/matvec_proc_1.pdftex_t} }
  \end{center}
  \caption{\textbf{Components of sparse matrix-vector multiply
      operation owned by process 1.} \textit{The numbers above the
      matrix columns indicate the process that owns the piece of the
      global vector they are acting on. In order to compute its local
      components of the matrix-vector product, process 1 needs its
      matrix elements along with all elements of the global vector
      owned by processes 2 and 3. The piece of the matrix shown is
      $\ve{A}_1$ and it is acting locally on $\ve{x}_1$ to compute the
      local piece of the product, $\ve{y}_1$.}}
  \label{fig:matvec_proc_1}
\end{figure}
For those pieces of the matrix that are owned by process 1 that act on
the vector owned by process 1, we do these multiplications first as no
communication is required. Next, process one gathers the components of
the global vector that it requires to complete its part of the vector
product. For this example, the components of matrix that will operate
on the global vector are zero, and therefore no vector elements are
required to be scattered from process 3 to process 1. Those matrix
elements that will act on the piece of the vector owned by process 2
are not all non-zero, and therefore we must gather the entire process
2 vector components onto process 1 to complete the multiplication.
Conversely, processes 2 and 3 must scatter its vector components that
are required by other processes (such as process 1) in order to
complete their pieces of the product. This then implies that these
domain connections for proper gather and scatter combinations must be
constructed a priori. These data structures are typically generated by
a data partitioning library. Mathematically, if we are performing a
global matrix-vector multiply of the form $\ve{A}\ve{x} = \ve{y}$,
then for this example on process 1 we have a sequence of local
matrix-vector multiplications: $\ve{A}_1\ve{x}_1 + \ve{A}_1\ve{x}_2 =
\ve{y}_1$. Here, some of the data is intrinisically local, and some
must be gathered from other processes using the partitioning data
structures.

\subsection{Parallel Perfomance Implications for Krylov Methods}
\label{subsec:projection_method_performance}
Knowing the parallel characteristics of the key operations we must
perform in order to implement Krylov methods, we can make a few
statements about parallel perfomance and implications for operation on
machines of increasing size.  For very large distributed machines,
global reduction operations required at several levels of Krylov
algorithms stand to reduce scalablitity and perfomance. Furthermore,
communication between adjacent domains in matrix-vector multiply
operations may also cause a bottleneck as the number of domains used
in simulation grows. The end result is that global data must be
collected and communicated. For scaling improvement, we seek a
reduction in these types of operations. In addition, these issues
become more prominent as the Krylov iterations progress, causing the
Krylov subspace to grow and the total number of operations needed to
orthogonalize that subspace to increase.

