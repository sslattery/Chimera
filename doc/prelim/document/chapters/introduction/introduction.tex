\chapter{Introduction}
\label{ch:introduction}
In many fields of engineering and physics, linear and nonlinear
problems are a primary focus of study. Recent focus on multiple
physics systems in the nuclear reactor modeling and simulation
community adds a new level of complexity to common nonlinear systems
as solution strategies change when they are coupled to other problems
\citep{u.s._department_of_energy_casl_2011}. Furthermore, a desire
for predictive simulations to enhance the safety and performance of
nuclear systems creates a need for extremely high fidelity
computations to be performed for these coupled systems as a means to
capture effects not modeled by coarser methods.

In order to achieve this high fidelity, state-of-the-art computing
\index{facilities} must be leveraged in a way that is both efficient
and considerate of hardware-related issues. As scientific computing
moves towards exascale facilities with machines of $O(1,000,000)$
cores already coming on-line, new algorithms to solve these complex
problems must be developed to leverage this new hardware
\citep{kogge_using_2011}. Issues such as resiliency to node failure,
limited growth of memory available per node, and scaling to large
numbers of cores will be pertinent to robust algorithms aimed at this
new hardware. Considering these issues, this preliminary report
proposes the development of a massively parallel Monte Carlo method
for linear problems and a novel Monte Carlo method to advance solution
techniques for nonlinear problems.

We discuss in this chapter motivation for advancing Monte Carlo
solvers by providing multiphysics problems of interest in nuclear
reactor analysis. Hardware-based motivations are also provided by
considering the impact of forthcoming computing architectures. In
addition, background on the current solver techniques for multiphysics
problems and a brief comparison to the proposed methods is provided
to further motivate this work.

\section{Physics-Based Motivation}
\label{sec:physics_motivation}
Predictive modeling and simulation capability requires the combination
of high fidelity models, high performance computing hardware that can
handle the intense computational loads required by these models, and
modern algorithms for solving these problems that leverage this high
performance hardware. For nuclear reactor analysis, this predictive
capability can enable tighter design tolerances for improved thermal
performance and efficiency, higher fuel burn-up and therefore reduction
in generated waste, and high confidence in accident scenario
models. The physics that dominate these types of analysis include
neutronics, thermal hydraulics, computational fluid dynamics, and
structural mechanics.

Although solution techniques in each of these individual categories
has advanced over the last few decades and in fact leveraged modern
algorithms and computer architectures, true predictive capability for
engineered systems can only be achieved through a coupled, multiple
physics analysis where the effects of feedback between physics are
modeled. For example, consider the safety analysis of a departure from
nucleate boiling scenario in the subchannel of a nuclear fuel
assembly.
\begin{figure}[htpb!]
  \begin{center}
    \scalebox{1.5}{
      \input{chapters/introduction/dnb_example.pdftex_t} }
  \end{center}
  \caption{\textbf{Multiphysics dependency analysis of departure from
      nucleate boiling.} \textit{A neutronics solution is required to
      compute power generation in the fuel pins, fluid dynamics is
      required to characterize boiling and fluid temperature and
      density, heat transfer is required to compute the fuel and
      cladding temperature, and the nuclear data modified with the
      temperature and density data. Strong coupling among the
      variables creates strong nonlinearities.}}
  \label{fig:dnb_example}
\end{figure}
When this event occurs, heat transfer is greatly reduced between the
fuel and the coolant due to the vapor layer generated by boiling,
causing the fuel center-line temperature to rapidly rise. To
characterize this boiling phenomena and how it affects fuel failure we
must consider a neutronics analysis in order to compute power
generation in the fuel pins, fluid dynamics analysis to characterize
coolant boiling, temperature, and density, solid material heat
transfer to characterize fuel and cladding temperature and heat
transfer with the coolant, and nuclear data processing to characterize
how changing material temperatures and densities changes the cross
sections needed for the neutronics calculation. As shown in
Figure~\ref{fig:dnb_example}, many couplings are required among
individual physics components in order to accurately model this
situation with each physics generating and receiving many
responses. Those variables that are very tightly coupled, such as the
temperatures generated by the fluid dynamics and heat transfer
components, will have strong nonlinearities in their behavior and
would therefore benefit from fully consistent nonlinear solution
schemes instead of fixed-point type iterations between
physics\footnote{Fixed-point iterations between physics are commonly
  referred to as Picard iterations.}. Furthermore, the space and time
scales over which these effects occur will also vary greatly.

The computational resources required to solve such problems are
tremendous. Recent work in modeling coupled fluid flow and solid
material heat and mass transfer in a reactor subsystem, similar to the
same components of the departure from nucleate boiling example above,
was performed as part of analysis of the Department of Energy's
Consortium for Advanced Simulation of Light Water Reactors (CASL)
modeling and simulation hub. CASL used the Drekar multiphysics code
developed at Sandia National Laboratories
\citep{pawlowski_drekar_2012} for modeling goals in
grid-to-rod-fretting analysis and will use a similar coupled physics
structure for future departure from nucleate boiling analysis with
comparison to experimental data. Using Drekar, multiphysics
simulations have been performed with fully consistent methods for the
solution of nonlinear systems using meshes of $O(\sn{1}{9})$ elements
leveraging $O(100,000)$ cores on leadership class machines. Neutronics
components to be implemented in CASL for multiphysics analysis, such
as the Denovo radiation transport code developed at Oak Ridge National
Laboratory \citep{evans_denovo:_2010}, compute trillions of unknowns
for full core reactor analysis on $O(\sn{1}{9})$ element meshes and
$O(100,000)$ cores as well. Given the large scale and complexity of
these problems, if we aim to advance multiphysics solution techniques,
then we are motivated to advance the solution of complex and general
nonlinear problems exploiting leadership class levels of parallelism.

\section{Hardware-Based Motivation}
\label{sec:hardware_motivation}
As leadership class machines move towards the exascale, new algorithms
must be developed that leverage their strengths and adapt to their
shortcomings. Basic research is required now to advance methods in
time for these new machines to become operational. Organized work is
already moving forward in this area with the Department of Energy's
Advanced Scientific Computing Research office specifically allocating
funding for the next several years to research resilient solver
technologies for exascale facilities
\citep{u.s._department_of_energy_resilient_2012}. Based on the
language in this call for proposals, we can identify key issues for
which a set of robust, massively parallel Monte Carlo solvers could
provide a solution. As machines begin to operate at hundreds of
petaflops peak performance and beyond, trends toward reduced energy
consumption will require incredibly high levels of concurrency to
achieve the desired computation rates. Furthermore, this drop in
power consumption will mean increased pressure on memory as memory per
node is expected to stagnate while cores per node is expected to
increase. As the number of cores increases, their clock speed is
expected to stagnate or even decrease to further reduce power
consumption and manufacturing costs.

The end result of these hardware changes is that the larger numbers of
low-powered processors will be prone to both soft failures such as bit
errors in floating point operations and hard failures where the data
owned by that processor cannot be recovered. Because these failures
are predicted to be common, resilient solver technologies are required
to overcome these events. With linear and nonlinear solvers based on
Monte Carlo techniques, such issues are alleviated by statistical
arguments. In the case of soft failures, isolated floating point
errors in Monte Carlo simulation are absorbed within tally statistics
while completely losing hardware during a hard failure is manifested
as a high variance event where some portion of the Monte Carlo
histories are lost. These stochastic methods are a paradigm shift from
current deterministic solver techniques that will suffer greatly from
the non-deterministic behavior expected from these exascale machines.

In addition to resiliency concerns, the memory restrictions on future
hardware will hinder modern solvers that derive their robustness from
using large amounts of memory. Stochastic methods that are formulated
to use less memory than conventional methods will serve to alleviate
some of this pressure. In addition, new parallel strategies that may
be implemented with stochastic methods could offer a new avenue for
leveraging the expected levels of high concurrency in exascale
machines.

\section{Research Outline}
\label{sec:research_outline}
For some time, the particle transport community has been utilizing
Monte Carlo methods for the solution of transport problems
\citep{lewis_computational_1993}. The partial differential equation
(PDE) community has focused on various deterministic methods for
solutions to linear problems \citep{saad_iterative_2003,
  kelley_iterative_1995}. In between these two areas are a not widely
known group of Monte Carlo methods for solving sparse linear systems
\citep{forsythe_matrix_1950, hammersley_monte_1964,
  halton_sequential_1962, halton_sequential_1994}. In recent years,
these methods have been further developed for radiation transport
problems in the form of Monte Carlo Synthetic-Acceleration (MCSA)
\citep{evans_monte_2009, evans_monte_2012} but have yet to be applied
to more general sparse linear systems commonly generated by the
computational physics community. Compared to other methods in this
regime, MCSA offers three attractive qualities; (1) the linear problem
operator need not be symmetric or positive-definite, thereby reducing
preconditioning complexity, (2) the stochastic nature of the solution
method provides a natural solution to the issue of resiliency, and (3)
is amenable to parallelization using modern methods developed by the
transport community \citep{wagner_hybrid_2010}. The development of
MCSA as a general linear solver and the development of a parallel MCSA
method will be new and unique features of this work, providing a
framework with which other issues such as resiliency may be addressed
in the future.

In addition to linear solver advancements, nonlinear solvers may also
benefit from a general and parallel MCSA scheme. In the nuclear
engineering community, nonlinear problems are often addressed by
either linearizing the problem or building a segregated scheme and
using traditionally iterative or direct methods to solve the resulting
system \citep{pletcher_computational_1997}. In the mathematics
community, various Newton methods have been popular
\citep{kelley_iterative_1995}. Recently, Jacobian-Free Newton-Krylov
(JFNK) schemes \citep{knoll_jacobian-free_2004} have been utilized in
multiple physics architectures and advanced single physics codes
\citep{gaston_parallel_2009}. The benefits of JFNK schemes are that
the Jacobian is never formed, simplifying the implementation, and a
Krylov solver is leveraged (typically GMRES or Conjugate Gradient),
providing excellent convergence properties for well-conditioned and
well-scaled systems. However, there are two potential drawbacks to
these methods for high fidelity predictive simulations: (1) the
Jacobian is approximated by a first-order differencing method on the
order of machine precision such that this error can grow beyond that
of those in a fine-grained system \citep{kelley_iterative_1995} and
(2) for systems that are not symmetric positive-definite (which will
be the case for most multiphysics systems and certainly for most
preconditioned systems) the Krylov subspace generated by the GMRES
solver may become prohibitively large
\citep{knoll_newton-krylov_1995}. To address these issues, this thesis
proposes a new and novel method for nonlinear systems based on the
MCSA method.

The Forward-Automated Newton-MCSA (FANM) method is proposed as new
nonlinear solution method. The key features of FANM are: full Jacobian
generation using modern Forward Automated Differentiation (FAD)
methods \citep{bartlett_automatic_2006}, and MCSA as the inner linear
solver. This method has several attractive properties. First, the
first-order approximation to the Jacobian used in JFNK type methods is
eliminated by generating the Jacobian explicitly with the model
equations through FAD. Second, the Jacobian need not be explicitly
formed by the user but is instead automated through FAD; this
eliminates the complexity of hand-coding derivatives and has also been
demonstrated to be more efficient computationally than evaluating
difference derivatives. Third, unlike GMRES, MCSA does not build a
subspace during iterations. Although the Jacobian must be explicitly
formed to use MCSA, for problems that take more than a few GMRES
iterations to converge the size of the Krylov subspace will grow
beyond that of the Jacobian. Finally, using MCSA for the linear solve
provides its benefits for preconditioning, potential resiliency, and
parallelism.

This preliminary report outlines in Chapter~\ref{ch:linear_problem}
the conventional methods used in practice for solving linear and
nonlinear problems to provide a mathematical basis upon which to build
new algorithms that aim to solve some of the aforementioned
issues. Parallel schemes for conventional methods are also provided
for background and understanding of how current methods may or may not
map to future hardware. In addition, some components of their parallel
implementations may be applied to stochastic methods development. In
Chapter~\ref{ch:stochastic_methods}, Monte Carlo algorithms for
solving linear systems and new parallel strategies will be outlined in
full with links made to past work and their potential for offering
improvements to the multiphysics analysis community. From these
stochastic methods, a new nonlinear method is proposed in
Chapter~\ref{ch:nonlinear_problem} and compared to conventional
methods for solving nonlinear problems. The research proposal for this
work is presented in Chapter~\ref{ch:research_proposal} with progress
to date reported including the development of a new finite
element-based physics framework that leverages a preliminary general
implementation of the stochastic method that will serve as a test bed
for these new methods. An outline of a set of numerical experiments
will then be provided that continues the development and
implementation of these new methods and verifies them with benchmark
solutions, culminating in the analysis of a multiphysics simulation of
a pressurized water reactor subsystem using a production code system.
