\chapter{Stochastic Solution Methods for Nonlinear Problems}
\label{ch:nonlinear_problem}
Nonlinear equation sets are a common occurence in multiphysics
problems. Systems of partial differntial equations such as those that
describe fluid flow or more general transport processes when
discretized by conventional methods yield discrete sets of stiff
equations with nonlinearities present in the variables. Traditionally,
such systems have been solved by linearizing them such that the
nonlinearities in the variables are eliminated and more traditional
linear methods can be used for solutions. Often characterized as
segregated methods where physics operators are split and their action
on the system approximated in steps, such methods lack consistency and
accuracy in resolving the nonlinear component of the solution. In the
last 30 years, fully consistent nonlinear methods based on Newton's
method have become more popular and many advances have been made in
the computational physics field to employ these methods.

Currently, Krylov methods are used with Newton's method to form a
robust solution sequence for these types of problems. However, given
our goals of advancing stochastic solver technology for linear
problems, we aim to apply them to these fully consistent schemes in
order to gauge if some improvements can be made. We therefore devise a
new method using MCSA in conjuction with new methods, accomodating the
fact that MCSA requires an explicitly formed linear operator in its
implementation as compared to Krylov methods. In addition to handling
this dependency, we will construct a parallel strategy for a nonlinear
MCSA-based method.

\section{Preliminaries}
\label{sec:nonlinear_preliminaries}
We formulate the \textit{nonlinear problem} as follows
\citep{knoll_jacobian-free_2004}:
\begin{equation}
  \ve{F}(\ve{u}) = \ve{0}\:,
  \label{eq:nonlinear_problem}
\end{equation}
where $\ve{u} \in \mathbb{R}^n$ is the solution vector and
$\ve{F}:\mathbb{R}^N \rightarrow \mathbb{R}^N$ is the function of
nonlinear residuals. We write the nonlinear system in this form so
that when an exact solution for $\ve{u}$ is achieved, all residuals
evaluate to zero. \textit{Newton's method} is a root finding algorithm
and therefore we can use it to solve Eq~(\ref{eq:nonlinear_problem})
if we interpret the exact solution $\ve{u}$ to be the roots of
$\ve{F}(\ve{u})$. Newton's method is also an iterative scheme, and we
can generate this procedure by building the Taylor expansion of the
$k+1$ iterate of the nonlinear residuals about the previous $k$
iterate:
\begin{equation}
  \ve{F}(\ve{u}^{k+1}) = \ve{F}(\ve{u}^{k}) +
  \ve{F}'(\ve{u}^{k})(\ve{u}^{k+1}-\ve{u}^{k}) +
  \frac{\ve{F}''(\ve{u}^{k})}{2}(\ve{u}^{k+1}-\ve{u}^{k})^2 + \cdots
  \:.
  \label{eq:newton_derivation_1}
\end{equation}
If we ignore the nonlinear terms in the expansion and assert that at
the $k+1$ iterate $\ve{u}^{k+1}$ is the exact solution such that
$\ve{F}(\ve{u}^{k+1}) = \ve{0}$, then we are left with the following
equality:
\begin{equation}
  -\ve{F}(\ve{u}^{k}) =
  \ve{F}'(\ve{u}^{k})(\ve{u}^{k+1}-\ve{u}^{k})\:.
  \label{eq:newton_derivation_2}
\end{equation}
We note two things of importance in
Eq~(\ref{eq:newton_derivation_2}). The first is that
$\ve{F}'(\ve{u}^{k})$ is in fact the \textit{Jacobian},
$\ve{J}(\ve{u})$, of the set of nonlinear residuals and is defined
element-wise as:
\begin{equation}
  J_{ij} = \frac{\partial F_i(\ve{u})}{\partial u_j}\:.
  \label{eq:jacobian_def}
\end{equation}
Second, we note that $(\ve{u}^{k+1}-\ve{u}^{k})$ is simply the
solution update from the $k$ iterate to the $k+1$ iterate. We will
define this update as the \textit{Newton correction} at the $k$
iterate, $\delta \ve{u}^k$. We can then rerarrange
Eq~(\ref{eq:newton_derivation_2}) to define the Newton iteration
scheme for nonlinear problems:
\begin{subequations}
  \begin{gather}
    \ve{J}(\ve{u}) \delta \ve{u}^k = -\ve{F}(\ve{u}^{k})\\
    \ve{u}^{k+1} = \ve{u}^k + \delta \ve{u}^k\:.
  \end{gather}
  \label{eq:newton_iteration}
\end{subequations}
We then have three distinct steps to perform: evaluation of the
nonlinear residuals using the solution at the $k$ iterate, the
solution of a linear system to compute the Newton correction where the
Jacobian matrix of the nonlinear equation set is the linear operator,
and the application of the correction to the previous iterate's
solution to arrive at the next iterate's solution. In the asymptotic
limit, the iterations of Newton's method will converge the nonlinear
residual quadratically \citep{kelley_iterative_1995}. Convergence
criteria is set for stopping the iteration sequence based on the
nonlinear residual. Commonly, the following criteria is used:
\begin{equation}
  ||\ve{F}(\ve{u}^{k})|| < \epsilon ||\ve{F}(\ve{u}^{0})||\:,
  \label{eq:newton_stopping_criteria}
\end{equation}
where $\epsilon$ is a user defined tolerance parameter. Newton's
method is \textit{consistent} in that all components of the nonlinear
functions that describe the physics we are modeling are updated
simulataneously in the iteration sequence with respect to one
another. This is in comparison to \textit{inconsistent} strategies,
such as a pressure correction strategy for solving the Navier-Stokes
equations \citep{pletcher_computational_1997}, where the components of
$\ve{u}$ are updated in a staggered fashion depending on the
particular equations that they are associated with.

\section{Inexact Newton Methods}
\label{sec:newton_methods}
Inexact Newton methods arise when the Jacobian operator is not exactly
inverted, resulting in an inexact Newton correction as initially
described by Dembo and others \citep{dembo_inexact_1982}. For common
sparse nonlinear systems, which in turn yield a sparse Jacobian
matrix, this situation occurs when conventional iterative methods are
applied. In their definition, Dembo formulated inexact methods such
that they are independent of the linear method used to solve for the
Newton correction and therefore are amenable to use with any linear
solver. Furthermore, they bind the convergence of the outer nonlinear
iteration to the inner linear iteration such that:
\begin{equation}
  ||\ve{J}(\ve{u}^k)\delta \ve{u}^k + \ve{F}(\ve{u}^k)|| \leq \eta^k
  ||\ve{F}(\ve{u}^k)||\:,
  \label{eq:inexact_newton_forcing}
\end{equation}
where $\eta^k \in [0,1)$ is defined as the \textit{forcing term} at
  the $k$ iterate. Eq~(\ref{eq:inexact_newton_forcing}) then states
  that the residual generated by the linear solver is bound by the
  nonlinear residual and how tightly it is bound is defined by the
  forcing term. This is useful in that we can vary how tightly coupled
  the convergence of the linear iterations used to generate the Newton
  correction is to the nonlinear iteration by relaxing or tightening
  the convergence properties on the linear iterative method. As a
  result, strategies for determining the forcing term can vary
  depending on the problem type and can greatly affect the convergence
  of the method or even prohibit convergence
  \citep{eisenstat_choosing_1996}. In addition, \textit{globalization
    methods} may be used to modify the Newton correction in a more
  desireable direction such that convergence properties can be
  improved when the initial guess for $\ve{u}$ is poor
  \citep{pawlowski_globalization_2006}.

\subsection{Newton-Krylov Methods}
\label{subsec:newton_krylov_methods}
A form of inexact Newton methods, \textit{Newton-Krylov methods} are
nonlinear iterative methods that leverage a Krylov subspace method as
the linear solver for generating the Newton correction
\citep{kelley_iterative_1995}. As we investigated in
Chapter~\ref{ch:linear_problem}, Krylov methods are robust and enjoy
efficient parallel implementations on modern
architectures. Furthermore, their lack of explicit dependence on the
operator make them easier to implement than other
methods. Additionally, although many iterations can become memory
intensive due to the need to store the Krylov subspace for the
orthogonalization procedure, at each nonlinear iteration this cost is
reset as the Jacbobian matrix will change due to its dependence on the
solution vector. This means that for every nonlinear iteration, a
completely new linear system is formed for generating the Newton
correction and we can modify the Krylov solver parameters accordingly
to accomodate this. In most nonlinear problems, the Jacobian operator
is generally nonsymmetric and therefore either Krylov methods with
long recurrence relations that can handle nonsymmetric systems must be
considered or the Newton correction system must be preconditioned such
that the operator is symmetric and short recurrence relation methods
can be potentially be used.

With many Krylov methods available, which to use with the Newton
method is dependent on many factors including convergence rates and
memory usage. Several studies have been performed to investigate this
\citep{mchugh_inexact_1993,knoll_newton-krylov_1995}. In their
numerical studies in 1995, Knoll and McHugh used the set of highly
nonlinear and stiff convection-diffusion-reaction equations to solve a
set of tokamak plasma problems with the goal of measuring solver
performance with Newton's method. They note several tradeoffs in using
Krylov methods with the Newton solver. The first is that the
optimization condition that results from the constraints (e.g. the
minimization of the GMRES residual over the Krylov space) can be
relaxed by restricting the size of the subspace such that only a fixed
number of subspace vectors may be maintained, thus reducing memory
requirements. We can also relax the optimization condition by instead
restarting the recurrence relation with a new set of vectors once a
certain number of vectors have been generated. The optimization
condition is maintain over that particular set of vectors, however,
Knoll and McHugh note that this ultimately slows the convergence rate
as compared to keeping all vectors as the new set of vectors is not
necessarily orthogonal to the previous set, and therfore not optimal
over the entire iteration procedure. The orthogonality condition can
be relaxed by using a recurrence relation that does not generate a
strictly orthonormal basis for the Krylov subspace such as the Lanzcos
biorthogonalization procedure, resulting in memory savings due to the
shorter Lanzcos recurrence relation.

As a comparison, Knoll and McHugh chose an Arnoldi-based GMRES with a
fixed vector basis approximately the size of the number of iterations
required to converge as the long recurrence relation solver and
conjugate gradients squared (CGS), bi-orthogonalized conjugate
gradient stabilized (Bi-CGSTAB), and transpose-free quasiminimal
residual (TFQMR) methdos as Lanzcos-based short recurrence relation
solvers. All solvers were used to compute the right-preconditioned
Newton correction system. For standard implementations of Newton's
method where the Jacobian operator was explicitly formed using
difference equations, all methods exhibited roughly equivalent
iteration count performance for both the inner linear iterations and
the outer nonlinear iterations in terms of iterations required to
converge. Bi-CGSTAB typically performed the best for
implementations where the Jacobian was explicitly formed and GMRES
perfoming best for matrix-free implementations. However, upon
investigating the convergence of the inner iterations, it was observed
that the GMRES solver was significantly more robust, always generating
a monotonically decreasing residual as compared to the Lanzcos-based
methods which had the tendency to oscillate. Based on these results,
in all of their future work Knoll and McHugh tended to use GMRES as
the Krylov solver \citep{knoll_jacobian-free_2004}.

\subsubsection{Jacobian-Free Approximation}
\label{subsubsec:jacobian_free_approximation}
In most cases, the Jacobian is difficult to form from the difference
equations and costly to evaluate for large equation sets. For simple
nonlinear cases such as the Navier-Stokes equations, the derivatives
can be computed and coded, but due to the complexity of those
derivatives and the resulting difference equations this task can be
tedious, error prone, and must be repeated for every equation
set. Furthermore, in their 1995 work, Knoll and McHugh also noted that
a dominating part of their computation time was the evaluation of the
difference equations for building the Jacobian
\citep{knoll_newton-krylov_1995}. By recognizing that Krylov methods
only need the action of the operator on the vector instead of the
operator itself, the Jacobian can instead be approximated through
various numerical methods including a difference-based Jacobian-free
formulation. Jacobian-Free methods, and in particular
\textit{Jacobian-Free Newton-Krylov} (JFNK) methods
\citep{knoll_jacobian-free_2004}, rely on forming the action of the
Jacobian on a vector as required by the Krylov solver through a
forward difference scheme. In this case, the action of the Jacobian on
some vector $\ve{v}$ is given as:
\begin{equation}
  \ve{J}(\ve{u})\ve{v} = \frac{\ve{F}(\ve{u} + \epsilon \ve{v}) -
    \ve{F}(\ve{u})}{\epsilon}\:,
  \label{eq:jacobian_free_product}
\end{equation}
where $\epsilon$ is a small number typically on the order of machine
precision. Kelley \citep{kelley_iterative_1995} points out a potential
downfall of this formulation in that if the discretization error in
$\ve{F}(\ve{u})$ is on the order of the perturbation parameter
$\epsilon$, then the finite difference error from
Eq~(\ref{eq:jacobian_free_product}) pollutes the solution. In
addition, Knoll and McHugh noted that for preconditioning purposes,
part of the Jacobian must still explicitly be formed periodically and
that linear solver robustness issues were magnified by the matrix-free
approach due to the first-order approximation. This formation
frequency coupled with the numerous evaluations of the Jacobian
approximation create a situation where after so many nonlinear
iterations, it becomes cheaper to instead fully form the
Jacobian. For simple equation sets, this may only take 5-10 Newton
iterations to reach this point while over 30 may be required for
larger equations sets and therefore larger Jacobians.

\subsubsection{Automatic Differentiation for Jacobian Generation}
\label{subsubsec:automatic_differentiation}
If it is acceptable to store the actual Jacobian matrix, other methods
are available to construct it without requiring hand-coding and
evaluating derivatives, thus eliminating the associated issues. In
addition, if any additional equations are added to the system or a
higher order functional approximation is desired, it would be useful
to avoid regenerating and coding these derivatives. Becoming more
prominent in the 1990's, \textit{automatic differentiation} is a
mechanism by which the derivatives of a function can be generated
automatically by evaluating it. Automatic differentiation is built on
the concept that all functions discretely represented in a computer
are ultimately represented by elementary mathematical operations. If
the chain rule is applied to those elementary operations, then the
derivatives of those functions can be computed to the order of
accuracy of their original discretization in a completely automated
way \citep{averick_computing_1994}.

The work of Bartlett and others \citep{bartlett_automatic_2006}
extended initial Fortran-based work in the area of automatic
differentiation implementations to leverage the parametric type and
operator overloading features of C++ \citep{stroustrup_c++_1997}. They
formulate the differentiation problem from an element viewpoint by
assuming that a global Jacobian can be assembled from local element
function evaluations of $e_k : \mathbb{R}^{n_k} \rightarrow
\mathbb{R}^{m_k}$, similar to the finite element assembly procedure
as:
\begin{equation}
  \ve{J}(\ve{u}) = \sum_{i=1}^N \ve{Q}^T_i \ve{J}_k \ve{P}_i\:,
  \label{eq:fad_global_jacobian}
\end{equation}
where $\ve{J}_{k_i} = \partial e_{k_i} / \partial P_i u$ is the
$k^{th}$ element function Jacobian, $\ve{Q} \in \mathbb{R}^{n_{k_i}
  \times N}$ is a projector onto the element domain and $\ve{P} \in
\mathbb{R}^{m_{k_i} \times N}$ a projector onto the element range for
$\ve{F}(\ve{u}) \in \mathbb{R}^{N \times N}$. The Jacobian matrix for
each element will therefore have entirely local data in a dense
structure, eliminating the need for parallel communication and sparse
techniques during differentiation. Only when all local differentials
are computed does communication of the Jacobian occur through
gather/scatter operations in order to properly assembly it. Also of
benefit is the fact that element-level computations generally consist
of a smaller number of degrees of freedom, thus reducing memory
requirements during evaluation as compared to a global formulation of
the problem. Such a formulation is not limited to finite element
formulations and is amenable to any scheme where the system is
globally sparse with degrees of freedom coupled to local domains
including finite volume representations. The templating capabilities
of C++ were leveraged with the element-based evaluation and assembly
scheme as in Eq~(\ref{eq:fad_global_jacobian}) by templating element
function evaluation code on the evaluation type. If these functions
are instantiated with standard floating point types then the residual
is returned. If they are instead instantiated with the
operator-overloaded automatic differentiation types, both the residual
and Jacobian are returned.

Of interest to Bartlett, Averick, and the many others that have
researched automatic differentation are measures of its performance
relative to hand-coded derivatives and capturing the Jacboian matrix
from matrix-free approximations. Given their element-based function
evaluation scheme, Bartlett's work varied the number of degrees of
freedom per element and compared both the floating point operation
count and CPU time for both the templated automatic differentiation
method and hand-coded derivatives for Jacobian evaluations. Although
they observed a 50\% increase in floating point operations in the
templated method over the hand-coded method, run times were observed
to be over 3 times faster for the templated method. They hypothesize
that this is due to the fact that the element-based formulation of the
templated method is causing better utilization of cache and therefore
faster data access. Furthermore, they observed linear scaling behavior
for automatic differentiation as the number of degrees of freedom per
element were increased up to a few hundred. Based on these results,
this type of automatic differentation formulation was deemed
acceptable for use in large-scale, production physics codes.

\section{The FANM Method}
\label{sec:fanm}
In production physics codes based on nonlinear equations sets,
Newton-Krylov methods are the primary means of generating a fully
consistent solution scheme
\citep{evans_development_2006,evans_enhanced_2007,gaston_parallel_2009,godoy_parallel_2012}. Typically,
for large scale simulations these problems are memory limited due to
the subspaces generated by robust Krylov methods. Often, a matrix-free
approach is chosen to relax memory requirements over directly
generating the Jacobian matrix and facilitate the
implementation. However, as we observed in previous sections, these
matrix-free methods suffer from poorly scaled problems and the first
order error introduced by the Jacobian approximation. In addition, it
was observed that the savings induced by the matrix-free approach is
eventually amortized over a number of nonlinear iterations where it
becomes more efficient computationally to instead form the Jacobian.

In Chapter~\ref{ch:stochastic_methods}, we focused our efforts on
developing and improving Monte Carlo methods for inverting linear
systems. These methods, when used to accelerate a stationary method in
MCSA, enjoy a robust implementation and exponential convergence
rates. Further, the only storage required is that of the full linear
system including the linear operator so that we may generate
transition probabilities for the random walk sequence. Although this
requires more storage to represent the linear system than that of a
Krylov method where the operator is not required, we do not incur any
additional storage costs once the iteration sequence
begins. Furthermore, in the context of nonlinear problems, the
Jacobian matrix that we are required to generate for the Monte Carlo
solvers may be generated at will from the nonlinear functions in the
Newton system using automatic differentation. Not only do we then have
a simple and automated way to generate the Jacobian, but we also enjoy
a Jacobian of numerical precision equivalent to that of our function
evaluations. We therefore propose the \textit{Forward-Automated
  Newton-MCSA} (FANM) method that utilizes all of the above
components. We hypothesize that such a method will be competetive with
Newton-Krylov methods not only from a convergence and timing
perspective, but also relax scaling requirements of matrix-free
methods and memory costs of both matrix-free and fully formed Jacobian
methods to allow the application developer to solve problems of finer
discretization and higher-order functional representations while
maintaining a robust and efficient parallel implementation.

\subsection{Jacobian Storage vs. Subspace Storage and Restarts}
\label{subsec:fanm_storage}
To gauge the memory benefits of a FANM method over a Krylov-based
scheme, we must look at the storage requirements of the Jacobian
matrix as compared to the Krylov subspace vectors for sparse linear
problems. Saad's text provides us relations for both of these cases
\citep{saad_iterative_2003}. Beginning with the Jacobian storage, for
sparse problems production linear algebra library implementations
utilize \textit{compressed row storage} to efficiently store the
non-zero components of a sparse matrix while still allowing for all
standard parallel matrix computations to be performed. Per
\S~\ref{sec:parallel_krylov_methods}, recall that efficient parallel
matrix-vector multiplications rely on processors knowing which other
processors contain their neighboring matrix and vector
elements. Typically this information is represented by a graph which
must be stored along with the matrix elements themselves. To store the
elements, consider the following sparse matrix:
\[
  \ve{A} =
  \begin{bmatrix}
    2 & 0 & 8 & 0 & 0 & 0 \\ 
    4 & 5 & 0 & 1 & 0 & 0 \\ 
    0 & 2 & 1 & 0 & 1 & 0 \\ 
    0 & 0 & 3 & 7 & 0 & 2 \\ 
    0 & 0 & 0 & 4 & 9 & 0 \\
    0 & 0 & 0 & 0 & 9 & 1 \\
  \end{bmatrix}\:.
\]
The compressed form of \ve{A} would then be:
\[
  \begin{array}{lccccccccccccccc}
    \ve{A}\ values: & 2 & 8 & 4 & 5 & 1 & 2 & 1 & 1 & 3 & 7 & 2 & 4 & 9
    & 9 & 1 \\ column: & 1 & 3 & 1 & 2 & 4 & 2 & 3 & 5 & 3 & 4 & 6 & 4
    & 5 & 5 & 6 \\ row\ start: & 1 & 3 & 6 & 9 & 12 & 14 & 16 \\
  \end{array}
\]
where the first row contains the non-zero values of the matrix
$\ve{A}$ starts at, the second row contains the column index of those
values, and the third row contains the index of the A value and column
index rows that each matrix row starts at ending with the start of the
next row. For this simple example, we actually break even by storing
36 elements in the full matrix and 36 pieces of data in the compressed
format. However, for large, sparse matrices there is a significant
storage savings using the compressed format.

In his discussion on orthogonalization schemes for Krylov subspaces,
Saad provides us with space and time complexities for these
operations. To find a solution vector $\ve{x} \in \mathbb{R}^{N \times
  N}$ with $m$ Krylov iterations (and therefore $m+1$ subspace
vectors) the storage requirement is then $(m+1) N$ for most common
orthogonalization techniques. Using our simple example above (even
though we did not see any gains in storage over the full matrix
representation), if it takes 10 GMRES iterations using Arnoldi
orthogonalization to converge to a solution, then a total of 66 pieces
of data are required to be stored instead of 36 for the explicitly
formed matrix case (we do not consider graph storage here for
comparison as both methods will require the graph for parallel
operations). For larger sparse matrices, this disparity in memory
requirements will be even greater, especially for ill-conditioned
systems that require many GMRES iterations to converge.

\subsection{Parallel FANM Implementation}
\label{subsec:parallel_fanm}
Like the parallelization of the MCSA method described in
\S~\ref{sec:parallel_stochastic_methods}, a parallel FANM method
relies on a basic set of parallel matrix-vector operations as well as
the global residual and Jacobian assembly procedure described in
\S~\ref{subsubsec:automatic_differentiation}. Consider the Newton
iteration scheme in Eq~(\ref{eq:newton_iteration}). We must first
assemble the linear system in parallel through the element-wise
function evaluations to generate both the global Jacobian operator and
the global residual vector on the right hand side. Per Bartlett's
work, efficient and automated parallel mechanisms are available to do
this through a sequence of scatter/gather operations. With these tools
available for residual and Jacobian generation, the remainder of the
parallel procedure is simple, with the linear Newton correction system
solved using the parallel MCSA method as previously described and the
Newton correction applied to the previous iterate's solution through a
parallel vector update operation.

As Newton methods are formulated independent of the inner linear
solver generating the Newton corrections, the actual performance of
the nonlinear iterations using MCSA is expected to be similar to that
of traditional Newton-Krylov methods. Furthermore, we expect to
achieve numerically identical answers with a Newton-MCSA method as
other Newton methods and we should indeed verify this. The parallel
performance of such a method will inherently be bound to the parallel
Monte Carlo implementation of the linear solver as the parallel
operations at the nonlinear iteration level are identical to those
that you would perform with a Newton-Krylov method. Matrix-free
formulations will have different parallel performance than these
methods, and therefore we should compare FANM perfomance to JFNK-based
schemes. More important than performance in this situation is the
reduced memory pressure that a FANM implementation provides, as
discussed in \S~\ref{subsec:fanm_storage}, because a FANM method will
not generate a subspace in the linear solver and compressed storage
for sparse matrices are utilized, we expect significant memory savings
over Newton-Krylov methods. We must measure the memory utilization of
both of these methods in order to quantify their differences and
provide additional analysis of the FANM method's merits, or lack
thereof.
