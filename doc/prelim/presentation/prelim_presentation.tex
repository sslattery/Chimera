\documentclass{beamer}
\usetheme[white]{Wisconsin}
\usepackage{longtable}
\usepackage{listings}
\usepackage{color}
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm} \usepackage{amsmath} \usepackage{tmadd,tmath}
\usepackage[mathcal]{euscript} \usepackage{color}
\usepackage{textcomp}
\usepackage{algorithm,algorithmic}
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\lstset{
  backgroundcolor=\color{lbcolor},
  tabsize=4,
  rulecolor=,
  language=c++,
  basicstyle=\scriptsize,
  upquote=true,
  aboveskip={1.5\baselineskip},
  columns=fixed,
  showstringspaces=false,
  extendedchars=true,
  breaklines=true,
  prebreak =
  \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
  frame=single,
  showtabs=false,
  showspaces=false,
  showstringspaces=false,
  identifierstyle=\ttfamily,
  keywordstyle=\color[rgb]{0,0,1},
  commentstyle=\color[rgb]{0.133,0.545,0.133},
  stringstyle=\color[rgb]{0.627,0.126,0.941},
}

%% colors
\setbeamercolor{boxheadcolor}{fg=white,bg=UWRed}
\setbeamercolor{boxbodycolor}{fg=black,bg=white}


%%---------------------------------------------------------------------------%%
\author{Stuart R. Slattery
  \\ Engineering Physics Department
  \\ University of Wisconsin - Madison
}

\date{\today} 
\title{Massively Parallel Monte Carlo Methods for Discrete Linear and
  Nonlinear Systems} 
\begin{document}
\maketitle

%%---------------------------------------------------------------------------%%
\begin{frame}{Introduction}

  \begin{itemize}
  \item Predictive modeling and simulation enhances engineering
    capability
  \item Modern work focused on this task leverages multiple physics
    simulation (CASL, NEAMS)
  \item New hardware drives algorithm development (petascale and
    exascale)
  \item Monte Carlo methods have the potential to provide great
    improvements that permit finer simulations and better mapping to
    future hardware
  \item A set of massively parallel Monte Carlo methods is proposed to
    advance multiple physics simulation on contemporary and future
    leadership class machines
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Physics-Based Motivation}
  
  \pause
  \begin{beamerboxesrounded}[upper=boxheadcolor,lower=boxbodycolor,shadow=true]
    {Predictive nuclear reactor analysis enables...}
    \begin{itemize}
    \item Tighter design tolerance for improved thermal performance
      and efficiency
    \item Higher fuel burn-up
    \item High confidence in accident scenario models
    \end{itemize}
  \end{beamerboxesrounded}

  \pause
  \begin{beamerboxesrounded}[upper=boxheadcolor,lower=boxbodycolor,shadow=true]
    {Multiple physics simulations are complicated...}
    \begin{itemize}
    \item Neutronics, thermal hydraulics, computational fluid
      dynamics, structural mechanics, and many other physics
    \item Consistent models yield nonlinearities in the variables
      through feedback effects
    \item Tremendous computational resources are required with
      $O(\sn{1}{9})$ element meshes and $O(100,000)+$ cores used in
      today's simulations.
    \end{itemize}
  \end{beamerboxesrounded}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Physics-Based Motivation: DNB}

  \begin{columns}

    \begin{column}{0.35\textwidth}
      \begin{figure}[htpb!]
        \begin{center}
          \scalebox{1}{ \input{dnb_schematic.pdftex_t} }
        \end{center}
        \caption{\textbf{Departure from nucleate boiling scenario.} }
      \end{figure}
    \end{column}

    \pause

    \begin{column}{0.65\textwidth}
      \begin{figure}[htpb!]
        \begin{center}
          \scalebox{0.8}{ \input{dnb_example.pdftex_t} }
        \end{center}
        \caption{\textbf{Multiphysics dependency analysis of departure
            from nucleate boiling.} }
      \end{figure}
    \end{column}

  \end{columns}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Hardware-Based Motivation}

  \begin{itemize}
  \item Modern hardware is moving in two directions:
    \begin{itemize}
    \item Lightweight machines
    \item Heterogeneous machines
    \item Both characterized by low power and high concurrency
    \end{itemize}
  \item Some issues:
    \begin{itemize}
    \item Higher potential for both soft and hard failures
    \item Memory restrictions are expected with a continued decrease
      in memory/FLOPS
    \end{itemize}
  \item Potential resolution from Monte Carlo:
    \begin{itemize}
    \item Soft failures buried within the tally variance
    \item Hard failures are high variance events
    \item Memory savings over conventional methods
    \end{itemize}
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Research Outline}
  \begin{itemize}
  \item Parallelization of Monte Carlo methods for discrete systems
    \begin{itemize}
    \item Parallel strategies taken from modern reactor physics
      methods
    \item Research is required to explore varying parallel strategies
    \item Scalability is of concern
    \end{itemize}
  \item Development of a nonlinear solver leveraging Monte Carlo
    \begin{itemize}
    \item Application to nonlinear problems of interest
    \item Memory benefits
    \item Performance benefits
    \end{itemize}
  \end{itemize}
\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Linear Operator Equations}

  \begin{itemize}
  \item We seek solutions of the general linear operator equation
  \end{itemize}

  \[
  \ve{A} \ve{x} = \ve{b}\:
  \]
  \[
  \ve{A} \in \mathbb{R}^{N \times N},\ \ve{A} : \mathbb{R}^{N}
  \rightarrow \mathbb{R}^{N},\ \ve{x} \in \mathbb{R}^N,\ \ve{b} \in
  \mathbb{R}^N\:
  \]


  \[
  \ve{r} = \ve{b} - \ve{A}\ve{x}\:
  \]

  \begin{itemize}
  \item $\ve{r}=\ve{0}$ when an exact solution is found.
  \end{itemize}

  \pause
  \begin{beamerboxesrounded}[upper=boxheadcolor,lower=boxbodycolor,shadow=true]
    {A Requirement} Assert that $\ve{A}$ is \textit{nonsingular}. The
    solution is then:
    \[
    \ve{x} = \ve{A}^{-1}\ve{b}\:
    \]
  \end{beamerboxesrounded}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Stationary Methods}

  \begin{itemize}
  \item General stationary methods are formed by splitting the linear
    operator
  \end{itemize}

  \[
  \ve{A} = \ve{M} - \ve{N}\:.
  \]

  \[
  \ve{x} = \ve{M}^{-1}\ve{N}\ve{x} + \ve{M}^{-1}\ve{b}\:.
  \]

  \begin{itemize}
  \item We identify $\ve{H} =\ve{M}^{-1}\ve{N}$ as the
    \textit{iteration matrix}
  \end{itemize}

  \[
  \ve{x}^{k+1} = \ve{H}\ve{x}^{k} + \ve{c}\:.
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Stationary Methods Convergence}

  \begin{itemize}
  \item The qualities of the iteration matrix dictate convergence
  \item Define $\ve{e}^k = \ve{x}^k-\ve{x}$ as the error at the
    $k^{th}$ iterate
  \end{itemize}

  \[
  \ve{e}^{k+1} = \ve{H} \ve{e}^k\:
  \]


  \begin{itemize}
  \item We diagonalize $\ve{H}$ to extract its Eigenvalues
  \end{itemize}

  \[
  ||\ve{e}^{k}||_2 = \rho(\ve{H})^k ||\ve{e}^0||_2\:,
  \]

  \begin{itemize}
  \item We bound $\ve{H}$ by $\rho(\ve{H}) < 1$ for convergence
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Projection Methods}

  \begin{itemize}
  \item Powerful class of iterative methods
  \item Provides theory that encapsulates most other iterative methods
  \item Leveraged in many modern physics codes at the petascale
  \end{itemize}

  \pause
  \begin{beamerboxesrounded}[upper=boxheadcolor,lower=boxbodycolor,shadow=true]
    {Search Subspace $\mathcal{K}$} Extract the solution from the
    search subspace:
    \[
    \tilde{\ve{x}} = \ve{x}_0 +
    \boldsymbol{\delta},\ \boldsymbol{\delta} \in \mathcal{K}\:
    \]
  \end{beamerboxesrounded}

  \pause
  \begin{beamerboxesrounded}[upper=boxheadcolor,lower=boxbodycolor,shadow=true]
    {Constraint Subspace $\mathcal{L}$} Constrain the extraction with
    the constraint subspace by asserting orthogonality with the
    residual:
    \[
    \langle \tilde{\ve{r}},\ve{w} \rangle = 0,\ \forall \ve{w} \in
    \mathcal{L}\:
    \]  
  \end{beamerboxesrounded}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{The Orthogonality Constraint}

  \[
  \tilde{\ve{r}} = \ve{r}_0 - \ve{A}\boldsymbol{\delta}
  \]

  \begin{figure}[htpb!]
    \begin{center}
      \scalebox{1.25}{ \input{orthogonal_residual.pdftex_t} }
    \end{center}
    \caption{\textbf{Orthogonality constraint of the new residual with
        respect to $\mathcal{L}$.} }
  \end{figure}

  \pause
  \begin{beamerboxesrounded}[upper=boxheadcolor,lower=boxbodycolor,shadow=true]
    {Minimization Property}

    The residual of the system will always be \textit{minimized} with
    respect to the constraints
    \[
    ||\tilde{\ve{r}}||_2 \leq ||\ve{r}_0||_2,\ \forall \ve{r}_0 \in
    \mathbb{R}^N\:,
    \]
  \end{beamerboxesrounded}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Putting it All Together}

  \begin{itemize}
  \item Choose $\ve{V}$ as a basis of $\mathcal{K}$ and $\ve{W}$ as a
    basis of $\mathcal{L}$
  \end{itemize}

  \[
  \boldsymbol{\delta} = \ve{V}\ve{y},\ \forall \ve{y} \in \mathbb{R}^N
  \]

  \[
  \ve{y} = (\ve{W}^T\ve{A}\ve{V})^{-1}\ve{W}^T\ve{r}_0
  \]

  \pause
  \begin{beamerboxesrounded}[upper=boxheadcolor,lower=boxbodycolor,shadow=true]
    {Projection Method Iteration}
    \[
    \ve{r}^k = \ve{b} - \ve{A}\ve{x}^k
    \]
    \[
    \ve{y}^k = (\ve{W}^T\ve{A}\ve{V})^{-1}\ve{W}^T\ve{r}^k
    \]
    \[
    \ve{x}^{k+1} = \ve{x}^k + \ve{V}\ve{y}^k\:
    \]
    \[
    Update\ \ve{V}\ and\ \ve{W}
    \]
  \end{beamerboxesrounded}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Krylov Subspace Methods}

  \[
  \mathcal{K}_m(\ve{A},\ve{r}_0) = span\{\ve{r}_0, \ve{A}\ve{r}_0,
  \ve{A}^2\ve{r}_0, \dots, \ve{A}^{m-1}\ve{r}_0\}
  \]
  \[
  \mathcal{L} = \ve{A} \mathcal{K}_m(\ve{A},\ve{r}_0)
  \]

  \begin{itemize}
  \item Yields the normal system $\ve{A}^T\ve{A}\ve{x} =
    \ve{A}^T\ve{b}$
  \item Must generate an orthonormal basis $\ve{V}_m \in \mathbb{R}^{N
    \times m}$ for $\mathcal{K}_m(\ve{A},\ve{r}_0)$
  \item $\ve{W}_m = \ve{A}\ve{V}_m$
  \item Typically choose a Gram-Schmidt-like procedure such as Arnoldi
    or Lanzcos
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}[fragile]{GMRES}

  \begin{algorithm}[H]
    \begin{algorithmic}[1]
      \STATE $\ve{r}_0 := \ve{b}-\ve{A}\ve{x}_0$ \STATE $\beta :=
      ||\ve{r}_0||_2$ \STATE $\ve{v}_1 := \ve{r}_0 / \beta$
      \COMMENT{Create the orthonormal basis for the Krylov subspace}
      \FOR{$j = 1, 2, \cdots, m$} \STATE $\ve{w}_j := \ve{A}\ve{v}_j$
      \FOR{$i = 1, 2, \cdots, j$} \STATE $h_{ij} \leftarrow \langle
      \ve{w}_j,\ve{v}_j \rangle$ \STATE $\ve{w}_j \leftarrow \ve{w}_j
      - h_{ij}\ve{v}_i$ \ENDFOR \STATE $h_{j+1,j} \leftarrow
      ||\ve{w}_j||_2$ \STATE $\ve{v}_{j+1} \leftarrow \ve{w}_j /
      h_{j+1,j}$ \ENDFOR \COMMENT{Apply the orthogonality constraints}
      \STATE $\ve{y}_m \leftarrow argmin_y ||\beta \ve{e}_1 -
      \ve{H}_m\ \ve{y}||_2 $ \STATE $\ve{x}_m \leftarrow \ve{x}_0 +
      \ve{V_m} \ve{y}_m$
    \end{algorithmic}
    \caption{GMRES Iteration}
  \end{algorithm}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Parallel Projection Methods}
  
  \begin{itemize}
  \item Parallel vector update
  \end{itemize}
  \[
  \ve{y}[n] \leftarrow \ve{y}[n] + a * \ve{x}[n],\ \forall n \in
     [1,N_g]
  \]
  \[
  \ve{y}[n] \leftarrow \ve{y}[n] + a * \ve{x}[n],\ \forall n \in
     [1,N_l]
  \]
  
  \begin{itemize}
  \item Parallel dot product
  \end{itemize}
  \[
  d_l = \ve{y}_l \cdot \ve{x}_l,\ d_g = \sum_p d_l
  \]

  \begin{itemize}
  \item Parallel vector norm
  \end{itemize}
  \[
  ||x||_{\infty,l} = \max_n \ve{y}[n],\ \forall n \in [1,N_l]
  \]
  \[
  ||x||_{\infty,g} = \max_p ||x||_{\infty,l}
  \]
  
\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Parallel Matrix-Vector Multiplication}

  \begin{figure}[htpb!]
    \begin{center}
      \scalebox{0.75}{ \input{partitioned_matrix.pdftex_t} }
    \end{center}
    \caption{\textbf{Matrix-vector multiply $\ve{A}\ve{x}=\ve{y}$
        operation on 3 processors.}
      \label{fig:partitioned_matvec_multiply} }
  \end{figure}

  \pause
  \begin{figure}[htpb!]
    \begin{center}
      \scalebox{0.75}{ \input{matvec_proc_1.pdftex_t} }
    \end{center}
    \caption{\textbf{Components of multiply operation owned by process
        1.} }
    \label{fig:matvec_proc_1}
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Projection Methods Summary}

  \begin{itemize}
  \item Used in practice everywhere
  \item Global reduction operations observed not to impede scalability
    \begin{itemize}
    \item Dot product
    \item Vector norms
    \end{itemize}
  \item Nearest neighbor computations have poor algorithmic strong
    scaling
    \begin{itemize}
    \item Matrix-vector multiply
    \item Weak scaling is better
    \end{itemize}
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Monte Carlo Methods for Discrete Linear Systems}

  \begin{itemize}
  \item First proposed by J. Von Neumann and S.M. Ulam in the 1940's
  \item Earliest published reference in 1950
  \item General lack of published work
  \item Modern work by Evans and others has yielded new applications
  \end{itemize}
\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Monte Carlo Linear Solver Preliminaries}

  \begin{itemize}
  \item Split the operator
  \end{itemize}

  \[
  \ve{H} = \ve{I} - \ve{A}
  \]

  \[
  \ve{x} = \ve{H} \ve{x} + \ve{b}
  \]

  \begin{itemize}
  \item Generate the \textit{Neumann series}
  \end{itemize}
  
  \[
  \ve{A}^{-1} = (\ve{I}-\ve{H})^{-1} = \sum_{k=0}^{\infty} \ve{H}^k
  \]

  \begin{itemize}
  \item Require $\rho(\ve{H}) < 1$ for convergence
  \end{itemize}

  \[
  \ve{A}^{-1}\ve{b} = \sum_{k=0}^{\infty} \ve{H}^k\ve{b} = \ve{x}
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Monte Carlo Linear Solver Preliminaries}

  \begin{itemize}
  \item Expand the Nuemann series
  \end{itemize}

  \[
  x_i = \sum_{k=0}^{\infty}\sum_{i_1}^{N}\sum_{i_2}^{N}\ldots
  \sum_{i_k}^{N}h_{i,i_1}h_{i_1,i_2}\ldots h_{i_{k-1},i_k}b_{i_k}
  \]

  \begin{itemize}
  \item Define a sequence of state transitions
  \end{itemize}
  
  \[
  \nu = i \rightarrow i_1 \rightarrow \cdots \rightarrow i_{k-1}
  \rightarrow i_{k}
  \]

  \begin{itemize}
  \item Define the \textit{Neumann-Ulam decomposition}\footnote{The
    Hadamard product $\ve{A} = \ve{B} \circ \ve{C}$ is defined
    element-wise as $a_{ij} = b_{ij} c_{ij}$.}
  \end{itemize}

  \[
  \ve{H} = \ve{P} \circ \ve{W}
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Direct Method}

  \begin{itemize}
  \item Compute row-normalized transition probabilities and weights
  \end{itemize}

  \[
  p_{ij} = \frac{|h_{ij}|}{\sum_j |h_{ij}|},\ w_{ij} =
  \frac{h_{ij}}{p_{ij}}
  \]

  \begin{itemize}
  \item Generate an expectation value for the solution
  \end{itemize}

  \[
  W_{m} = \sum_{m=0}^k w_{i,i_1} w_{i_1,i_2} \cdots w_{i_{m-1},i_m}
  \]
  \[
  X_{\nu}(i_0 = i) = \sum_{m=0}^k W_{m} b_{i_m}
  \]
\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Direct Method}

  \begin{itemize}
  \item Compute the probability of a particular random walk
    permutation
  \end{itemize}

  \[
  P_{\nu} = p_{i,i_1} p_{i_1,i_2} \cdots p_{i_{k-1},i_k}
  \]

  \begin{itemize}
  \item Generate the estimator
  \end{itemize}

  \[
  E\{X(i_0 = i)\} = \sum_{\nu} P_{\nu} X_{\nu}
  \]

  \begin{itemize}
  \item Check that we recover the exact solution
  \end{itemize}

  \[
  \begin{split}
    E\{X(i_0 = i)\}
    &=\sum_{k=0}^{\infty}\sum_{i_1}^{N}\sum_{i_2}^{N}\ldots
    \sum_{i_k}^{N} p_{i,i_1}p_{i_1,i_2}\ldots p_{i_{k-1},i_k}
    w_{i,i_1}w_{i_1,i_2}\ldots w_{i_{k-1},i_k} b_{i_k}\\ &= x_i\:,
  \end{split}
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Direct Method: Evolution of a Solution}

  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=4in]{direct_1.png}
    \end{center}
    \caption{\textbf{Direct solution to Poisson Equation.} \textit{1
        history per state, \sn{2.5}{3} total histories. 0.785 seconds
        CPU time.} }
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Direct Method: Evolution of a Solution}

  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=4in]{direct_10.png}
    \end{center}
    \caption{\textbf{Direct solution to Poisson Equation.} \textit{10
        histories per state, \sn{2.5}{4} total histories. 5.9 seconds
        CPU time.} }
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Direct Method: Evolution of a Solution}

  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=4in]{direct_100.png}
    \end{center}
    \caption{\textbf{Direct solution to Poisson Equation.} \textit{100
        histories per state, \sn{2.5}{5} total histories. 54.7 seconds
        CPU time.} }
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Direct Method: Evolution of a Solution}

  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=4in]{direct_1000.png}
    \end{center}
    \caption{\textbf{Direct solution to Poisson Equation.} \textit{100
        histories per state, \sn{2.5}{6} total histories. 644 seconds
        CPU time.} }
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Adjoint Method}

  \begin{itemize}
  \item Solve the adjoint linear system
  \end{itemize}

  \[
  \ve{A}^T \ve{y} = \ve{d}
  \]

  \[
  \ve{y} = \ve{H}^T \ve{y} + \ve{d}
  \]

  \begin{itemize}
  \item Set the adjoint constraint
  \end{itemize}

  \[
  \langle \ve{A}^T \ve{x}, \ve{y} \rangle = \langle \ve{x}, \ve{A}
  \ve{y} \rangle
  \]

  \[
  \langle \ve{x}, \ve{d} \rangle = \langle \ve{y}, \ve{b} \rangle
  \]
  
\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Adjoint Method}

  \begin{itemize}
  \item Generate the Neumann series for the adjoint operator
  \end{itemize}

  \[
  \ve{y} = (\ve{I} - \ve{H}^T)^{-1} \ve{d}
  \]

  \[
  \ve{y} = \sum_{k=0}^{\infty} (\ve{H}^T)^k\ve{d}
  \]

  \begin{itemize}
  \item Expand the series
  \end{itemize}

  \[
  y_i = \sum_{k=0}^{\infty}\sum_{i_1}^{N}\sum_{i_2}^{N}\ldots
  \sum_{i_k}^{N}h_{i_k,i_{k-1}}\ldots h_{i_2,i_1} h_{i_1,i} d_{i_k}
  \]

  \begin{itemize}
  \item Pick another constraint to yield the original solution
  \end{itemize}

  \[
  \ve{d} = \boldsymbol{\delta}_i,\ \langle \ve{y}, \ve{b} \rangle =
  \langle \ve{x}, \boldsymbol{\delta}_i \rangle = x_i
  \]
  
\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Adjoint Method}

  \begin{itemize}
  \item Use the adjoint Neumann-Ulam decomposition
  \end{itemize}

  \[
  \ve{H}^{T} = \ve{P} \circ \ve{W}
  \]

  \[
  p_{ij} = \frac{|h_{ji}|}{\sum_j |h_{ji}|},\ w_{ij} =
  \frac{h_{ji}}{p_{ij}}
  \]

  \begin{itemize}
  \item Build the estimator and expectation value
  \end{itemize}

  \[
  X_{\nu} = \sum_{m=0}^k W_{m} b_{i_0} \delta_{i,i_m}
  \]

  \[
  \begin{split}
    E\{X_j\} &=\sum_{k=0}^{\infty}\sum_{i_1}^{N}\sum_{i_2}^{N}\ldots
    \sum_{i_k}^{N} b_{i_0} h_{i,i_1}h_{i_1,i_2}\ldots h_{i_{k-1},i_k}
    \delta_{i_k,j} \\ &= x_{j}\:,
  \end{split}
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Adjoint Method: Evolution of a Solution}

  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=4in]{adjoint_1.png}
    \end{center}
    \caption{\textbf{Adjoint solution to Poisson Equation.}
      \textit{\sn{1}{0} total histories, 0.286 seconds CPU time.} }
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Adjoint Method: Evolution of a Solution}

  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=4in]{adjoint_10.png}
    \end{center}
    \caption{\textbf{Adjoint solution to Poisson Equation.}
      \textit{\sn{1}{1} total histories, 0.278 seconds CPU time.} }
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Adjoint Method: Evolution of a Solution}

  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=4in]{adjoint_100.png}
    \end{center}
    \caption{\textbf{Adjoint solution to Poisson Equation.}
      \textit{\sn{1}{2} total histories, 0.275 seconds CPU time.} }
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Adjoint Method: Evolution of a Solution}

  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=4in]{adjoint_1000.png}
    \end{center}
    \caption{\textbf{Adjoint solution to Poisson Equation.}
      \textit{\sn{1}{3} total histories, 0.291 seconds CPU time.} }
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Adjoint Method: Evolution of a Solution}

  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=4in]{adjoint_10000.png}
    \end{center}
    \caption{\textbf{Adjoint solution to Poisson Equation.}
      \textit{\sn{1}{4} total histories, 0.428 seconds CPU time.} }
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Adjoint Method: Evolution of a Solution}

  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=4in]{adjoint_100000.png}
    \end{center}
    \caption{\textbf{Adjoint solution to Poisson Equation.}
      \textit{\sn{1}{5} total histories, 1.76 seconds CPU time.} }
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Adjoint Method: Evolution of a Solution}

  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=4in]{adjoint_1000000.png}
    \end{center}
    \caption{\textbf{Adjoint solution to Poisson Equation.}
      \textit{\sn{1}{6} total histories, 15.1 seconds CPU time.} }
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Adjoint Method: Evolution of a Solution}

  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=4in]{adjoint_10000000.png}
    \end{center}
    \caption{\textbf{Adjoint solution to Poisson Equation.}
      \textit{\sn{1}{7} total histories, 149 seconds CPU time.} }
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Sequential Monte Carlo}

  \begin{itemize}
  \item Neumann-Ulam methods bound by the Central Limit Theorem
  \item Halton proposed an iterative residual method
  \item Iteration error decoupled from Monte Carlo error
  \item Exponential convergence
  \end{itemize}

  \[
  \ve{r}^k = \ve{b} - \ve{A}\ve{x}^k
  \]
  \[
  \ve{A}\boldsymbol{\delta}^{k} = \ve{r}^{k}
  \]
  \[
  \ve{x}^{k+1} = \ve{x}^k + \boldsymbol{\delta}^{k}
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Monte Carlo Synthetic-Acceleration}
  \begin{itemize}
  \item Split the operator to yield Richardson's iteration
  \end{itemize}

  \[
  \ve{x} = (\ve{I} - \ve{A})\ve{x} + \ve{b}
  \]
  \[
  \ve{x}^{k+1} = (\ve{I} - \ve{A})\ve{x}^k + \ve{b}
  \]

  \begin{itemize}
  \item Define the iteration error
  \end{itemize}

  \[
  \delta \ve{x}^k = \ve{x} - \ve{x}^k
  \]
  \[
  \delta \ve{x}^{k+1} = (\ve{I} - \ve{A})\delta \ve{x}^k
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Monte Carlo Synthetic-Acceleration}

  \begin{itemize}
  \item Subtract $(\ve{I} - \ve{A})\delta \ve{x}^{k+1}$
  \end{itemize}

  \[
  \begin{split}
    \ve{A}\delta \ve{x}^{k+1} &= (\ve{I} -
    \ve{A})(\ve{x}^{k+1}-\ve{x}^{k}) \\ &= \ve{r}^{k+1}
  \end{split}
  \]

  \begin{itemize}
  \item The following converges in one iteration with exact inversion
    of $\ve{A}$:
  \end{itemize}

  \[
  \ve{x}^{k+1} = (\ve{I} - \ve{A})\ve{x}^k + \ve{b}
  \]
  \[
  \ve{A} \delta \ve{x}^{k+1} = \ve{r}^{k+1}
  \]
  \[
  \ve{x} = \ve{x}^{k+1} + \delta \ve{x}^{k+1}
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Monte Carlo Synthetic-Acceleration}

  \begin{beamerboxesrounded}[upper=boxheadcolor,lower=boxbodycolor,shadow=true]
    {MCSA Iteration}

    \[
    \ve{x}^{k+1/2} = (\ve{I} - \ve{A})\ve{x}^k + \ve{b}
    \]
    \[
    \ve{r}^{k+1/2} = \ve{b} - \ve{A}\ve{x}^{k+1/2}
    \]
    \[
    \hat{\ve{A}}\delta\ve{x}^{k+1/2} = \ve{r}^{k+1/2}
    \]
    \[
    \ve{x}^{k+1} = \ve{x}^{k+1/2} + \delta \ve{x}^{k+1/2}
    \]

  \end{beamerboxesrounded}

  \begin{itemize}
  \item Adjoint Neumann-Ulam solver computes the correction
  \item Decouples Monte Carlo error from solution error
  \item Exponential convergence
  \item Demonstrated by Evans and colleagues to be competitive with
    Krylov methods
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Preconditioning Monte Carlo Methods}

  \begin{itemize}
  \item No symmetry requirements
  \item Require $\rho(\ve{H}) < 1$
  \item Choose Jacobi preconditioning at a minimum
  \end{itemize}

  \[
  \ve{M} = diag(\ve{A})
  \]
  \[
  \ve{M}^{-1}\ve{A}\ve{x} = \ve{M}^{-1}\ve{b}
  \]
  
  \begin{itemize}
  \item Yields a preconditioned MCSA iteration with no in-state
    transitions
  \end{itemize}

  \[
  \ve{x}^{k+1/2} = (\ve{I} - \ve{M}^{-1}\ve{A})\ve{x}^k + \ve{b}
  \]
  \[
  \ve{r}^{k+1/2} = \ve{b} - \ve{M}^{-1}\ve{A}\ve{x}^{k+1/2}
  \]
  \[
  \ve{M}^{-1}\ve{A}\delta\ve{x}^{k+1/2} = \ve{r}^{k+1/2}
  \]
  \[
  \ve{x}^{k+1} = \ve{x}^{k+1/2} + \delta \ve{x}^{k+1/2}
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Direct vs. Adjoint Analysis}

  \begin{itemize}
  \item Analysis needed to select Monte Carlo method
  \item Time-dependent 2-dimensional Poisson equation
  \item Spectral radius fixed
  \item Sparsity varied with 2 Laplacian stencils
  \end{itemize}

  \[
  \nabla^2_5 = \frac{1}{\Delta^2}[u_{i-1,j} + u_{i+1,j} + u_{i,j-1} +
    u_{i,j+1} - 4 u_{i,j}]
  \]
  \[
  \begin{split}
    \nabla^2_9 = \frac{1}{6\Delta^2}[4 u_{i-1,j} + 4 u_{i+1,j} + 4
      u_{i,j-1} + 4 u_{i,j+1} + u_{i-1,j-1}\\ + u_{i-1,j+1} +
      u_{i+1,j-1} + u_{i+1,j+1} - 20 u_{i,j}]
  \end{split}
  \]

  \begin{itemize}
  \item Implicit Euler time differencing
  \end{itemize}

  \[
  \ve{A} \ve{u}^{n+1} = \ve{u}^n
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Direct vs. Adjoint Analysis}

  \begin{columns}

    \begin{column}{0.5\textwidth}
      \begin{figure}[h!]
        \centering
        \includegraphics[width=2.5in,clip]{Adjoint_Direct_CPU_Time.pdf}
        \caption{\textbf{CPU Time (s) to converge vs. Problem Size
            ($N$ for an $N \times N$ square mesh).} }
      \end{figure}
    \end{column}

    \begin{column}{0.5\textwidth}
      \begin{figure}[h!]
        \centering
        \includegraphics[width=2.5in,clip]{Adjoint_Direct_Iterations.pdf}
        \caption{\textbf{Iterations to converge vs. Problem Size ($N$
            for an $N \times N$ square mesh).} }
      \end{figure}
    \end{column}

  \end{columns}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Direct vs. Adjoint Analysis}

  \begin{figure}[h!]
    \centering
    \includegraphics[width=2.5in,clip]{Adjoint_Direct_Convergence.pdf}
    \caption{\textbf{Infinity norm of the solution residual
        vs. iteration number for a problem of fixed size.} }
  \end{figure}

  \begin{itemize}
  \item CPU time dominating factor in method selection
  \item Significant speedup with adjoint method
  \item Does not affect convergence behavior
  \item Use adjoint with MCSA and Sequential Monte Carlo
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Generalization of MCSA for Linear Problems}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Parallelization of Monte Carlo Methods}

  \begin{itemize}
  \item No literature observed for parallel Neumann-Ulam solvers
  \item Numerous references for modern parallel Monte Carlo methods in
    reactor physics
  \item MCSA parallelism comes from parallel matrix/vector operations
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Domain Decomposition}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Multiple-Set Overlapping-Domain Decomposition}

  \begin{figure}[htpb!]
    \begin{center}
      \scalebox{1.0}{ \input{msod_example.pdftex_t} }
    \end{center}
    \caption{\textbf{Overlapping domain example illustrating how
        domain overlap can reduce communication costs.}}
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Domain-to-Domain Communication}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Load Balancing}

  \begin{figure}[htpb!]
    \begin{center}
      \scalebox{1.0}{ \input{procassini_example.pdftex_t} }
    \end{center}
    \caption{\textbf{Example illustrating how domain decomposition can
        create load balance issues in Monte Carlo.}}
    \label{fig:procassini_example}
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Reproducability}

  \begin{figure}[htpb!]
    \begin{center}
      \scalebox{1.0}{ \input{gentile_example.pdftex_t} }
    \end{center}
    \caption{\textbf{Gentile's example illustrating how domain
        decomposition can create reproducibility issues in Monte
        Carlo.}}
    \label{fig:gentile_example}
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Parallel Adjoint Method}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Parallel MCSA}

  \begin{beamerboxesrounded}[upper=boxheadcolor,lower=boxbodycolor,shadow=true]
    {MCSA Iteration}

    \[
    \ve{x}^{k+1/2} = (\ve{I} - \ve{A})\ve{x}^k + \ve{b}
    \]
    \[
    \ve{r}^{k+1/2} = \ve{b} - \ve{A}\ve{x}^{k+1/2}
    \]
    \[
    \hat{\ve{A}}\delta\ve{x}^{k+1/2} = \ve{r}^{k+1/2}
    \]
    \[
    \ve{x}^{k+1} = \ve{x}^{k+1/2} + \delta \ve{x}^{k+1/2}
    \]

  \end{beamerboxesrounded}

  \begin{itemize}
  \item Richardson iteration and residual computation require parallel
    matrix-vector multiply and parallel vector update
  \item This work will generate a parallel adjoint Neumann-Ulam solver
  \item Application of correction requires parallel vector update
  \item Convergence checks through parallel norm computation
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Monte Carlo Solution Methods for Nonlinear Problems}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Nonlinear Preliminaries}

  \begin{itemize}
  \item We seek solutions of the general nonlinear problem
  \end{itemize}

  \[
  \ve{F}(\ve{u}) = \ve{0}
  \]
  \[
  \ve{u} \in \mathbb{R}^n,\ \ve{F}:\mathbb{R}^N \rightarrow
  \mathbb{R}^N
  \]

  \begin{itemize}
  \item We interpret the exact solution $\ve{u}$ to be the roots of
    $\ve{F}(\ve{u})$
  \item Taylor expand the residuals at the $k+1$ iterate about the $k$
    iterate
  \end{itemize}

  \[
  \ve{F}(\ve{u}^{k+1}) = \ve{F}(\ve{u}^{k}) +
  \ve{F}'(\ve{u}^{k})(\ve{u}^{k+1}-\ve{u}^{k}) +
  \frac{\ve{F}''(\ve{u}^{k})}{2}(\ve{u}^{k+1}-\ve{u}^{k})^2 + \cdots
  \]

  \begin{itemize}
  \item Assert $\ve{u}^{k+1}$ is the exact solution
  \end{itemize}

  \[
  -\ve{F}(\ve{u}^{k}) = \ve{F}'(\ve{u}^{k})(\ve{u}^{k+1}-\ve{u}^{k})
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Nonlinear Preliminaries}

  \begin{itemize}
  \item $\ve{F}'(\ve{u}^{k})$ is the \textit{Jacobian}
    $\ve{J}(\ve{u})$
  \end{itemize}

  \[
  J_{ij} = \frac{\partial F_i(\ve{u})}{\partial u_j}
  \]

  \begin{itemize}
  \item $(\ve{u}^{k+1}-\ve{u}^{k})$ is the solution update from the
    $k$ iterate to the $k+1$ iterate
  \end{itemize}

  \[
  \delta \ve{u}^k = \ve{u}^{k+1}-\ve{u}^{k}
  \]

  \begin{itemize}
  \item Form Newton's method
  \end{itemize}
  \[
  \ve{J}(\ve{u}) \delta \ve{u}^k = -\ve{F}(\ve{u}^{k})
  \]
  \[
  \ve{u}^{k+1} = \ve{u}^k + \delta \ve{u}^k
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Newton-Krylov Methods}

  \begin{itemize}
  \item A form of inexact Newton methods
  \end{itemize}

  \[
  ||\ve{J}(\ve{u}^k)\delta \ve{u}^k + \ve{F}(\ve{u}^k)|| \leq \eta^k
  ||\ve{F}(\ve{u}^k)||,\ \eta^k \in [0,1)
    \]

    \begin{itemize}
    \item Choose a Krylov method to solve for the Newton correction
    \item GMRES with a long recurrence relation obeserved as more
      robust
    \item Generates a monotonically decreasing residual from
      maintaining the optimization and orthogonality conditions
    \end{itemize}

    \pause Where does the Jacobian come from?  \pause
    \begin{itemize}
    \item The Jacobian can come from hand-coded derivatives
      \begin{itemize}
      \item Tedious and error prone
      \item Repeated for each equation set and hard to do for
        multiphysics
      \end{itemize}
    \end{itemize} 

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Matrix-Free Approximation}

  
  \begin{itemize}
  \item Krylov methods only need the action of the linear operator
  \item The action of the Jacobian can be approximated using a forward
    difference
  \end{itemize}

  \[
  \ve{J}(\ve{u})\ve{v} = \frac{\ve{F}(\ve{u} + \epsilon \ve{v}) -
    \ve{F}(\ve{u})}{\epsilon}
  \]

  \begin{itemize}
  \item Forms the basis of Jacobian-Free Newton-Krylov (JFNK) methods
    \begin{itemize}
    \item Sensitive to scaling and discretization error
    \item Still have to form part of Jacobian periodically for
      preconditioning
    \item Eventually break even with generating and storing the full
      Jacobian
    \end{itemize}
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Automatic Differentiation}

  \begin{itemize}
  \item Automatically generate Jacobians from nonlinear function
    evaluations
    \begin{itemize}
    \item Overload math operators and apply the chain rule (FAD)
    \item Yields evaluations as accurate as function discretization
    \end{itemize}
  \item Modern packages take an element-level assembly approach
    \begin{itemize}
    \item Function evaluations are local, communication builds global
      data
    \end{itemize}
  \end{itemize}

  \[
  \ve{J}(\ve{u}) = \sum_{i=1}^N \ve{Q}^T_i \ve{J}_k \ve{P}_i
  \]
  \[
  e_k : \mathbb{R}^{n_k} \rightarrow \mathbb{R}^{m_k},\ \ve{J}_{k_i} =
  \partial e_{k_i} / \partial P_i u,\ \ve{P} \in \mathbb{R}^{m_{k_i}
    \times N}
  \]

  \begin{itemize}
  \item Performance studies give acceptable results for use in
    large-scale, production physics codes
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}[fragile]{The FANM Method}

  Forward-Automated Newton-MCSA

  \pause
  \begin{algorithm}[H]
    \begin{algorithmic}[1]
      \STATE $k := 0$ \WHILE{$||\ve{F}(\ve{u}^{k})|| > \epsilon
        ||\ve{F}(\ve{u}^{0})||$} \STATE $\ve{J}(\ve{u}^{k}) \leftarrow
      AD(\ve{F}(\ve{u}^k))$ \COMMENT{Automatic differentation} \STATE
      $\ve{J}(\ve{u}^k) \delta \ve{u}^k = -\ve{F}(\ve{u}^{k})$
      \COMMENT{Solve for the Newton correction with MCSA} \STATE
      $\ve{u}^{k+1} \leftarrow \ve{u}^k + \delta \ve{u}^k$ \STATE $k
      \leftarrow k+1$ \ENDWHILE
    \end{algorithmic}
    \caption{FANM}
  \end{algorithm}

  \pause
  \begin{itemize}
  \item Robustness of Newton's method (inexact)
  \item Accuracy and convenience of FAD
  \item Parallelism, memory, and resiliency benefits of MCSA
  \item Requires only nonlinear function evaluations
  \end{itemize}


\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Jacobian Storage vs. Subspace Storage}

  \begin{columns}

    \begin{column}{0.5\textwidth}
      \begin{itemize}
      \item Jacobian will be in a compressed row storage format
      \end{itemize}
    \end{column}

    \begin{column}{0.5\textwidth}
      \small{\[ \ve{A} =
        \begin{bmatrix}
          2 & 0 & 8 & 0 & 0 & 0 \\ 4 & 5 & 0 & 1 & 0 & 0 \\ 0 & 2 & 1
          & 0 & 1 & 0 \\ 0 & 0 & 3 & 7 & 0 & 2 \\ 0 & 0 & 0 & 4 & 9 &
          0 \\ 0 & 0 & 0 & 0 & 9 & 1 \\
        \end{bmatrix}      
        \]}
    \end{column}

  \end{columns}

  \pause \small{ \[
    \begin{array}{lccccccccccccccc}
      \ve{A}\ values: & 2 & 8 & 4 & 5 & 1 & 2 & 1 & 1 & 3 & 7 & 2 & 4
      & 9 & 9 & 1 \\ column: & 1 & 3 & 1 & 2 & 4 & 2 & 3 & 5 & 3 & 4 &
      6 & 4 & 5 & 5 & 6 \\ row\ start: & 1 & 3 & 6 & 9 & 12 & 14 & 16
      \\
    \end{array}
    \]}

  \pause
  \begin{itemize}
  \item $m$ Krylov iterations require $(m+1)$ subspace vectors
  \item Storage requirement is $\big[(m+1) N\big]$ for $\ve{x} \in
    \mathbb{R}^{N \times N}$
  \end{itemize}

  \pause
  \begin{beamerboxesrounded}[upper=boxheadcolor,lower=boxbodycolor,shadow=true]
    {If we need 10 Krylov iterations to converge...}
    \begin{itemize}
    \item 66 elements required for subspace vectors
    \item 72 total elements required for Jacobian and probability
      matrix
    \end{itemize}
  \end{beamerboxesrounded}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Parallel FANM Method}

  \begin{algorithm}[H]
    \begin{algorithmic}[1]
      \STATE $k := 0$ \WHILE{$||\ve{F}(\ve{u}^{k})|| > \epsilon
        ||\ve{F}(\ve{u}^{0})||$} \STATE $\ve{J}(\ve{u}^{k}) \leftarrow
      AD(\ve{F}(\ve{u}^k))$ \COMMENT{Automatic differentation} \STATE
      $\ve{J}(\ve{u}^k) \delta \ve{u}^k = -\ve{F}(\ve{u}^{k})$
      \COMMENT{Solve for the Newton correction with MCSA} \STATE
      $\ve{u}^{k+1} \leftarrow \ve{u}^k + \delta \ve{u}^k$ \STATE $k
      \leftarrow k+1$ \ENDWHILE
    \end{algorithmic}
    \caption{FANM}
  \end{algorithm}

  \begin{itemize}
  \item Modern FAD packages are parallelized using element-level
    assembly
  \item This work will generate a parallel MCSA solver
  \item Application of the Newton correction requires a parallel
    vector update
  \item Convergence checks through parallel norm comptutation
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Research Proposal}

  \begin{itemize}
  \item Experimental framework
  \item Methods verification
  \item Numerical experiments
  \item Challenge problem
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Experimental Framework}

  \begin{itemize}
  \item Need a framework to generate linear and nonlinear systems in
    parallel
    \begin{itemize}
    \item Linear algebra data structures
    \item Parallel communication framework
    \item Mesh generation and partitioning
    \item Automatic differentiation and more...
    \end{itemize}
  \item Choose the finite element method to leverage a finite element
    assembly engine
  \end{itemize}

  \[
  \ve{A}(\ve{u}) = \ve{0},\ \forall \ve{u} \in \Omega
  \]
  \[
  \ve{B}(\ve{u}) = \ve{0},\ \forall \ve{u} \in \Gamma
  \]
  \[
  \int_{\Omega} \ve{v}^T \ve{A}(\ve{u}) d\Omega + \int_{\Gamma}
  \bar{\ve{v}}^T \ve{B}(\ve{u}) d\Gamma = \ve{0}
  \]
  \[
  \int_{\Omega} \ve{C}(\ve{v}^T) \ve{D}(\ve{u}) d\Omega +
  \int_{\Gamma} \ve{E}(\bar{\ve{v}}^T) \ve{F}(\ve{u}) d\Gamma = \ve{0}
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Monte Carlo Methods Verification}

  \begin{itemize}
  \item Analytic solution to the heat equation for the linear methods
  \item Sequence of Navier-Stokes benchmarks for the nonlinear methods
    \begin{itemize}
    \item Thermal convection cavity problem (De Vahl Davis, 1983)
    \item Lid driven cavity problem (Ghia et al., 1982)
    \item Backward-Facing step problem (Gartling, 1990)
    \end{itemize}
  \item Tuning benchmark parameters varies the strength of
    nonlinearities
  \end{itemize}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      \[
      \rho \ve{u} \cdot \nabla \ve{u} - \nabla \cdot \ve{T} - \rho
      \ve{g} = \ve{0}
      \]
      \[
      \nabla \cdot \ve{u} = 0
      \]
      \[
      \rho C_p \ve{u} \cdot \nabla T + \nabla \cdot \ve{q} = 0
      \]
    \end{column}

    \begin{column}{0.5\textwidth}
      \[
      \ve{T} = -P \ve{I} + \mu[\nabla \ve{u} + \nabla \ve{u}^T]
      \]
      \[
      \ve{q} = - k \nabla T
      \]
    \end{column}
  \end{columns}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Proposed Numerical Experiments}

  Domain Overlap Studies for the Parallel Neumann-Ulam Method


  Domain Replication Studies for the Parallel Neumann-Ulam Method

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Proposed Numerical Experiments}

  Parallel Performance and Numerical Accuracy Studies for the MCSA
  Method

  Parallel Performance and Numerical Accuracy Studies for the FANM
  Method

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Proposed Challenge Problem}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Conclusion}

\end{frame}

%%---------------------------------------------------------------------------%%

\end{document}
