\documentclass{beamer}
\usetheme[white]{Wisconsin}
\usepackage{longtable}
\usepackage{listings}
\usepackage{color}
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm} \usepackage{amsmath} \usepackage{tmadd,tmath}
\usepackage[mathcal]{euscript} \usepackage{color}
\usepackage{textcomp}
\usepackage{algorithm,algorithmic}
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\lstset{
  backgroundcolor=\color{lbcolor},
  tabsize=4,
  rulecolor=,
  language=c++,
  basicstyle=\scriptsize,
  upquote=true,
  aboveskip={1.5\baselineskip},
  columns=fixed,
  showstringspaces=false,
  extendedchars=true,
  breaklines=true,
  prebreak =
  \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
  frame=single,
  showtabs=false,
  showspaces=false,
  showstringspaces=false,
  identifierstyle=\ttfamily,
  keywordstyle=\color[rgb]{0,0,1},
  commentstyle=\color[rgb]{0.133,0.545,0.133},
  stringstyle=\color[rgb]{0.627,0.126,0.941},
}

%% colors
\setbeamercolor{boxheadcolor}{fg=white,bg=UWRed}
\setbeamercolor{boxbodycolor}{fg=black,bg=white}


%%---------------------------------------------------------------------------%%
\author{Stuart R. Slattery
  \\ Engineering Physics Department
  \\ University of Wisconsin - Madison
}

\date{\today} 
\title{Massively Parallel Monte Carlo Methods for Discrete Linear and
  Nonlinear Systems} 
\begin{document}
\maketitle

%%---------------------------------------------------------------------------%%
\begin{frame}{Introduction}

  \begin{itemize}
    \item Predictive modeling and simulation enhances engineering
      capability
    \item Modern work focused on this task leverages multiple physics
      simulation (CASL, NEAMS)
    \item New hardware drives algorithm development (petascale and
      exascale)
    \item Monte Carlo methods have the potential to provide great
      improvements that permit finer simulations and better mapping to
      future hardware
    \item A set of massively parallel Monte Carlo methods is proposed
      to advance multiple physics simulation on contemporary and
      future leadership class machines
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Physics-Based Motivation}
 
  \pause 
  \begin{beamerboxesrounded}[upper=boxheadcolor,lower=boxbodycolor,shadow=true]
    {Predictive nuclear reactor analysis enables...}
    \begin{itemize}
    \item Tighter design tolerance for improved thermal performance
      and efficiency
    \item Higher fuel burn-up
    \item High confidence in accident scenario models
    \end{itemize}
  \end{beamerboxesrounded}

  \pause 
  \begin{beamerboxesrounded}[upper=boxheadcolor,lower=boxbodycolor,shadow=true]
  {Multiple physics simulations are complicated...}
    \begin{itemize}
    \item Neutronics, thermal hydraulics, computational fluid
      dynamics, structural mechanics, and many other physics
    \item Consistent models yield nonlinearities in the variables
      through feedback effects
    \item Tremendous computational resources are required with
      $O(\sn{1}{9})$ element meshes and $O(100,000)+$ cores used in
      today's simulations.
    \end{itemize}
  \end{beamerboxesrounded}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Physics-Based Motivation: DNB}

  \begin{columns}

    \begin{column}{0.35\textwidth}
      \begin{figure}[htpb!]
        \begin{center}
          \scalebox{1}{ \input{dnb_schematic.pdftex_t} }
        \end{center}
        \caption{\textbf{Departure from nucleate boiling scenario.} }
      \end{figure}
    \end{column}

    \pause

    \begin{column}{0.65\textwidth}
      \begin{figure}[htpb!]
        \begin{center}
          \scalebox{0.8}{ \input{dnb_example.pdftex_t} }
        \end{center}
        \caption{\textbf{Multiphysics dependency analysis of departure
            from nucleate boiling.} }
      \end{figure}
    \end{column}

  \end{columns}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Hardware-Based Motivation}

  \begin{itemize}
  \item Modern hardware is moving in two directions:
    \begin{itemize}
    \item Lightweight machines
    \item Heterogeneous machines
    \item Both characterized by low power and high concurrency
    \end{itemize}
  \item Some issues:
    \begin{itemize}
    \item Higher potential for both soft and hard failures
    \item Memory restrictions are expected with a continued decrease
      in memory/FLOPS
    \end{itemize}
  \item Potential resolution from Monte Carlo:
    \begin{itemize}
    \item Soft failures buried within the tally variance
    \item Hard failures are high variance events
    \item Memory savings over conventional methods
    \end{itemize}
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Research Outline}
  \begin{itemize}
    \item Parallelization of Monte Carlo methods for discrete systems
      \begin{itemize}
      \item Parallel strategies taken from modern reactor physics
        methods
      \item Research is required to explore varying parallel
        strategies
      \item Scalability is of concern
      \end{itemize}
    \item Development of a nonlinear solver leveraging Monte Carlo
      \begin{itemize}
        \item Application to nonlinear problems of interest
        \item Memory benefits
        \item Performance benefits
      \end{itemize}
  \end{itemize}
\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Linear Operator Equations}

  \begin{itemize}
  \item We seek solutions of the general linear operator equation
  \end{itemize}

  \[
  \ve{A} \ve{x} = \ve{b}\:
  \]
  \[
  \ve{A} \in \mathbb{R}^{N \times N},\ \ve{A} : \mathbb{R}^{N}
  \rightarrow \mathbb{R}^{N},\ \ve{x} \in \mathbb{R}^N,\ \ve{b} \in
  \mathbb{R}^N\:
  \]


  \[
  \ve{r} = \ve{b} - \ve{A}\ve{x}\:
  \]

  \begin{itemize}
  \item $\ve{r}=\ve{0}$ when an exact solution is found.
  \end{itemize}

  \pause
  \begin{beamerboxesrounded}[upper=boxheadcolor,lower=boxbodycolor,shadow=true]
    {A Requirement}
    Assert that $\ve{A}$ is \textit{nonsingular}. The solution is then:
    \[
    \ve{x} = \ve{A}^{-1}\ve{b}\:
    \]
  \end{beamerboxesrounded}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Stationary Methods}

  \begin{itemize}
  \item General stationary methods are formed by splitting the linear
    operator
  \end{itemize}

  \[
  \ve{A} = \ve{M} - \ve{N}\:.
  \]

  \[
  \ve{x} = \ve{M}^{-1}\ve{N}\ve{x} + \ve{M}^{-1}\ve{b}\:.
  \]

  \begin{itemize}
  \item We identify $\ve{H} =\ve{M}^{-1}\ve{N}$ as the
    \textit{iteration matrix}
  \end{itemize}

  \[
  \ve{x}^{k+1} = \ve{H}\ve{x}^{k} + \ve{c}\:.
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Stationary Methods Convergence}

  \begin{itemize}
  \item The qualities of the iteration matrix dictate convergence 
  \item Define $\ve{e}^k = \ve{x}^k-\ve{x}$ as the error at the
    $k^{th}$ iterate
  \end{itemize}

  \[
  \ve{e}^{k+1} = \ve{H} \ve{e}^k\:
  \]


  \begin{itemize}
  \item We diagonalize $\ve{H}$ to extract its Eigenvalues
  \end{itemize}

  \[
  ||\ve{e}^{k}||_2 = \rho(\ve{H})^k ||\ve{e}^0||_2\:,
  \]

  \begin{itemize}
  \item We bound $\ve{H}$ by $\rho(\ve{H}) < 1$ for convergence
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Projection Methods}

  \begin{itemize}
    \item Powerful class of iterative methods
    \item Provides theory that encapsulates most other iterative
      methods 
    \item Leveraged in many modern physics codes at the petascale
  \end{itemize}

  \pause
  \begin{beamerboxesrounded}[upper=boxheadcolor,lower=boxbodycolor,shadow=true]
    {Search Subspace $\mathcal{K}$} 
    Extract the solution from the search subspace:
    \[
    \tilde{\ve{x}} = \ve{x}_0 +
    \boldsymbol{\delta},\ \boldsymbol{\delta} \in \mathcal{K}\:
    \]
  \end{beamerboxesrounded}

  \pause
  \begin{beamerboxesrounded}[upper=boxheadcolor,lower=boxbodycolor,shadow=true]
    {Constraint Subspace $\mathcal{L}$} 
    Constrain the extraction with the constraint subspace by asserting
    orthogonality with the residual:
    \[
    \langle \tilde{\ve{r}},\ve{w} \rangle = 0,\ \forall \ve{w} \in
    \mathcal{L}\:
    \]  
  \end{beamerboxesrounded}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{The Orthogonality Constraint}

  \[
  \tilde{\ve{r}} = \ve{r}_0 - \ve{A}\boldsymbol{\delta}
  \]

  \begin{figure}[htpb!]
    \begin{center}
      \scalebox{1.25}{
        \input{orthogonal_residual.pdftex_t} }
    \end{center}
    \caption{\textbf{Orthogonality constraint of the new residual with
        respect to $\mathcal{L}$.} }
  \end{figure}

  \pause
  \begin{beamerboxesrounded}[upper=boxheadcolor,lower=boxbodycolor,shadow=true]
    {Minimization Property}

    The residual of the system will always be \textit{minimized} with
    respect to the constraints
    \[
    ||\tilde{\ve{r}}||_2 \leq ||\ve{r}_0||_2,\ \forall \ve{r}_0 \in
    \mathbb{R}^N\:,
    \]
  \end{beamerboxesrounded}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Putting it All Together}

  \begin{itemize}
  \item Choose $\ve{V}$ as a basis of $\mathcal{K}$ and $\ve{W}$ as a
    basis of $\mathcal{L}$
  \end{itemize}

  \[
  \boldsymbol{\delta} = \ve{V}\ve{y},\ \forall \ve{y} \in
  \mathbb{R}^N
  \]

  \[
  \ve{y} = (\ve{W}^T\ve{A}\ve{V})^{-1}\ve{W}^T\ve{r}_0
  \]

  \pause
  \begin{beamerboxesrounded}[upper=boxheadcolor,lower=boxbodycolor,shadow=true]
    {Projection Method Iteration}
  \[
  \ve{r}^k = \ve{b} - \ve{A}\ve{x}^k
  \]
  \[
  \ve{y}^k = (\ve{W}^T\ve{A}\ve{V})^{-1}\ve{W}^T\ve{r}^k
  \]
  \[
  \ve{x}^{k+1} = \ve{x}^k + \ve{V}\ve{y}^k\:
  \]
  \[
  Update\ \ve{V}\ and\ \ve{W}
  \]
  \end{beamerboxesrounded}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Krylov Subspace Methods}

  \[
  \mathcal{K}_m(\ve{A},\ve{r}_0) = span\{\ve{r}_0, \ve{A}\ve{r}_0,
  \ve{A}^2\ve{r}_0, \dots, \ve{A}^{m-1}\ve{r}_0\}
  \]
  \[
  \mathcal{L} = \ve{A} \mathcal{K}_m(\ve{A},\ve{r}_0)
  \]

  \begin{itemize}
  \item Yields the normal system $\ve{A}^T\ve{A}\ve{x} =
    \ve{A}^T\ve{b}$
  \item Must generate an orthonormal basis $\ve{V}_m \in \mathbb{R}^{N
    \times m}$ for $\mathcal{K}_m(\ve{A},\ve{r}_0)$
  \item $\ve{W}_m = \ve{A}\ve{V}_m$
  \item Typically choose a Gram-Schmidt-like procedure such as
    Arnoldi or Lanzcos
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}[fragile]{GMRES}

\begin{algorithm}[H]
  \begin{algorithmic}[1]
    \STATE $\ve{r}_0 := \ve{b}-\ve{A}\ve{x}_0$
    \STATE $\beta := ||\ve{r}_0||_2$
    \STATE $\ve{v}_1 := \ve{r}_0 / \beta$
    \COMMENT{Create the orthonormal basis for the Krylov subspace}
    \FOR{$j = 1, 2, \cdots, m$}
    \STATE $\ve{w}_j := \ve{A}\ve{v}_j$
    \FOR{$i = 1, 2, \cdots, j$}
    \STATE $h_{ij} \leftarrow \langle \ve{w}_j,\ve{v}_j \rangle$
    \STATE $\ve{w}_j \leftarrow \ve{w}_j - h_{ij}\ve{v}_i$
    \ENDFOR
    \STATE $h_{j+1,j} \leftarrow ||\ve{w}_j||_2$
    \STATE $\ve{v}_{j+1} \leftarrow \ve{w}_j / h_{j+1,j}$
    \ENDFOR
    \COMMENT{Apply the orthogonality constraints}
    \STATE $\ve{y}_m \leftarrow argmin_y ||\beta \ve{e}_1 - \ve{H}_m\
    \ve{y}||_2 $
    \STATE $\ve{x}_m \leftarrow \ve{x}_0 + \ve{V_m} \ve{y}_m$
  \end{algorithmic}
  \caption{GMRES Iteration}
\end{algorithm}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Parallel Projection Methods}
  
  \begin{itemize}
  \item Parallel vector update
  \end{itemize}
  \[
  \ve{y}[n] \leftarrow \ve{y}[n] + a * \ve{x}[n],\ \forall n \in [1,N_g]
  \]
  \[
  \ve{y}[n] \leftarrow \ve{y}[n] + a * \ve{x}[n],\ \forall n \in [1,N_l]
  \]
  
  \begin{itemize}
  \item Parallel dot product
  \end{itemize}
  \[
  d_l = \ve{y}_l \cdot \ve{x}_l,\ d_g = \sum_p d_l
  \]

  \begin{itemize}
  \item Parallel vector norm
  \end{itemize}
  \[
  ||x||_{\infty,l} = \max_n \ve{y}[n],\ \forall n \in [1,N_l]
  \]
  \[
  ||x||_{\infty,g} = \max_p ||x||_{\infty,l}
  \]
  
\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Parallel Matrix-Vector Multiplication}

\begin{figure}[htpb!]
  \begin{center}
    \scalebox{0.75}{
      \input{partitioned_matrix.pdftex_t} }
  \end{center}
  \caption{\textbf{Matrix-vector multiply $\ve{A}\ve{x}=\ve{y}$
      operation on 3 processors.}
  \label{fig:partitioned_matvec_multiply} }
\end{figure}

\pause
\begin{figure}[htpb!]
  \begin{center}
    \scalebox{0.75}{
      \input{matvec_proc_1.pdftex_t} }
  \end{center}
  \caption{\textbf{Components of multiply operation owned by process
      1.} }
  \label{fig:matvec_proc_1}
\end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Projection Method Notes}

\begin{itemize}
  \item Global reduction operations observed not to impede scalability
    \begin{itemize}
    \item Dot product
    \item Vector norms
    \end{itemize}
  \item Nearest neighbor computations have poor algorithmic strong
    scaling
    \begin{itemize}
      \item Matrix-vector multiply
      \item Weak scaling is better
    \end{itemize}
   
\end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Monte Carlo Methods for Discrete Linear Systems}

  \begin{itemize}
    \item First proposed by J. Von Neumann and S.M. Ulam in the 1940's
    \item Earliest published reference in 1950
    \item General lack of published work
    \item Modern work by Evans and others has yielded new applications
  \end{itemize}
\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Monte Carlo Linear Solver Preliminaries}

  \begin{itemize}
  \item Split the operator
  \end{itemize}

  \[
  \ve{H} = \ve{I} - \ve{A}
  \]

  \[
  \ve{x} = \ve{H} \ve{x} + \ve{b}
  \]

  \begin{itemize}
  \item Generate the \textit{Neumann series}
  \end{itemize}
  
  \[
  \ve{A}^{-1} = (\ve{I}-\ve{H})^{-1} = \sum_{k=0}^{\infty} \ve{H}^k
  \]

  \begin{itemize}
  \item Require $\rho(\ve{H}) < 1$ for convergence
  \end{itemize}

  \[
  \ve{A}^{-1}\ve{b} = \sum_{k=0}^{\infty} \ve{H}^k\ve{b} = \ve{x}
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Monte Carlo Linear Solver Preliminaries}

  \begin{itemize}
  \item Expand the Nuemann series
  \end{itemize}

  \[
  x_i = \sum_{k=0}^{\infty}\sum_{i_1}^{N}\sum_{i_2}^{N}\ldots
  \sum_{i_k}^{N}h_{i,i_1}h_{i_1,i_2}\ldots h_{i_{k-1},i_k}b_{i_k}
  \]

  \begin{itemize}
  \item Define a sequence of state transitions
  \end{itemize}
  
  \[
  \nu = i \rightarrow i_1 \rightarrow \cdots \rightarrow i_{k-1}
  \rightarrow i_{k}
  \]

  \begin{itemize}
  \item Define the \textit{Neumann-Ulam decomposition}\footnote{The
    Hadamard product $\ve{A} = \ve{B} \circ \ve{C}$ is defined
    element-wise as $a_{ij} = b_{ij} c_{ij}$.}
  \end{itemize}

  \[
  \ve{H} = \ve{P} \circ \ve{W}
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Direct Method}

  \begin{itemize}
    \item Compute row-normalized transition probabilities and weights
  \end{itemize}

  \[
  p_{ij} = \frac{|h_{ij}|}{\sum_j |h_{ij}|},\ w_{ij} =
  \frac{h_{ij}}{p_{ij}}
  \]

  \begin{itemize}
    \item Generate an expectation value for the solution
  \end{itemize}

  \[
  W_{m} = \sum_{m=0}^k w_{i,i_1} w_{i_1,i_2} \cdots w_{i_{m-1},i_m}
  \]
  \[
  X_{\nu}(i_0 = i) = \sum_{m=0}^k W_{m} b_{i_m}
  \]
\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Direct Method}

  \begin{itemize}
    \item Compute the probability of a particular random walk permutation
  \end{itemize}

  \[
  P_{\nu} = p_{i,i_1} p_{i_1,i_2} \cdots p_{i_{k-1},i_k}
  \]

  \begin{itemize}
  \item Generate the estimator
  \end{itemize}

  \[
  E\{X(i_0 = i)\} = \sum_{\nu} P_{\nu} X_{\nu}
  \]

  \begin{itemize}
  \item Check that we recover the exact solution
  \end{itemize}

  \[
  \begin{split}
    E\{X(i_0 = i)\}
    &=\sum_{k=0}^{\infty}\sum_{i_1}^{N}\sum_{i_2}^{N}\ldots
    \sum_{i_k}^{N} p_{i,i_1}p_{i_1,i_2}\ldots p_{i_{k-1},i_k}
    w_{i,i_1}w_{i_1,i_2}\ldots w_{i_{k-1},i_k} b_{i_k}\\ &= x_i\:,
  \end{split}
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Direct Method: Evolution of a Solution}

  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=4in]{direct_1.png}
    \end{center}
    \caption{\textbf{Direct solution to Poisson Equation.} \textit{1
        history per state, 2500 total histories. 0.785 seconds CPU
        time} }
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Adjoint Method}

  \begin{itemize}
  \item Solve the adjoint linear system
  \end{itemize}

  \[
  \ve{A}^T \ve{y} = \ve{d}
  \]

  \[
  \ve{y} = \ve{H}^T \ve{y} + \ve{d}
  \]

  \begin{itemize}
  \item Set the adjoint constraint
  \end{itemize}

  \[
  \langle \ve{A}^T \ve{x}, \ve{y} \rangle = \langle \ve{x}, \ve{A}
  \ve{y} \rangle
  \]

  \[
  \langle \ve{x}, \ve{d} \rangle = \langle \ve{y}, \ve{b} \rangle
  \]
  
\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Adjoint Method}

  \begin{itemize}
  \item Generate the Neumann series for the adjoint operator
  \end{itemize}

  \[
  \ve{y} = (\ve{I} - \ve{H}^T)^{-1} \ve{d}
  \]

  \[
  \ve{y} = \sum_{k=0}^{\infty} (\ve{H}^T)^k\ve{d}
  \]

  \begin{itemize}
  \item Expand the series
  \end{itemize}

  \[
  y_i = \sum_{k=0}^{\infty}\sum_{i_1}^{N}\sum_{i_2}^{N}\ldots
  \sum_{i_k}^{N}h_{i_k,i_{k-1}}\ldots h_{i_2,i_1} h_{i_1,i} d_{i_k}
  \]

  \begin{itemize}
  \item Pick another constraint to yield the original solution
  \end{itemize}

  \[
  \ve{d} = \boldsymbol{\delta}_i,\ \langle \ve{y}, \ve{b} \rangle =
  \langle \ve{x}, \boldsymbol{\delta}_i \rangle = x_i
  \]
  
\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Adjoint Method}

  \begin{itemize}
  \item Use the adjoint Neumann-Ulam decomposition
  \end{itemize}

  \[
  \ve{H}^{T} = \ve{P} \circ \ve{W}
  \]

  \[
  p_{ij} = \frac{|h_{ji}|}{\sum_j |h_{ji}|},\ w_{ij} =
  \frac{h_{ji}}{p_{ij}}
  \]

  \begin{itemize}
  \item Build the estimator and expectation value
  \end{itemize}

  \[
  X_{\nu} = \sum_{m=0}^k W_{m} b_{i_0} \delta_{i,i_m}
  \]

  \[
  \begin{split}
    E\{X_j\} &=\sum_{k=0}^{\infty}\sum_{i_1}^{N}\sum_{i_2}^{N}\ldots
    \sum_{i_k}^{N} b_{i_0} h_{i,i_1}h_{i_1,i_2}\ldots h_{i_{k-1},i_k}
    \delta_{i_k,j} \\ &= x_{j}\:,
  \end{split}
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Adjoint Method: Evolution of a Solution}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Sequential Monte Carlo}

  \begin{itemize}
  \item Neumann-Ulam methods bound by the Central Limit Theorem
  \item Halton proposed an iterative residual method
  \item Iteration error decoupled from Monte Carlo error
  \item Exponential convergence
  \end{itemize}

  \[
  \ve{r}^k = \ve{b} - \ve{A}\ve{x}^k
  \]
  \[
  \ve{A}\boldsymbol{\delta}^{k} = \ve{r}^{k}
  \]
  \[
  \ve{x}^{k+1} = \ve{x}^k + \boldsymbol{\delta}^{k}
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Monte Carlo Synthetic-Acceleration}
  \begin{itemize}
  \item Split the operator to yield Richardson's iteration
  \end{itemize}

  \[
  \ve{x} = (\ve{I} - \ve{A})\ve{x} + \ve{b}
  \]
  \[
  \ve{x}^{k+1} = (\ve{I} - \ve{A})\ve{x}^k + \ve{b}
  \]

  \begin{itemize}
  \item Define the iteration error
  \end{itemize}

  \[
  \delta \ve{x}^k = \ve{x} - \ve{x}^k
  \]
  \[
  \delta \ve{x}^{k+1} = (\ve{I} - \ve{A})\delta \ve{x}^k
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Monte Carlo Synthetic-Acceleration}

  \begin{itemize}
  \item Subtract $(\ve{I} - \ve{A})\delta \ve{x}^{k+1}$
  \end{itemize}

  \[
  \begin{split}
    \ve{A}\delta \ve{x}^{k+1} &= (\ve{I} -
    \ve{A})(\ve{x}^{k+1}-\ve{x}^{k}) \\ &= \ve{r}^{k+1}
  \end{split}
  \]

  \begin{itemize}
  \item The following converges in one iteration with exact inversion
    of $\ve{A}$:
  \end{itemize}

  \[
  \ve{x}^{k+1} = (\ve{I} - \ve{A})\ve{x}^k + \ve{b}
  \]
  \[
  \ve{A} \delta \ve{x}^{k+1} = \ve{r}^{k+1}
  \]
  \[
  \ve{x} = \ve{x}^{k+1} + \delta \ve{x}^{k+1}
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Monte Carlo Synthetic-Acceleration}

  \begin{beamerboxesrounded}[upper=boxheadcolor,lower=boxbodycolor,shadow=true]
    {MCSA Iteration}

    \[
    \ve{x}^{k+1/2} = (\ve{I} - \ve{A})\ve{x}^k + \ve{b}
    \]
    \[
    \ve{r}^{k+1/2} = \ve{b} - \ve{A}\ve{x}^{k+1/2}
    \]
    \[
    \hat{\ve{A}}\delta\ve{x}^{k+1/2} = \ve{r}^{k+1/2}
    \]
    \[
    \ve{x}^{k+1} = \ve{x}^{k+1/2} + \delta \ve{x}^{k+1/2}
    \]

  \end{beamerboxesrounded}

  \begin{itemize}
  \item Adjoint Neumann-Ulam solver computes the correction
  \item Decouples Monte Carlo error from solution error
  \item Exponential convergence
  \item Demonstrated by Evans and colleagues to be competitive with
    Krylov methods
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Preconditioning Monte Carlo Methods}

  \begin{itemize}
    \item No symmetry requirements
    \item Require $\rho(\ve{H}) < 1$
    \item Choose Jacobi preconditioning at a minimum
  \end{itemize}

  \[
  \ve{M} = diag(\ve{A})
  \]
  \[
  \ve{M}^{-1}\ve{A}\ve{x} = \ve{M}^{-1}\ve{b}
  \]
  
  \begin{itemize}
  \item Yields a preconditioned MCSA iteration with no in-state
    transitions
  \end{itemize}

  \[
  \ve{x}^{k+1/2} = (\ve{I} - \ve{M}^{-1}\ve{A})\ve{x}^k + \ve{b}
  \]
  \[
  \ve{r}^{k+1/2} = \ve{b} - \ve{M}^{-1}\ve{A}\ve{x}^{k+1/2}
  \]
  \[
  \ve{M}^{-1}\ve{A}\delta\ve{x}^{k+1/2} = \ve{r}^{k+1/2}
  \]
  \[
  \ve{x}^{k+1} = \ve{x}^{k+1/2} + \delta \ve{x}^{k+1/2}
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Direct vs. Adjoint Analysis}

  \begin{itemize}
  \item Analysis needed to select Monte Carlo method
  \item Time-dependent 2-dimensional Poisson equation
  \item Spectral radius fixed
  \item Sparsity varied with 2 Laplacian stencils
  \end{itemize}

  \[
  \nabla^2_5 = \frac{1}{\Delta^2}[u_{i-1,j} + u_{i+1,j} + u_{i,j-1} +
    u_{i,j+1} - 4 u_{i,j}]
  \]
  \[
  \begin{split}
    \nabla^2_9 = \frac{1}{6\Delta^2}[4 u_{i-1,j} + 4 u_{i+1,j} + 4
      u_{i,j-1} + 4 u_{i,j+1} + u_{i-1,j-1}\\ + u_{i-1,j+1} +
      u_{i+1,j-1} + u_{i+1,j+1} - 20 u_{i,j}]
  \end{split}
  \]

  \begin{itemize}
  \item Implicit Euler time differencing
  \end{itemize}

  \[
  \ve{A} \ve{u}^{n+1} = \ve{u}^n
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Direct vs. Adjoint Analysis}

  \begin{columns}

    \begin{column}{0.5\textwidth}
      \begin{figure}[h!]
        \centering
        \includegraphics[width=2.5in,clip]{Adjoint_Direct_CPU_Time.pdf}
        \caption{\textbf{CPU Time (s) to converge vs. Problem Size ($N$ for
            an $N \times N$ square mesh).} }
      \end{figure}
    \end{column}

    \begin{column}{0.5\textwidth}
      \begin{figure}[h!]
        \centering
        \includegraphics[width=2.5in,clip]{Adjoint_Direct_Iterations.pdf}
        \caption{\textbf{Iterations to converge vs. Problem Size ($N$ for an
            $N \times N$ square mesh).} }
      \end{figure}
    \end{column}

  \end{columns}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Direct vs. Adjoint Analysis}

  \begin{figure}[h!]
    \centering
    \includegraphics[width=2.5in,clip]{Adjoint_Direct_Convergence.pdf}
    \caption{\textbf{Infinity norm of the solution residual
        vs. iteration number for a problem of fixed size.} }
  \end{figure}

  \begin{itemize}
  \item CPU time dominating factor in method selection
  \item Significant speedup with adjoint method
  \item Does not affect convergence behavior
  \item Use adjoint with MCSA and Sequential Monte Carlo
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Parallelization of Monte Carlo Methods}

  \begin{itemize}
  \item No literature observed for parallel Neumann-Ulam solvers
  \item Numerous references for modern parallel Monte Carlo methods
    in reactor physics
  \item MCSA parallelism comes from parallel matrix/vector operations
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Domain Decomposition}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Multiple-Set Overlapping-Domain Decomposition}

  \begin{figure}[htpb!]
    \begin{center}
      \scalebox{1.0}{
        \input{msod_example.pdftex_t} }
    \end{center}
    \caption{\textbf{Overlapping domain example illustrating how domain
        overlap can reduce communication costs.}}
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Domain-to-Domain Communication}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Load Balancing}

  \begin{figure}[htpb!]
    \begin{center}
      \scalebox{1.0}{
        \input{procassini_example.pdftex_t} }
    \end{center}
    \caption{\textbf{Example illustrating how domain decomposition can
        create load balance issues in Monte Carlo.}}
    \label{fig:procassini_example}
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Reproducability}

  \begin{figure}[htpb!]
    \begin{center}
      \scalebox{1.0}{
        \input{gentile_example.pdftex_t} }
    \end{center}
    \caption{\textbf{Gentile's example illustrating how domain
        decomposition can create reproducibility issues in Monte Carlo.}}
    \label{fig:gentile_example}
  \end{figure}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Parallel Adjoint Method}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Parallel MCSA}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Monte Carlo Solution Methods for Nonlinear Problems}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Nonlinear Preliminaries}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Nonlinear Preliminaries}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Newton-Krylov Methods}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Newton-Krylov Methods}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Matrix-Free Approximation}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Automatic Differentiation}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{The FANM Method}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Jacobian Storage vs. Subspace Storage}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Parallel FANM Method}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Research Proposal}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Experimental Framework}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Progress to Date}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Progress to Date}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Progress to Date}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Monte Carlo Methods Verification}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Proposed Numerical Experiments}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Proposed Challenge Problem}

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Conclusion}

\end{frame}

%%---------------------------------------------------------------------------%%

\end{document}


