\chapter{Parallel Monte Carlo Solution Methods for Linear Systems}
\label{ch:parallel_methods}

For Monte Carlo methods, and in particular MCSA, to be viable at the
production scale, scalable parallel implementations are required. In a
general linear algebra context for discrete systems, this has not yet
been achieved. Therefore, we will develop and implement parallel
algorithms for these methods leveraging both the knowledge gained from
the general parallel implementations of Krylov methods and modern
parallel strategies for Monte Carlo as developed by the reactor
physics community. In order to formulate a parallel MCSA algorithm, we
recognize that the algorithm occurs in two stages, an outer iteration
performing Richardson's iteration and applying the correction, and an
inner Monte Carlo solver that is generating the correction via the
adjoint method. The parallel aspects of both these components must be
considered.

%%---------------------------------------------------------------------------%%
\section{Domain Decomposition for Monte Carlo}
\label{sec:msod}
As observed in the discussion on parallel Krylov methods, large-scale
problems will surely have their data partitioned such that each
parallel process owns a subset of the equations in the linear
system. Given this convention, the adjoint Monte Carlo algorithm must
perform random walks over a domain that is decomposed and must remain
decomposed due to memory limitations. This naturally leads us to seek
parallel algorithms that handle domain decomposition.

In the context of radiation transport, Brunner and colleagues provided
a survey of algorithms for achieving this as implemented in production
implicit Monte Carlo codes \citep{brunner_comparison_2006}. In their
work they identify two data sets that are required to be communicated:
the sharing of particles that are transported from one domain to
another and therefore from one processor to another and a global
communication that signals if particle transport has been completed on
all processors. The algorithms presented are a fully-locking
synchronous scheme, an asynchronous-send/synchronous-receive pattern,
a traditional master/slave scheme, and a modified master/slave scheme
the implements a binary tree pattern for the global reduction type
operations needed to communicate between the master and slave
processes. They observed that the modified master/slave scheme
performed best in that global communications were implemented more
efficiently than those required by the asynchronous
scheme. Furthermore, none of these schemes handled load-imbalanced
cases efficiently. Such cases will be common if the source sampled in
the Monte Carlo random walk is not isotropic and not evenly
distributed throughout the global domain. It was noted that
efficiencies were improved by increasing the frequency by which
particle data was communicated between domain-adjacent
processors. However, this ultimately increases communication costs. In
2009, Brunner extended his work by using a more load-balanced approach
with a fully asynchronous communication pattern
\citep{brunner_efficient_2009}. Although the extended implementation
was more robust and allowed for scaling to larger numbers of
processors, performance issues were still noted with parallel
efficiency improvements needed in both the weak and strong scaling
cases for unbalanced problems. These results led Brunner to conclude
that a combination of domain decomposition and domain replication
could be used to solve some of these issues.

\subsection{Multiple-Set Overlapping-Domain Decomposition}
\label{subsec:msod}
In 2010, Wagner and colleagues developed the \textit{multiple-set
  overlapping-domain} (MSOD) decomposition for parallel Monte Carlo
applications for full-core light water reactor analysis
\citep{wagner_hybrid_2010}. In their work, an extension of Brunner's,
their scheme employed the similar parallel algorithms for particle
transport but a certain amount of overlap between adjacent domains was
used to decrease the number of particles leaving the local domain. In
addition, Wagner utilized a level of replication of the domain such
that the domain was only decomposed on $O(100)$ processors and if
replicated $O(1,000)$ times achieves simulation on $O(100,000)$
processors, thus providing spatial and particle parallelism. Each
collection of processors that constitutes a representation of the
entire domain is referred to as a set, and within a set overlap occurs
among its sub-domains. The original motivation was to decompose the
domain in a way that it remained in a physical cabinet in a large
distributed machine, thus reducing latency costs during
communication. A multiple set scheme is also motivated by the fact
that communication during particle transport only occurs within a set,
limiting communications during the transport procedure to a group of
$O(100)$ processors, a number that was shown to have excellent
parallel efficiencies in Brunner's work and therefore will scale well
in this algorithm. The overlapping domains within each set also
demonstrated reduced communication costs. On each processor, the
source is sampled in the local domain that would exist if no overlap
was used while tallies can be made over the entire overlapping domain.

To demonstrate this, consider the example adapted from Mervin's work
with Wagner and others in the same area \citep{mervin_variance_2012}
and presented in Figure~\ref{fig:msod_example}.
\begin{figure}[htpb!]
  \begin{center}
    \scalebox{1.5}{
      \input{chapters/parallel_mc/msod_example.pdftex_t} }
  \end{center}
  \caption{\textbf{Overlapping domain example illustrating how domain
      overlap can reduce communication costs.}
    \textit{All particles start in the blue region of interest. The
      dashed line represents 0.5 domain overlap between domains.}}
  \label{fig:msod_example}
\end{figure}
In this example, 3 particle histories are presented emanating from the
blue region of interest. Starting with particle A, if no domain
overlap is used then the only the blue domain exists on the starting
processor. Particle A is then transported through 3 other domains
before the history ends, therefore requiring three communications to
occur in Brunner's algorithm. If a 0.5 domain overlap is permitted as
shown by the dashed line, then the starting process owns enough of the
domain such that no communications must occur in order to complete the
particle A transport process. Using 0.5 domain overlap also easily
eliminates cases such as the represented by the path of particle C. In
this case, particle C is scattering between two adjacent domains,
incurring a large latency cost for a single particle. Finally, with
particle B we observe that 0.5 domain overlap will still not eliminate
all communications. However, if 1 domain overlap were used, the entire
geometry shown in Figure~\ref{fig:msod_example} would be contained on
the source processor and therefore transport of all 3 particles
without communication would occur.

Wagner and colleagues used this methodology for a 2-dimensional
calculation of a pressurized water reactor core and varied the domain
overlap from 0 to 3 domain overlap (a $7 \times 7$ box in the context
of our example) where a domain constituted a fuel assembly. For the
fully domain decomposed case, they observed that 76.12\% of all source
particles leave the domain. At 1.5 domain overlap, the percentage of
source particles born in the center assembly leaving the processor
domain dropped to 1.05\% and even further for 0.02\% for the 3 domain
overlap. Based on these results, this overlap approach, coupled with
the multiple sets paradigm that will scale for existing parallel
transport algorithms, provides a scalable Monte Carlo algorithm for
today's modern machines.

%%---------------------------------------------------------------------------%%
\section{Domain-to-Domain Communication}
\label{sec:mc_comm_costs}
With Wagner's work, we observed that domain overlap implemented with a
domain decomposed Monte Carlo setting reduces the amount of nearest
neighbor communication that must occur during transport. 

To date, parallel Neumann-Ulam methods have been limited to full
domain replication with parallelism exploited through individual
histories \cite{alexandrov_efficient_1998}. In reactor physics Monte
Carlo applications, however, domain decomposition has been identified
as a key principle in moving forward in high performance computing to
enable higher fidelity simulations
\cite{brunner_comparison_2006,siegel_analysis_2012}. To accomplish
this, we recognize from the literature that stochastic histories must
be transported from domain to domain as the simulation progresses and
they transition to states that are not in the local domain. Because we
have chosen a domain decomposition strategy in a parallel environment,
this means that communication of these histories must occur between
compute nodes owning neighboring pieces of the global domain. We wish
to characterize this communication not only because communication is
in general expensive, but also because these nearest-neighbor
communication sequences, specifically, have poor algorithmic strong
scaling \cite{gropp_high-performance_2001}.

The purpose of this study is to provide a simple, analytic theory
based on the properties of the linear system that will allow for
estimates of the domain decomposed behavior of the adjoint
Neumann-Ulam method. When solving problems where the linear operator
is symmetric, a host of analytic theories exist based on the
eigenvalue spectrum of the operator that characterize their behavior
in the context of deterministic linear solvers. Using past work, these
theories are adapted to the domain decomposed adjoint Neumann-Ulam
method using the one-speed, two-dimensional neutron diffusion
equation. In this paper we describe the adjoint Neumann-Ulam Monte
Carlo method followed by a presentation of the model problem. Using
the linear system generated by the discretization of the model
problem, we use a spectral analysis to generate analytic relations for
the eigenvalues of the operator based on system parameters. Using the
eigenvalue spectra, we then build relationships to characterize the
transport of stochastic histories in a decomposed domain and the
fraction of histories that leak from a domain and will therefore have
to be communicated. Finally, we compare these analytic results to
numerical experiments conducted with the model problem and draw
conclusions looking towards future work.

\subsection{Model Problem}
\label{subsec:model_problem}
For our numerical experiments, we choose the one-speed,
two-dimensional neutron diffusion equation as a model problem
\cite{duderstadt_nuclear_1976}:
\begin{equation}
  -\boldsymbol{\nabla} \cdot D \boldsymbol{\nabla} \phi + \Sigma_a
  \phi = S\:,
  \label{eq:diffusion_eq}
\end{equation}
where $\phi$ is the neutron flux, $\Sigma_a$ is the absorption cross
section, and $S$ is the source of neutrons. In addition, $D$ is the
diffusion coefficient defined as:
\begin{equation}
  D = \frac{1}{3 ( \Sigma_t - \bar{\mu}\Sigma_s )}\:,
  \label{eq:diffusion_coeff}
\end{equation}
where $\Sigma_s$ is the scattering cross section, $\Sigma_t = \Sigma_a
+ \Sigma_s$ is the total cross section, and $\bar{\mu}$ is the cosine
of the average scattering angle. For simplicity, we will take
$\bar{\mu} = 0$ for our analysis giving $D=(3 \Sigma_t)^{-1}$. In
addition, to further simplify we will assume a homogeneous domain such
that the cross sections remain constant throughout. Doing this permits
us to rewrite Eq~(\ref{eq:diffusion_eq}) as:
\begin{equation}
  -D \boldsymbol{\nabla}^2 \phi + \Sigma_a \phi = S\:.
  \label{eq:diffusion_eq_simple}
\end{equation}

We choose a finite difference scheme on a square Cartesian grid to
discretize the problem. For the Laplacian, we choose the 9-point
stencil shown in Figure~\ref{fig:stencil} over a grid of size $h$
\cite{leveque_finite_2007}:
\begin{multline}
  \nabla^2_9\phi = \frac{1}{6h^2}[4 \phi_{i-1,j} + 4 \phi_{i+1,j}
    + 4 \phi_{i,j-1} + 4 \phi_{i,j+1} + \phi_{i-1,j-1}\\ +
    \phi_{i-1,j+1} + \phi_{i+1,j-1} + \phi_{i+1,j+1} - 20
    \phi_{i,j}]\:.
  \label{eq:nine_point_stencil}
\end{multline}
\begin{figure}[t!]
  \begin{center}
    \scalebox{1.25}{\input{chapters/parallel_mc/stencil.pdftex_t}}
  \end{center}
  \caption{\textbf{Nine-point Laplacian stencil.}}
  \label{fig:stencil}
\end{figure}
We then have the following linear system to solve:
\begin{multline}
  -\frac{1}{6h^2}[4 \phi_{i-1,j} + 4 \phi_{i+1,j} + 4
    \phi_{i,j-1} + 4 \phi_{i,j+1} + \phi_{i-1,j-1}\\ + \phi_{i-1,j+1}
    + \phi_{i+1,j-1} + \phi_{i+1,j+1} - 20 \phi_{i,j}] + \Sigma_a
  \phi_{i,j} = s_{i,j}\:,
  \label{eq:fd_system}
\end{multline}
and in operator form:
\begin{equation}
  \ve{D}\boldsymbol{\phi}=\ve{s}\:,
  \label{eq:operator_system}
\end{equation}
where $\ve{D}$ is the diffusion operator, $\ve{s}$ is the source in
vector form and $\boldsymbol{\phi}$ is the vector of unknown fluxes.

To close the system, a set of boundary conditions is required. In the
case of a non-reentrant current condition applied to all global
boundaries of the domain, we choose the formulation of Duderstadt by
assuming the flux is zero at some ghost point beyond the
grid. Consider for example the equations on the $i=0$ boundary of the
domain:
\begin{multline}
  -\frac{1}{6h^2}[4 \phi_{-1,j} + 4 \phi_{1,j} + 4 \phi_{0,j-1} +
    4 \phi_{0,j+1} + \phi_{-1,j-1}\\ + \phi_{-1,j+1} + \phi_{1,j-1} +
    \phi_{1,j+1} - 20 \phi_{0,j}] + \Sigma_a \phi_{0,j} = s_{0,j}\:.
  \label{eq:x_min_bnd}
\end{multline}
Here we note some terms where $i=-1$ and therefore are representative
of grid points beyond the boundary of the domain. We set the flux at
these points to be zero, giving a valid set of equations for the $i=0$
boundary:
\begin{multline}
  -\frac{1}{6h^2}[4 \phi_{1,j} + 4 \phi_{0,j-1} + 4 \phi_{0,j+1}
    \\ + \phi_{-1,j+1} + \phi_{1,j-1} + \phi_{1,j+1} - 20 \phi_{0,j}]
  + \Sigma_a \phi_{0,j} = s_{0,j}\:.
  \label{eq:x_min_bnd_2}
\end{multline}
We repeat this procedure for the other boundaries of the domain. For
reflecting boundary conditions, the net current across a boundary is
zero.

\subsection{Spectral Analysis}
\label{subsec:spectral_analysis}
The convergence of the Neumann series in
Eq~(\ref{eq:adjoint_neumann_series}) approximated by the Monte Carlo
solver is dependent on the eigenvalues of the iteration matrix. We
will compute these eigenvalues by assuming eigenfunctions of the form
\cite{leveque_finite_2007}:
\begin{equation}
  \Phi_{p,q}(x,y) = e^{2 \pi \imath p x} e^{2 \pi \imath q y}\:,
  \label{eq:eigenfunction_form}
\end{equation}
where different combinations of $p$ and $q$ represent the different
eigenmodes of the solution. As these are valid forms of the solution,
then the action of the linear operator on these eigenfunctions should
give the eigenvalues of the matrix as they exist on the unit circle in
the complex plane.

\subsection{Iteration Matrix Spectrum}
\label{subsec:iteration_spectrum}
For the model problem, we first compute the eigenvalues for the
diffusion operator $\ve{D}$ by applying the operator to the
eigenfunctions and noting that $x=ih$ and $y=jh$:
\begin{multline}
  \ve{D}\Phi_{p,q}(x,y) = \lambda_{p,q}(\ve{D})
  =\\ -\frac{D}{6h^2}\Big[4 e^{-2 \pi \imath p h} + 4 e^{2 \pi \imath
      p h} + 4 e^{-2 \pi \imath q h} + 4 e^{2 \pi \imath q h} + e^{-2
      \pi \imath p h} e^{-2 \pi \imath q h} \\ + e^{-2 \pi \imath p h}
    e^{2 \pi \imath q h} + e^{2 \pi \imath p h} e^{-2 \pi \imath q h}
    + e^{2 \pi \imath p h} e^{2 \pi \imath q h} - 20\Big] + \Sigma_a
  \:.
  \label{eq:deriv_diff_1}
\end{multline}
Using Euler's formula, we can collapse the exponentials to
trigonometric functions:
\begin{equation}
  \lambda_{p,q}(\ve{D}) = -\frac{D}{6h^2}[ 8 \cos(\pi p h) + 8
    \cos(\pi q h) + 4 \cos(\pi p h) \cos(\pi q h) - 20] + \Sigma_a\:.
  \label{eq:deriv_diff_2}
\end{equation}

As Eq~(\ref{eq:diffusion_eq}) is diagonally dominant, Jacobi
preconditioning is sufficient to reduce the spectral radius of the
iteration matrix below unity and therefore ensure convergence of the
Neumann series. The preconditioner in this case is then $\ve{M} =
diag(\ve{D})$ such that we are solving the following linear system:
\begin{equation}
  \ve{M}^{-1} \ve{D} \boldsymbol{\phi} = \ve{M}^{-1} \ve{s}\:.
  \label{eq:precond_diffsion}
\end{equation}
The operator $\ve{M}^{-1} \ve{D}$ is merely the original diffusion
operator with each row scaled by the diagonal component. As we have
defined a homogeneous domain, the scaling factor, $\alpha$, is the
same for all rows in the operator and defined as the $\phi_{i,j}$
coefficient from Eq~(\ref{eq:fd_system}):
\begin{equation}
  \alpha = \Bigg[\frac{10 D}{3 h^2} + \Sigma_a\Bigg]^{-1}\:.
  \label{eq:jacobi_scaling}
\end{equation}
Using this coefficient, we then have the following spectrum of
preconditioned eigenvalues:
\begin{equation}
  \lambda_{p,q}(\ve{M}^{-1} \ve{D}) = \alpha \lambda_{p,q}(\ve{D})\:.
  \label{eq:preconditioned_eigenvalues}
\end{equation}

The spectral radius of the iteration matrix is obtained by seeking its
largest eigenvalue. As with the diffusion operator, we can use the
same analysis techniques to find the eigenvalues for the iteration
matrix. We use a few simplifications by noting that if the Jacobi
preconditioned iteration matrix is $\ve{H} = \ve{I} -
\ve{M}^{-1}\ve{D}$, the we except all terms on the diagonal of the
iteration matrix to be zero such that we have the following stencil:
\begin{equation}
  \ve{H}\boldsymbol{\phi} = \frac{\alpha D}{6h^2}[4 \phi_{i-1,j}
    + 4 \phi_{i+1,j} + 4 \phi_{i,j-1} + 4 \phi_{i,j+1} +
    \phi_{i-1,j-1} + \phi_{i-1,j+1} + \phi_{i+1,j-1} +
    \phi_{i+1,j+1}]\:.
  \label{eq:iteration_stencil}
\end{equation}
Inserting the eigenfunctions defined by
Eq~(\ref{eq:eigenfunction_form}) we get:
\begin{multline}
  \lambda_{p,q}(\ve{H}) = \frac{\alpha D}{6h^2}\Big[4 e^{-2 \pi \imath p
      h} + 4 e^{2 \pi \imath p h} + 4 e^{-2 \pi \imath q h} + 4 e^{2
      \pi \imath q h} + e^{-2 \pi \imath p h} e^{-2 \pi \imath q h}
    \\ + e^{-2 \pi \imath p h} e^{2 \pi \imath q q} + e^{2 \pi \imath
      p h} e^{-2 \pi \imath q h} + e^{2 \pi \imath p h} e^{2 \pi
      \imath q h}\Big]\:,
  \label{eq:iteration_deriv}
\end{multline}
which simplifies to:
\begin{equation}
  \lambda_{p,q}(\ve{H}) = \frac{\alpha D}{6h^2}[ 8 \cos(\pi p h) + 8
    \cos(\pi q h) + 4 \cos(\pi p h) \cos(\pi q h)]\:,
  \label{eq:iteration_spectrum}
\end{equation}
giving the eigenvalue spectrum for the Jacobi preconditioned iteration
matrix. To find these maxium eigenvalue,
Eq~(\ref{eq:preconditioned_eigenvalues}) is plotted as a function of
$p$ with $p=q$ in Figure~\ref{fig:diffusion_spectrum} for various
values of $\Sigma_a$.
\begin{figure}[t!]
  \begin{center}
    \includegraphics[width=5in,clip]{chapters/parallel_mc/diffusion_spectrum.png}
  \end{center}
  \caption{\textbf{Eigenvalue spectra for the diffusion equation.}}
  \label{fig:diffusion_spectrum}
\end{figure}.
 We find that the maximum eigenvalue exists when $p=q=0$,
giving the following for the spectral radius of the Jacobi
preconditioned iteration matrix:
\begin{equation}
  \rho(\ve{H}) = \frac{10 \alpha D}{3 h^2}\:.
  \label{eq:iteration_radius}
\end{equation}

\subsection{Neumann Series Convergence}
\label{subsec:neumann_convergence}
The adjoint Monte Carlo method is effectively an approximation to a
stationary method. Stationary methods for linear systems arise from
splitting the operator in Eq~(\ref{eq:linear_problem}) and iterating:
\begin{equation}
  \ve{x}^{k+1} = \ve{H}\ve{x}^k + \ve{c}\:,
  \label{eq:linear_iterative_method}
\end{equation}
with $k \in \mathbb{Z}^+$ defined as the \textit{iteration
  index}. Defining $\ve{e}^k = \ve{u}^k - \ve{u}$ as the solution
error at the $k^{th}$ iterate, the error after $k$ iterations is then:
\begin{equation}
  \ve{e}^{k} = \ve{H}^k\ve{e}^0\:. 
  \label{eq:linear_k_iter_error}
\end{equation}
By assuming $\ve{H}$ is diagonalizable \cite{leveque_finite_2007}, we
then have:
\begin{equation}
  ||\ve{e}^{k}||_2 \leq \rho(\ve{H})^k ||\ve{e}^0||_2\:.
  \label{eq:linear_k_iter_norm3}
\end{equation}

In the adjoint Neumann-Ulam method, $k$ iterations, equivalent to $k$
applications of the iteration matrix, are approximated by a random
walk of average length $k$ to yield the summation in
Eq~(\ref{eq:adjoint_neumann_solution})
\cite{dimov_new_1998,danilov_asymptotic_2000}. This random walk
length, or the number of transitions before the termination of a
history (either by the weight cutoff, absorption, or exiting the
global domain) is therefore approximately the number of stationary
iterations required to converge to the specified tolerance. In the
case of the adjoint Neumann-Ulam method, no such tolerance exists,
however, we have specified a weight cutoff, $W_c$, that determines
when low-weight histories will be prematurely terminated as their
contributions are deemed minute. After $k$ iterations, a stationary
method is terminated as the error has reached some fraction,
$\epsilon$, of the initial error:
\begin{equation}
  ||\ve{e}^{k}||_2 = \epsilon ||\ve{e}^0||_2\:.
  \label{eq:linear_k_iter_norm4}
\end{equation}
Per Eq~(\ref{eq:linear_k_iter_norm3}), we see that this fraction is
equivalent to $\epsilon = \rho(\ve{H})^k$. For the adjoint
Neumann-Ulam method, if we take this fraction to be the weight cutoff,
a measure of how accurately the contributions of a particular history
to the solution are tallied, we then have the following relationship
for $k$:
\begin{equation}
  k = \frac{ \log(W_c) }{ \log( \rho(\ve{H}) ) }\:.
  \label{eq:analytic_k}
\end{equation}
This then gives us a means to estimate the length of the random walks
that will be generated from a particular linear operator based on the
eigenvalues of its iteration matrix (independent of the linear
operator splitting chosen) and based on the weight cutoff parameter
used in the Neumann-Ulam method.

\subsubsection{Domain Leakage Approximations}
\label{subsubsec:domain_leak_approx}
In a domain decomposed situation, not all histories will remain within
the domain they started in and must instead be communicated. This
communication, expected to be expensive, was analyzed by Siegel and
colleagues for idealized, load balanced situations for full nuclear
reactor core Monte Carlo simulations \cite{siegel_analysis_2012}.  To
quantify the number of particles that leak out of the local domain
they define a leakage fraction, $\Lambda$, as:
\begin{equation}
  \Lambda = \frac{average\ \#\ of\ particles\ leaving\ local\ domain}
          {total\ of\ \#\ of\ particles\ starting\ in\ local\ domain}\:.
          \label{eq:leakage_fraction}
\end{equation}
For their studies, Siegel and colleagues assumed that the value of
$\Lambda$ was dependent on the total cross section of the system via
the Wigner rational approximation. Outlined more thoroughly by Hwang's
chapter in \cite{azmy_nuclear_2010}, we will use both the Wigner
rational approximation and the mean chord approximation as a means to
estimate the leakage fraction.

In the case of domain decomposed linear operator equations, we can use
diffusion theory to estimate the optical thickness of a domain in the
decomposition and the corresponding leakage fraction in terms of
properties of the linear operator and the discretization. To begin we
must first calculate the mean distance a Monte Carlo history will move
in the grid by computing the mean squared distance of its movement
along the chord of length $l$ defined across the domain. After a
single transition a history will have moved a mean squared distance
of:
\begin{equation}
  \langle \bar{r_1^2} \rangle = (n_s h)^2\:,
  \label{eq:step_1_length}
\end{equation}
where $h$ is the size of the discrete grid elements along the chord
and $n_s$ is the number of grid elements a history will move on
average every transition. For our diffusion model problem, $n_s$ would
equate to the expected number of states in the $i$ (or $j$ as the problem is
symmetric) direction that a history will move in a single
transition and is dependent on the stencil used for the
discretization. After $k$ transitions in the random walk, the history
will have moved a mean squared distance of:
\begin{equation}
  \langle \bar{r_k^2} \rangle = k (n_s h)^2\:.
  \label{eq:step_k_length}
\end{equation}
If our chord is of length $l$ and there are $n_i$ grid elements (or
states to which a history may transition) along that chord, then $h =
l / n_i$ giving:
\begin{equation}
  \langle \bar{r_k^2} \rangle = k \Bigg(\frac{n_s l}{n_i}\Bigg)^2\:.
  \label{eq:step_k_length_sub}
\end{equation}
From diffusion theory, we expect the average number of interactions
along the chord to be:
\begin{equation}
  \tau = \frac{l}{2 d \sqrt{\langle \bar{r_k^2} \rangle}}\:,
  \label{eq:optical_thickness_1}
\end{equation}
where $d$ is the dimensionality of the problem and $\sqrt{\langle
  \bar{r_k^2} \rangle}$ is effectively the mean free path of the Monte
Carlo history in the domain. We can readily interpret $\tau$ to be the
\textit{effective optical thickness} of a domain of length
$l$. Inserting Eq~(\ref{eq:step_k_length_sub}) we arrive at:
\begin{equation}
  \tau = \frac{n_i}{2 d n_s \sqrt{k}}\:,
  \label{eq:optical_thickness_2}
\end{equation}
which if expanded with Eq~(\ref{eq:analytic_k}) gives us the final
relation for the effective optical thickness:
\begin{equation}
  \tau = \frac{n_i}{2 d n_s}
  \sqrt{\frac{\log(\rho(\ve{H}))}{\log(W_c)}}\:.
  \label{eq:optical_thickness_3}
\end{equation}

For optically thin domains, we expect that most histories will be
communicated, while optically thick domains will leak the fraction of
histories that did not interact within. Using the optical thickness
defined in Eq~(\ref{eq:optical_thickness_3}), we can then complete the
leakage approximations by defining the bounds of $\tau \rightarrow 0,
\Lambda \rightarrow 1$ and $\tau \rightarrow \infty, \Lambda
\rightarrow \tau^{-1}$.  With these bounds we can then define the
leakage fraction out of a domain for the adjoint Neumann-Ulam method
using the Wigner rational approximation:
\begin{equation}
  \Lambda = \frac{1}{1+\tau}\:,
  \label{eq:wigner_domain_leakage}
\end{equation}
and using the mean-chord approximation:
\begin{equation}
  \Lambda = \frac{1-e^{-\tau}}{\tau}\:.
  \label{eq:mean_chord_domain_leakage}
\end{equation}
Here, the leakage fraction is explicitly bound to the eigenvalues of
the iteration matrix, the size of the domain, the content of the
discretization stencil, and the weight cutoff selected to terminate
low weight histories.

\subsection{Numerical Experiments}
\label{subsec:numerical_experiments}
To test the relationships developed by the spectral analysis, we form
two simple numerical experiments using the diffusion model problem:
one to measure the length of the random walks as a function of the
iteration matrix eigenvalues, and one to measure the domain leakage
fraction as a function of the iteration matrix eigenvalues and the
discretization properties. Before doing this, we verify our
computation of the spectral radius of the iteration matrix by
numerically computing the largest eigenvalue of the diffusion operator
using an iterative eigenvalue solver. For this verification, a $100
\times 100$ square grid with $h=0.01$, $h=0.1$, and $h=1.0$ and the
absorption cross varied from 0 to 100 while the scattering cross
section was fixed at unity. Figure~\ref{fig:measured_spec_rad} gives
the measured spectral radius of the iteration matrix and the computed
spectral radius for the preconditioned diffusion operator using
Eq~(\ref{eq:iteration_radius}) as function of the absorption to
scattering ratio $(\Sigma_a / \Sigma_s)$. Excellent agreement was
observed between the analytic and numerical results with all data
points computed within the tolerance of the iterative eigenvalue
solver.
\begin{figure}[ht!]
  \begin{spacing}{1.0}
    \begin{center}
      \includegraphics[width=4in,clip]{chapters/parallel_mc/measured_spec_rad.png}
    \end{center}
    \caption{Measured and analytic preconditioned diffusion operator
      spectral radius as a function of the absorption cross section to
      scattering cross section ratio. Values of $h=0.01$, $h=0.1$, and
      $h=1.0$ were used. The red data was computed numerically by an
      eigensolver while the black dashed data was generated by
      Eq~(\ref{eq:iteration_radius}).}
    \label{fig:measured_spec_rad}
  \end{spacing}
\end{figure}

\subsubsection{Random Walk Length}
\label{subsubsec:walk_length}
With the spectral data in hand, we can go about setting up an
experiment to measure the length of the random walks generated by the
adjoint Neumann-Ulam solver. To do this, we again use a $100 \times
100$ square grid with $h=0.1$ and the absorption cross varied from 0
to 100 while the scattering cross section was fixed at unity. Three
weight cutoff values of \sn{1}{-2}, \sn{1}{-4}, and \sn{1}{-8} were
used with 10,000 histories generated by a point source of strength 1
in the center of the domain. For each of the histories, the number of
transitions made was tallied to provide an effective value of $k$ for
each history. This value was then averaged over all histories to get a
measured value of $k$ for the particular operator. On the left,
Figure~\ref{fig:measured_length} presents these measurements as well
as the analytic result computed by Eq~(\ref{eq:analytic_k}) as a
function of the iteration matrix spectral radius, $\rho(\ve{H})$. On
the right, Figure~\ref{fig:measured_length} gives the relative error
between the predicted and observed results. We note good qualitative
agreement between the measured and analytic results. However, we
observe a larger relative error for both long and short random walks.
\begin{figure}[ht!]
  \begin{spacing}{1.0}
    \begin{center}
      \includegraphics[width=6.5in,clip]{chapters/parallel_mc/measured_length.png}
    \end{center}
    \caption{Measured and analytic random walk length as a function of
      the iteration matrix spectral radius. The weight cutoff was
      varied with \sn{1}{-2}, \sn{1}{-4}, and \sn{1}{-8}. In the left
      plot, the red data was computed numerically by an adjoint
      Neumann-Ulam implementation while the black dashed data was
      generated by Eq~(\ref{eq:analytic_k}). In the right plot, the
      relative error between the predicted and measured results is
      presented for each weight cutoff.}
    \label{fig:measured_length}
  \end{spacing}
\end{figure}

\subsubsection{Domain Leakage}
\label{subsubsec:domain_leakage}
Finally, we seek to measure the leakage from a domain in a domain
decomposed Monte Carlo calculation and assess the quality of our
analytic relation for the optical thickness of domain and the
associated leakage approximations. For this experiment, a square grid
with $h=0.1$ was decomposed into 9 square domains, 3 in each cardinal
direction with measurements occurring in the central domain without
boundary grid points. For the cross sections, the absorption cross
section was varied from 1 to 100 while the scattering cross section
was set to zero to create a purely absorbing environment with weight
cutoff of \sn{1}{-4}. The optical thickness of these domains will vary
as a function of the absorption cross section if the other parameters
are fixed. To compute the optical thickness, along with the spectral
radius as given by Eq~(\ref{eq:iteration_radius}), we also need the
parameters $n_i$ and $n_s$ which respectively describe the typical
domain length and the average number of states moved along that
typical length per history transition. For our grid above, the domains
are varied in size with $50 \times 50$, $100 \times 100$, and $200
\times 200$ cells giving $n_i=50$, $n_i=100$, and $n_i=200$ grid
points or states along the typical length of the domain
respectively. Looking at the Laplacian stencil in
Eq~(\ref{eq:nine_point_stencil}), we see that all history transitions
will only move a single state in either the $i$ or $j$ directions due
to the symmetry of the problem. Furthermore, if we choose the $i$
direction, not all states we will transition to will move the history
in that direction. Therefore, we look to the definition of the
iteration matrix in Eq~(\ref{eq:iteration_stencil}) and the definition
of the adjoint probability matrix in Eq~(\ref{eq:adjoint_probability})
to estimate the $n_s$ parameter. For a particular transition starting
at state $(i,j)$, 6 of the 8 possible new states in the stencil move
the history in $i$ direction with relative coefficients of 4 for
moving in the $(\pm i,0)$ direction and of 1 for moving in the $(\pm
i,\pm j)$. These coefficients dictate the frequency those states are
visited relative to the others. For those 6 states we can visit along
the typical length, their sum is 12 out of the total 20 for the
coefficients for all possible states with their ratio giving $n_s =
\frac{3}{5}$.

To compute the leakage fraction numerically, \sn{3}{5} histories were
sampled from a uniform source of strength unity over the global
domain. At the start of a stage of histories, the number of histories
starting in the center domain was computed and as the stage
progressed, the number of histories that exited that domain was
tallied with the ratio of the two numbers providing a numerical
measure for the leakage fraction. Figure~\ref{fig:measured_leakage}
gives the domain leakage measurements for the domain in the center of
the global grid as well as the analytic result computed by
Eqs~(\ref{eq:wigner_domain_leakage}) and
(\ref{eq:mean_chord_domain_leakage}) as a function of the iteration
matrix spectral radius.
\begin{figure}[ht!]
  \begin{spacing}{1.0}
    \begin{center}
      \includegraphics[width=4.5in,clip]{chapters/parallel_mc/leakage_variation.png}
    \end{center}
    \caption{Measured and analytic domain leakage as a function of the
      iteration matrix spectral radius. To test the behavior with
      respect to domain size, $n_i=50$, $n_i=100$,and $n_i=200$ were
      used. The red data was computed numerically by a
      domain-decomposed adjoint Neumann-Ulam implementation, the black
      dashed data was generated by
      Eq~(\ref{eq:mean_chord_domain_leakage}) using the mean-chord
      approximation, and the dashed-dotted black data was generated by
      Eq~(\ref{eq:wigner_domain_leakage}) using the Wigner rational
      approximation.}
    \label{fig:measured_leakage}
  \end{spacing}
\end{figure}
Again, we note good qualitative agreement between the measured and
analytic quantities but we begin to see the limits of the leakage
approximations. To compare the quality of the two approximations, the
absolute error between the computed leakage fraction and that
generated by the Wigner rational and mean chord approximations is
plotted in Figure~\ref{fig:leakage_error} for all domain sizes
tested. 
\begin{figure}[ht!]
  \begin{spacing}{1.0}
    \begin{center}
      \includegraphics[width=4.5in,clip]{chapters/parallel_mc/leakage_error.png}
    \end{center}
    \caption{Measured and analytic domain leakage absolute error as a
      function of the iteration matrix spectral radius.  To test the
      behavior with respect to domain size, $n_i=50$ (green),
      $n_i=100$ (blue), and $n_i=200$ (red) were used. The dashed
      lines represent the error using the Wigner rational
      approximation while the solid lines represent the error using
      the mean-chord approximation.}
    \label{fig:leakage_error}
  \end{spacing}
\end{figure}
From these error results, the mean chord approximation is shown to
have a lower error for ill-conditioned systems as compared to the
Wigner approximation while the Wigner approximation produces less
error for more well-conditioned systems. We also note that for the
optically thick domains, the error is likely corresponded to that
observed in Figure~\ref{fig:measured_length} for the $k$ parameter
while the large relative error in $k$ for optically thin domains does
not affect the approximation significantly. In general, the mean chord
approximation is a better choice to estimate the leakage fraction in a
domain from the adjoint Neumann-Ulam method and except for a single
data point with $n_i=50$, the mean chord approximation yielded leakage
fractions within 0.05 of the measured results. As the domain becomes
more optically thick (with both increasing $n_i$ and decreasing
$\rho(\ve{H})$), the approximations are more accurate.

We have presented an analytic analysis of the domain decomposed
behavior of the adjoint Neumann Ulam method for linear systems. Good
agreement was observed for the derived analytic relationships for
random walk length and leakage fraction when compared to numerical
results generated by a domain decomposed implementation of the adjoint
method.

In future work, these relationships will serve as guidelines for
selecting an appropriate parallel algorithm strategy and provide a
basis for performance models for future parallel implementations of
the adjoint Neumann-Ulam method. To be applicable to a broader range
of problems, these models could potentially be extended to
non-symmetric systems. In addition, these relations will be used to
analyze parallel implementations of Monte Carlo synthetic acceleration
methods that leverage the adjoint method and serve as an initial
grounds for assessing their feasibility for large-scale problems.

%%---------------------------------------------------------------------------%%
\section{Load Balancing Concerns}
\label{sec:mc_load_balancing}
Although domain decomposition was shown to be efficient in a perfectly
load balanced situation in Siegel's work \citep{siegel_analysis_2012},
careful consideration must be made for situations where this is not
the case. Given the stochastic nature of the problem and lack of a
globally homogeneous domain, parallel Monte Carlo simulations are
inherently load imbalanced. Procassini and others worked to alleviate
some of the load imbalances that are generated by both particle and
spatial parallelism and are therefore applicable to the MSOD algorithm
\citep{procassini_dynamic_2005}. They chose a dynamic balancing scheme
in which the number of times a particular domain was replicated was
dependent on the amount of work in that domain (i.e. domains with a
high particle flux and therefore more particle histories to compute
require more work). In this variation, domains that require more work
will be replicated more frequently at reduced particle counts in each
replication. Furthermore, Procassini and colleagues noted that as the
simulation progressed and particles were transported throughout the
domain, the amount of replication for each domain would vary as
particle histories began to diffuse, causing some regions to have
higher work loads and some to have smaller work loads than the initial
conditions.

Consider the example in Figure~\ref{fig:procassini_example} adapted
from Procassini's work.
\begin{figure}[htpb!]
  \begin{center}
    \scalebox{1.5}{
      \input{chapters/parallel_mc/procassini_example.pdftex_t} }
  \end{center}
  \caption{\textbf{Example illustrating how domain decomposition can
      create load balance issues in Monte Carlo.}  \textit{A domain is
      decomposed into 4 zones on 8 processors with a point source in
      the lower left zone. As the particles diffuse from the source in
      the random walk sequence as shown in the top row, their tracks
      populate the entire domain. As given in the bottom row, as the
      global percentage of particles increases in a zone, that zone's
      replication count is increased.}}
  \label{fig:procassini_example}
\end{figure}
In this example, a geometry is decomposed into 4 domains on 8
processors with a point source in the bottom left domain. To begin,
because the point source is concentrated in one domain, that domain is
replicated 5 times such the amount of work it has to do per processor
is roughly balanced with the others. As the particles begin to diffuse
away from the point source, the amount of replication is adjusted to
maintain load balance. Near the end of the simulation, the diffusion
of particles is enough that all domains have equal replication.  By
doing this, load balance is improved as each domain has approximately
equal work although each domain may represent a different spatial
location and have a differing number of histories to
compute. Compared to Wagner's work where the fission source was
distributed relatively evenly throughout the domain, fixed source
problems (and especially those that have a point-like source) like
those presented in Procassini's work will be more prone to changing
load balance requirements.

%%---------------------------------------------------------------------------%%
\section{Reproducible Domain Decomposed Results}
\label{sec:reproducible_mc}
The 2006 work of Brunner is notable in that the Monte Carlo codes used
to implement and test the algorithms adhered to a strict policy of
generating identical results independent of domain decomposition or
domain replication as derived from the work of Gentile and colleagues
\citep{gentile_obtaining_2005}. In Gentile's work, a procedure is
given for obtaining results reproducible to machine precision for an
arbitrary number of processors and domains. Differences can arise from
using a different random number sequence in each domain and performing
a sequence of floating point operations on identical data in a
different order, leading to variations in round-off error and
ultimately a non-identical answer. They use a simple example,
recreated below in Figure~\ref{fig:gentile_example}, that illustrates
these issues.
\begin{figure}[htpb!]
  \begin{center}
    \scalebox{1.5}{
      \input{chapters/parallel_mc/gentile_example.pdftex_t} }
  \end{center}
  \caption{\textbf{Gentile's example illustrating how domain
      decomposition can create reproducibility issues in Monte Carlo.}
    \textit{Both particles A and B start in zone 1 on processor
      1. Particle A moves to zone 2 on processor 2 and scatters back
      to zone 1 while B scatters in zone 1 and remains there. A1 and
      A2 denote the track of particle A that is in zone 1 while B1 and
      B2 denote the track of particle B that is in zone 1.}}
  \label{fig:gentile_example}
\end{figure}
In this example, the domain is decomposed on two processors with each
processor owning one of the two zones. Starting with particle A, it is
born in zone 1 and is transported to zone 2 where a scattering event
occurs. Concerning the first reproducibility issue, if the same
sequence of random numbers is not used to compute the trajectory from
the new scattering event, we cannot expect to achieve the same result
if the domain were not decomposed. If the numbers are different, the
scattering event in zone 2 may keep the particle there or even eject
it from the domain if the sequence were different. The second issue is
demonstrated by adding another particle B that remains in the
domain. In this case, an efficient algorithm will transport particle A
on processor 1 until it leaves zone 1 and then transport particle
B. Particle A will not renter the domain until it has been
communicated to processor 2, processor 2 performs the transport, and
it is communicated back to processor 1. If we are doing a track-length
tally in zone 1, then we sum the tracks lengths observed in that
zone. In the single processor, single zone case particle A would be
transported in its entirety and then particle B transported. This
would result in a tally sum with the following order of operations:
$(((A1+A2)+B1)+B2)$. If we were instead to use 2 processors, we would
instead have the following order: $(((A1+B1)+B2)+A2)$. In the context
of floating point operations, we cannot expect these to have an
identical result to machine precision as round-off approximations will
differ resulting in non-commutative addition.

Procassini's solutions to these problems are elegant in that they
require a minimal amount of modification to be applied to the Monte
Carlo algorithm. To solve the first issue, in order to ensure each
particle maintains an invariant random number sequence that determines
its behavior regardless of domain decomposition, each particle is
assigned a random number seed that describes its current state upon
entering the domain of a new processor. These seeds are derived from
the actual geometric location of the particle such that it is
decomposition invariant. Non-commutative floating point operations are
overcome by instead mapping floating point values to 64-bit integer
values for which additions will always be commutative. Once the
operations are complete, these integers are mapped back to floating
point values.

%%---------------------------------------------------------------------------%%
\section{Parallel Adjoint Method}
\label{sec:parallel_adjoint}
We can take much of what was learned from the survey of parallel Monte
Carlo methods for radiation transport and directly apply it to a
parallel formulation of our stochastic linear solvers. Direct analogs
can be derived from these works by noting that the primary difference
between solving a linear system with Monte Carlo methods and fixed
source Monte Carlo transport problems is the content of the Markov
chains that are generated. The transitions represented by these chains
are bound by probabilities and weights and are initiated by the
sampling of a source. In the context of transport problems, those
transitions represent events such as particle scattering and
absorption with probabilities that are determined by physical data in
the form of cross sections. For stochastic matrix inversion, those
transitions represent moving between the equations of the linear
system (and therefore the physical domain which they represent) and
their probabilities are defined by the coefficients of those
equations. Ultimately, we tally the contributions to generate
expectation values in the desired states as we progress through the
chains. Therefore, parallel methods for Monte Carlo radiation
transport can be abstracted and we can use those concepts that apply
to matrix inversion methods as an initial means of developing a
parallel Neumann-Ulam-type solver. Based on the results observed in
the last decade of parallel Monte Carlo development, we can generate
many practical questions that this type of work could answer.

Given a decomposed domain, per Wagner's work
\citep{wagner_hybrid_2010} is clear that domain overlap significantly
reduces the amount of communication required between adjacent domains
as histories move to states that are not owned by the local
processor. How much domain overlap is suitable for matrix inversion
problems? Are we memory limited by large problems such that only so
much overlap is feasible? Are there metrics related to the properties
of the matrix including the eigenvalues and sparsity pattern such that
we can provide guidelines for selecting the amount of overlap
required? Furthermore, as Wagner and colleagues observed
\citep{wagner_hybrid_2010}, with marginal domain overlap for reactor
problems, the percentage of histories leaving the local domain can
easily be reduced to less than 1\%. Evans and colleagues in their
initial MCSA development typically used only 50 particle histories in
order to compute a correction in an MCSA iteration
\citep{evans_monte_2012}. Per the Central Limit Theorem, that
correlates to a statistical uncertainty of 14.1\% in the correction,
yet as compared to conventional Krylov solvers, MCSA achieved
identical numerical results. If good convergence and numerically
accurate solutions can be achieved with such a large uncertainty in
the correction computation, then perhaps with enough domain overlap
the minimal amount of histories that do transition to non-local states
can be ignored and thus eliminate all communication in the parallel
Monte Carlo adjoint solver transport sequence and create an
embarrassingly parallel method. Given Gropp's work on parallel Krylov
methods that we discussed in
\S~\ref{subsec:projection_method_performance} and the empirical
results of Siegel \citep{siegel_analysis_2012}, we know that these
nearest neighbor computations between adjacent domains do not scale as
well as global reduction operations and therefore we are improving
scaling by eliminating them from the transport sequence. It will be
important to determine if the expectation value bias in the MCSA
correction generated by this approximation will remain within the
bounds of the already high statistical uncertainty for unbiased
estimates, thus providing numerically equivalent results.

From a domain replication perspective, this may be difficult to
achieve with a production scale linear solver. Typically, memory is at
a premium and therefore the more distinct domains available in the
decomposition, the spatially finer and/or the numerically more
accurate the discretization that can be implemented. How much
replication is possible for large problems? How does replication
facilitate parallel performance when coupled with domain overlap? How
can we measure how much domain overlap is feasible? Replicating
domains may therefore run into memory limitations for exceptionally
large problems such that the operator, solution vector, and source
vector must be copied in their entirety multiple times. From a
resiliency standpoint, such an operation will be required, and
therefore its performance and memory implications on conventional
problems must be analyzed.

%%---------------------------------------------------------------------------%%
\section{Parallel MCSA}
\label{sec:parallel_mcsa}
With a parallel adjoint Neumann-Ulam solver implementation, the
parallel implementation of the MCSA method will be trivial. Recall the
MCSA iteration procedure outlined in Eq~(\ref{eq:mcsa}). In
\S~\ref{sec:parallel_krylov_methods} we discussed parallel matrix and
vector operations as utilized in conventional Krylov methods. We
utilize these here for the parallel MCSA implementation. In the first
step, a parallel matrix-vector multiply is used to apply the split
operator to the previous iterate's solution. A parallel vector update
is then performed with the source vector to arrive at the initial
iteration guess. In the next step, the residual is computed by the
same operations where now the operator is applied to the solution
guess with a parallel matrix-vector multiply and then a parallel
vector update with the source vector is performed. Once the correction
is computed with a parallel adjoint Neumann-Ulam solve, this
correction is applied to the guess with a parallel vector update to
get the new iteration solution. Additionally, as given by
Eq~(\ref{eq:mcsa_stopping_criteria}), 2 parallel vector reductions
will be required to check the stopping criteria: one initially to
compute the infinity norm of the source vector, and another at every
iteration to compute the infinity norm of the residual vector. For
this implementation, all of the issues that will be potentially
generated by the parallel adjoint solver implementation will manifest
themselves here as the quality of the correction will be of intense
study.

In addition to parallel implementation and performance, MCSA's
potential for aiding advancement in non-symmetric matrix solutions
leads to a natural comparison with the GMRES algorithm. As both
solvers are aimed at the same class of problem, we desire a set of
metrics that will allow us to quantitatively compare the two. Given
that the Krylov subspace maintained by GMRES can become large, do we
benefit from a memory standpoint with an MCSA scheme in that no
subspace is required? Does this benefit outweigh the fact that the
linear operator must be explicitly formed in order to build the
transition probabilities for the random walk sequence? For
non-symmetric systems, does MCSA exhibit similar convergence
properties to Krylov methods? If a Krylov methods build a subspace,
can those memory savings in MCSA be used to implement domain
replication in the adjoint solver? Such questions can be answered by a
comparative study of the two solvers that controls the system size and
the iterations required to converge.
