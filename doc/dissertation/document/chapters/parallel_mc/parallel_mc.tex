\chapter{Parallel Monte Carlo Solution Methods for Linear Systems}
\label{ch:parallel_methods}

For Monte Carlo methods, and in particular MCSA, to be viable at the
production scale, scalable parallel implementations are required. In a
general linear algebra context for discrete systems, this has not yet
been achieved. Therefore, we will develop and implement parallel
algorithms for these methods leveraging both the knowledge gained from
the general parallel implementations of Krylov methods and modern
parallel strategies for Monte Carlo as developed by the reactor
physics community. In order to formulate a parallel MCSA algorithm, we
recognize that the algorithm occurs in two stages, an outer iteration
performing Richardson's iteration and applying the correction, and an
inner Monte Carlo solver that is generating the correction via the
adjoint method. The parallel aspects of both these components must be
considered.

%%---------------------------------------------------------------------------%%
\section{Domain Decomposition for Monte Carlo}
\label{sec:msod}
As observed in the discussion on parallel Krylov methods, large-scale
problems will surely have their data partitioned such that each
parallel process owns a subset of the equations in the linear
system. Given this convention, the adjoint Monte Carlo algorithm must
perform random walks over a domain that is decomposed and must remain
decomposed due to memory limitations. This naturally leads us to seek
parallel algorithms that handle domain decomposition.

In the context of radiation transport, Brunner and colleagues provided
a survey of algorithms for achieving this as implemented in production
implicit Monte Carlo codes \citep{brunner_comparison_2006}. In their
work they identify two data sets that are required to be communicated:
the sharing of particles that are transported from one domain to
another and therefore from one processor to another and a global
communication that signals if particle transport has been completed on
all processors. The algorithms presented are a fully-locking
synchronous scheme, an asynchronous-send/synchronous-receive pattern,
a traditional master/slave scheme, and a modified master/slave scheme
the implements a binary tree pattern for the global reduction type
operations needed to communicate between the master and slave
processes. They observed that the modified master/slave scheme
performed best in that global communications were implemented more
efficiently than those required by the asynchronous
scheme. Furthermore, none of these schemes handled load-imbalanced
cases efficiently. Such cases will be common if the source sampled in
the Monte Carlo random walk is not isotropic and not evenly
distributed throughout the global domain. It was noted that
efficiencies were improved by increasing the frequency by which
particle data was communicated between domain-adjacent
processors. However, this ultimately increases communication costs. In
2009, Brunner extended his work by using a more load-balanced approach
with a fully asynchronous communication pattern
\citep{brunner_efficient_2009}. Although the extended implementation
was more robust and allowed for scaling to larger numbers of
processors, performance issues were still noted with parallel
efficiency improvements needed in both the weak and strong scaling
cases for unbalanced problems. These results led Brunner to conclude
that a combination of domain decomposition and domain replication
could be used to solve some of these issues.

\subsection{Multiple-Set Overlapping-Domain Decomposition}
\label{subsec:msod}
In 2010, Wagner and colleagues developed the \textit{multiple-set
  overlapping-domain} (MSOD) decomposition for parallel Monte Carlo
applications for full-core light water reactor analysis
\citep{wagner_hybrid_2010}. In their work, an extension of Brunner's,
their scheme employed the similar parallel algorithms for particle
transport but a certain amount of overlap between adjacent domains was
used to decrease the number of particles leaving the local domain. In
addition, Wagner utilized a level of replication of the domain such
that the domain was only decomposed on $O(100)$ processors and if
replicated $O(1,000)$ times achieves simulation on $O(100,000)$
processors, thus providing spatial and particle parallelism. Each
collection of processors that constitutes a representation of the
entire domain is referred to as a set, and within a set overlap occurs
among its sub-domains. The original motivation was to decompose the
domain in a way that it remained in a physical cabinet in a large
distributed machine, thus reducing latency costs during
communication. A multiple set scheme is also motivated by the fact
that communication during particle transport only occurs within a set,
limiting communications during the transport procedure to a group of
$O(100)$ processors, a number that was shown to have excellent
parallel efficiencies in Brunner's work and therefore will scale well
in this algorithm. The overlapping domains within each set also
demonstrated reduced communication costs. On each processor, the
source is sampled in the local domain that would exist if no overlap
was used while tallies can be made over the entire overlapping domain.

To demonstrate this, consider the example adapted from Mervin's work
with Wagner and others in the same area \citep{mervin_variance_2012}
and presented in Figure~\ref{fig:msod_example}.
\begin{figure}[htpb!]
  \begin{center}
    \scalebox{1.5}{
      \input{chapters/parallel_mc/msod_example.pdftex_t} }
  \end{center}
  \caption{\textbf{Overlapping domain example illustrating how domain
      overlap can reduce communication costs.}
    \textit{All particles start in the blue region of interest. The
      dashed line represents 0.5 domain overlap between domains.}}
  \label{fig:msod_example}
\end{figure}
In this example, 3 particle histories are presented emanating from the
blue region of interest. Starting with particle A, if no domain
overlap is used then the only the blue domain exists on the starting
processor. Particle A is then transported through 3 other domains
before the history ends, therefore requiring three communications to
occur in Brunner's algorithm. If a 0.5 domain overlap is permitted as
shown by the dashed line, then the starting process owns enough of the
domain such that no communications must occur in order to complete the
particle A transport process. Using 0.5 domain overlap also easily
eliminates cases such as the represented by the path of particle C. In
this case, particle C is scattering between two adjacent domains,
incurring a large latency cost for a single particle. Finally, with
particle B we observe that 0.5 domain overlap will still not eliminate
all communications. However, if 1 domain overlap were used, the entire
geometry shown in Figure~\ref{fig:msod_example} would be contained on
the source processor and therefore transport of all 3 particles
without communication would occur.

Wagner and colleagues used this methodology for a 2-dimensional
calculation of a pressurized water reactor core and varied the domain
overlap from 0 to 3 domain overlap (a $7 \times 7$ box in the context
of our example) where a domain constituted a fuel assembly. For the
fully domain decomposed case, they observed that 76.12\% of all source
particles leave the domain. At 1.5 domain overlap, the percentage of
source particles born in the center assembly leaving the processor
domain dropped to 1.05\% and even further for 0.02\% for the 3 domain
overlap. Based on these results, this overlap approach, coupled with
the multiple sets paradigm that will scale for existing parallel
transport algorithms, provides a scalable Monte Carlo algorithm for
today's modern machines.

%%---------------------------------------------------------------------------%%
\section{Load Balancing Concerns}
\label{sec:mc_load_balancing}
Although domain decomposition was shown to be efficient in a perfectly
load balanced situation in Siegel's work \citep{siegel_analysis_2012},
careful consideration must be made for situations where this is not
the case. Given the stochastic nature of the problem and lack of a
globally homogeneous domain, parallel Monte Carlo simulations are
inherently load imbalanced. Procassini and others worked to alleviate
some of the load imbalances that are generated by both particle and
spatial parallelism and are therefore applicable to the MSOD algorithm
\citep{procassini_dynamic_2005}. They chose a dynamic balancing scheme
in which the number of times a particular domain was replicated was
dependent on the amount of work in that domain (i.e. domains with a
high particle flux and therefore more particle histories to compute
require more work). In this variation, domains that require more work
will be replicated more frequently at reduced particle counts in each
replication. Furthermore, Procassini and colleagues noted that as the
simulation progressed and particles were transported throughout the
domain, the amount of replication for each domain would vary as
particle histories began to diffuse, causing some regions to have
higher work loads and some to have smaller work loads than the initial
conditions.

Consider the example in Figure~\ref{fig:procassini_example} adapted
from Procassini's work.
\begin{figure}[htpb!]
  \begin{center}
    \scalebox{1.5}{
      \input{chapters/parallel_mc/procassini_example.pdftex_t} }
  \end{center}
  \caption{\textbf{Example illustrating how domain decomposition can
      create load balance issues in Monte Carlo.}  \textit{A domain is
      decomposed into 4 zones on 8 processors with a point source in
      the lower left zone. As the particles diffuse from the source in
      the random walk sequence as shown in the top row, their tracks
      populate the entire domain. As given in the bottom row, as the
      global percentage of particles increases in a zone, that zone's
      replication count is increased.}}
  \label{fig:procassini_example}
\end{figure}
In this example, a geometry is decomposed into 4 domains on 8
processors with a point source in the bottom left domain. To begin,
because the point source is concentrated in one domain, that domain is
replicated 5 times such the amount of work it has to do per processor
is roughly balanced with the others. As the particles begin to diffuse
away from the point source, the amount of replication is adjusted to
maintain load balance. Near the end of the simulation, the diffusion
of particles is enough that all domains have equal replication.  By
doing this, load balance is improved as each domain has approximately
equal work although each domain may represent a different spatial
location and have a differing number of histories to
compute. Compared to Wagner's work where the fission source was
distributed relatively evenly throughout the domain, fixed source
problems (and especially those that have a point-like source) like
those presented in Procassini's work will be more prone to changing
load balance requirements.

%%---------------------------------------------------------------------------%%
\section{Reproducible Domain Decomposed Results}
\label{sec:reproducible_mc}
The 2006 work of Brunner is notable in that the Monte Carlo codes used
to implement and test the algorithms adhered to a strict policy of
generating identical results independent of domain decomposition or
domain replication as derived from the work of Gentile and colleagues
\citep{gentile_obtaining_2005}. In Gentile's work, a procedure is
given for obtaining results reproducible to machine precision for an
arbitrary number of processors and domains. Differences can arise from
using a different random number sequence in each domain and performing
a sequence of floating point operations on identical data in a
different order, leading to variations in round-off error and
ultimately a non-identical answer. They use a simple example,
recreated below in Figure~\ref{fig:gentile_example}, that illustrates
these issues.
\begin{figure}[htpb!]
  \begin{center}
    \scalebox{1.5}{
      \input{chapters/parallel_mc/gentile_example.pdftex_t} }
  \end{center}
  \caption{\textbf{Gentile's example illustrating how domain
      decomposition can create reproducibility issues in Monte Carlo.}
    \textit{Both particles A and B start in zone 1 on processor
      1. Particle A moves to zone 2 on processor 2 and scatters back
      to zone 1 while B scatters in zone 1 and remains there. A1 and
      A2 denote the track of particle A that is in zone 1 while B1 and
      B2 denote the track of particle B that is in zone 1.}}
  \label{fig:gentile_example}
\end{figure}
In this example, the domain is decomposed on two processors with each
processor owning one of the two zones. Starting with particle A, it is
born in zone 1 and is transported to zone 2 where a scattering event
occurs. Concerning the first reproducibility issue, if the same
sequence of random numbers is not used to compute the trajectory from
the new scattering event, we cannot expect to achieve the same result
if the domain were not decomposed. If the numbers are different, the
scattering event in zone 2 may keep the particle there or even eject
it from the domain if the sequence were different. The second issue is
demonstrated by adding another particle B that remains in the
domain. In this case, an efficient algorithm will transport particle A
on processor 1 until it leaves zone 1 and then transport particle
B. Particle A will not renter the domain until it has been
communicated to processor 2, processor 2 performs the transport, and
it is communicated back to processor 1. If we are doing a track-length
tally in zone 1, then we sum the tracks lengths observed in that
zone. In the single processor, single zone case particle A would be
transported in its entirety and then particle B transported. This
would result in a tally sum with the following order of operations:
$(((A1+A2)+B1)+B2)$. If we were instead to use 2 processors, we would
instead have the following order: $(((A1+B1)+B2)+A2)$. In the context
of floating point operations, we cannot expect these to have an
identical result to machine precision as round-off approximations will
differ resulting in non-commutative addition.

Procassini's solutions to these problems are elegant in that they
require a minimal amount of modification to be applied to the Monte
Carlo algorithm. To solve the first issue, in order to ensure each
particle maintains an invariant random number sequence that determines
its behavior regardless of domain decomposition, each particle is
assigned a random number seed that describes its current state upon
entering the domain of a new processor. These seeds are derived from
the actual geometric location of the particle such that it is
decomposition invariant. Non-commutative floating point operations are
overcome by instead mapping floating point values to 64-bit integer
values for which additions will always be commutative. Once the
operations are complete, these integers are mapped back to floating
point values.

%%---------------------------------------------------------------------------%%
\section{Parallel Adjoint Method}
\label{sec:parallel_adjoint}
We can take much of what was learned from the survey of parallel Monte
Carlo methods for radiation transport and directly apply it to a
parallel formulation of our stochastic linear solvers. Direct analogs
can be derived from these works by noting that the primary difference
between solving a linear system with Monte Carlo methods and fixed
source Monte Carlo transport problems is the content of the Markov
chains that are generated. The transitions represented by these chains
are bound by probabilities and weights and are initiated by the
sampling of a source. In the context of transport problems, those
transitions represent events such as particle scattering and
absorption with probabilities that are determined by physical data in
the form of cross sections. For stochastic matrix inversion, those
transitions represent moving between the equations of the linear
system (and therefore the physical domain which they represent) and
their probabilities are defined by the coefficients of those
equations. Ultimately, we tally the contributions to generate
expectation values in the desired states as we progress through the
chains. Therefore, parallel methods for Monte Carlo radiation
transport can be abstracted and we can use those concepts that apply
to matrix inversion methods as an initial means of developing a
parallel Neumann-Ulam-type solver. Based on the results observed in
the last decade of parallel Monte Carlo development, we can generate
many practical questions that this type of work could answer.

Given a decomposed domain, per Wagner's work
\citep{wagner_hybrid_2010} is clear that domain overlap significantly
reduces the amount of communication required between adjacent domains
as histories move to states that are not owned by the local
processor. How much domain overlap is suitable for matrix inversion
problems? Are we memory limited by large problems such that only so
much overlap is feasible? Are there metrics related to the properties
of the matrix including the eigenvalues and sparsity pattern such that
we can provide guidelines for selecting the amount of overlap
required? Furthermore, as Wagner and colleagues observed
\citep{wagner_hybrid_2010}, with marginal domain overlap for reactor
problems, the percentage of histories leaving the local domain can
easily be reduced to less than 1\%. Evans and colleagues in their
initial MCSA development typically used only 50 particle histories in
order to compute a correction in an MCSA iteration
\citep{evans_monte_2012}. Per the Central Limit Theorem, that
correlates to a statistical uncertainty of 14.1\% in the correction,
yet as compared to conventional Krylov solvers, MCSA achieved
identical numerical results. If good convergence and numerically
accurate solutions can be achieved with such a large uncertainty in
the correction computation, then perhaps with enough domain overlap
the minimal amount of histories that do transition to non-local states
can be ignored and thus eliminate all communication in the parallel
Monte Carlo adjoint solver transport sequence and create an
embarrassingly parallel method. Given Gropp's work on parallel Krylov
methods that we discussed in
\S~\ref{subsec:projection_method_performance} and the empirical
results of Siegel \citep{siegel_analysis_2012}, we know that these
nearest neighbor computations between adjacent domains do not scale as
well as global reduction operations and therefore we are improving
scaling by eliminating them from the transport sequence. It will be
important to determine if the expectation value bias in the MCSA
correction generated by this approximation will remain within the
bounds of the already high statistical uncertainty for unbiased
estimates, thus providing numerically equivalent results.

From a domain replication perspective, this may be difficult to
achieve with a production scale linear solver. Typically, memory is at
a premium and therefore the more distinct domains available in the
decomposition, the spatially finer and/or the numerically more
accurate the discretization that can be implemented. How much
replication is possible for large problems? How does replication
facilitate parallel performance when coupled with domain overlap? How
can we measure how much domain overlap is feasible? Replicating
domains may therefore run into memory limitations for exceptionally
large problems such that the operator, solution vector, and source
vector must be copied in their entirety multiple times. From a
resiliency standpoint, such an operation will be required, and
therefore its performance and memory implications on conventional
problems must be analyzed.

%%---------------------------------------------------------------------------%%
\section{Parallel MCSA}
\label{sec:parallel_mcsa}
With a parallel adjoint Neumann-Ulam solver implementation, the
parallel implementation of the MCSA method will be trivial. Recall the
MCSA iteration procedure outlined in Eq~(\ref{eq:mcsa}). In
\S~\ref{sec:parallel_krylov_methods} we discussed parallel matrix and
vector operations as utilized in conventional Krylov methods. We
utilize these here for the parallel MCSA implementation. In the first
step, a parallel matrix-vector multiply is used to apply the split
operator to the previous iterate's solution. A parallel vector update
is then performed with the source vector to arrive at the initial
iteration guess. In the next step, the residual is computed by the
same operations where now the operator is applied to the solution
guess with a parallel matrix-vector multiply and then a parallel
vector update with the source vector is performed. Once the correction
is computed with a parallel adjoint Neumann-Ulam solve, this
correction is applied to the guess with a parallel vector update to
get the new iteration solution. Additionally, as given by
Eq~(\ref{eq:mcsa_stopping_criteria}), 2 parallel vector reductions
will be required to check the stopping criteria: one initially to
compute the infinity norm of the source vector, and another at every
iteration to compute the infinity norm of the residual vector. For
this implementation, all of the issues that will be potentially
generated by the parallel adjoint solver implementation will manifest
themselves here as the quality of the correction will be of intense
study.

In addition to parallel implementation and performance, MCSA's
potential for aiding advancement in non-symmetric matrix solutions
leads to a natural comparison with the GMRES algorithm. As both
solvers are aimed at the same class of problem, we desire a set of
metrics that will allow us to quantitatively compare the two. Given
that the Krylov subspace maintained by GMRES can become large, do we
benefit from a memory standpoint with an MCSA scheme in that no
subspace is required? Does this benefit outweigh the fact that the
linear operator must be explicitly formed in order to build the
transition probabilities for the random walk sequence? For
non-symmetric systems, does MCSA exhibit similar convergence
properties to Krylov methods? If a Krylov methods build a subspace,
can those memory savings in MCSA be used to implement domain
replication in the adjoint solver? Such questions can be answered by a
comparative study of the two solvers that controls the system size and
the iterations required to converge.
