\chapter{Parallel Monte Carlo Synthetic \\ Acceleration Methods\ }
\label{ch:parallel_methods}
For MCSA to be viable at the production scale for nuclear engineering
applications, scalable parallel implementations of the algorithm are
required. Reviewing the literature, MCSA has yet to be parallelized
and the Neumann-Ulam Monte Carlo method on which it is built has only
been parallelized through history-level parallelism with full domain
replication \citep{alexandrov_efficient_1998}. In order to solve large
linear transport systems with MCSA, a domain-decomposed parallel
strategy is required. In the formulation of a parallel MCSA algorithm,
we recognize that the algorithm occurs in two stages, an outer
iteration performing fixed point iteration and applying the
correction, and an inner Monte Carlo solver that is generating the
correction via the adjoint or forward methods. The parallel aspects of
both these components must be considered. Therefore, we will develop
and implement parallel algorithms for the Neumann-Ulam and MCSA
methods leveraging both the knowledge gained from the general parallel
implementations of Krylov methods reviewed in
Appendix~\ref{ch:linear_problem} and modern parallel strategies for
domain decomposed Monte Carlo as developed by the reactor physics
community.

In this chapter we briefly review particle transport methods for
domain decomposed Monte Carlo. Considering a domain decomposed
Neumann-Ulam method, we derive an analytic framework based on
algebraic quantities from which to estimate its performance when
applied to neutron transport systems. We then devise a parallel
algorithm for the Neumann-Ulam method based on the multiple-set
overlapping-domain decomposition algorithm and a parallel algorithm
for the MCSA iteration that leverages the parallel Monte Carlo
algorithm and general parallel matrix-vector operations. In identical
fashion to the serial computations presented in
Chapter~\ref{ch:spn_equations}, we use parallel solutions for the
$SP_N$ equations for the fuel assembly criticality problem to verify
the correctness of the parallel algorithm and implementation by
comparing the numerical results against production Krylov
methods. Using the verified implementation of the algorithm, we
perform parallel scaling studings to test its performance on a
leadership class machine and compare this performance to the
production Krylov methods.

%%---------------------------------------------------------------------------%%
\section{Domain Decomposed Monte Carlo\ }
\label{sec:msod}
Large-scale problems will for reasons typically related to memory
restrictions or performance have their data partitioned such that each
parallel process owns a subset of the equations in the linear
system. Given this convention, the Neumann-Ulam Monte Carlo algorithm
must perform random walks over a domain that is decomposed and must
remain decomposed to avoid the same performance and memory
restrictions that required the domain to be decomposed. To parallelize
this algorithm, we then seek parallel Monte Carlo algorithms that
handle domain decomposition.

To motivate this problem, consider the square domain presented in
Figure~\ref{fig:ddmc_example} and on this domain we imagine a Monte
Carlo particle transport problem. If the domain were decomposed into 9
subdomains as shown, each of those subdomains and their associated
data (i.e. cross sections) could be owned by a different parallel
process in the computation. In Figure~\ref{fig:ddmc_example}, the
tracks of 3 particles that are born in the center subdomain are
shown. In this example, particle A is first transported from the
center domain to the domain directly to the left. Before the
scattering event in the new domain may be processed, particle A must
vbe communicated between between the two parallel processes that own
those domains. For any given particle in the transport simulation,
this communication event may hardly occur during the lifetime of the
particle (particle B), or may occur many times (particle C) depending
on the problem parameters and the random path in phase space taken by
the particle. Scalable parallel algorithms for domain decomposed Monte
Carlo are those that handle this domain-to-domain communication of
particles effectively and balance it with the on-process computations
for particle interactions, tallies, response calculations, and other
simulation requirements.
\begin{figure}[t!]
  \begin{center}
    \scalebox{1.5}{
      \input{chapters/parallel_mc/ddmc_example.pdftex_t} }
  \end{center}
  \caption{\textbf{Domain decomposed Monte Carlo transport example
      illustrating how domain decomposition requires parallel
      communication of particles.}}
  \label{fig:ddmc_example}
\end{figure}

For a domain decomposed Neumann-Ulam method, the situation is very
much the same where instead the transport process is now discrete and
the "physics" of the transport process is the Monte Carlo game with
probabilities and weights described by
Eq~(\ref{eq:neumann_ulam_decomposition}) for the given linear
problem. Figure~\ref{fig:ddnu_example} gives the domain decomposed
Neumann-Ulam analog of particle transport problem in
Figure~\ref{fig:ddmc_example}. Imagine in this problem that we are
solving for the monoenergetic scalar neutron flux as in the model
diffusion problem presented in Appendix~\ref{chap:diffusion_problem}.
In this problem, the solution has only a spatial dependence and
therefore each point in the mesh corresponds to an equation in the
resulting linear system. In Figure~\ref{fig:ddnu_example}, the domain
has now been discretized by this mesh and again decomposed into 9
subdomains, each owning a piece of the mesh and therefore the
corresponding equations in the linear system. As we play the Monte
Carlo game presented in Chapter~\ref{ch:stochastic_methods}, the
discretization and resulting Neumann-Ulam decomposition of the problem
describes how each state in the system is coupled to the other states
in the system through the set of equations that form the linear
system. This coupling is then responsible for the discrete random walk
that each history takes as shown in this example. As discrete states
(or rows in the linear system) that do not exist in the local domain
are reached during the random walk by stochastic histories, the
histories must be communicated to the subdomain that does own the
discrete state in an analogous fashion to the particles in the
previous example.
\begin{figure}[t!]
  \begin{center}
    \scalebox{1.5}{
      \input{chapters/parallel_mc/ddnu_example.pdftex_t} }
  \end{center}
  \caption{\textbf{Domain decomposed Neumann-Ulam example illustrating
      how domain decomposition requires parallel communication of
      histories.} \textit{Each mesh point corresponds to an equation
      in the linear system and the coupling among equations is
      described by the discretization of the problem.}}
  \label{fig:ddnu_example}
\end{figure}

To further motivate using a parallel algorithm similar to particle
transport methods, again consider history A in
Figure~\ref{fig:ddnu_example}. This history begins in the center
subdomain and through the course of transitioning through states in
the system arrives at a state that exists in the subdomain directly to
the left of the center subdomain. History A must then be communicated
from the center subdomain to the subdomain immediately to the left to
continue the random walk. As with the particle transport system, we
may find that histories in a domain decomposed Neumann-Ulam method are
only communicated a few times (history B) if at all, or they may be
communicated many times (history C) depending on the discretization
and the outcome of the Monte Carlo game. In identical fashion to the
particle transport problem, a domain decomposed Neumann-Ulam method
has the same communcation requirements with all histories in both
examples requiring the same parallel operations. The amount of
communication of histories from domain to domain will be the primary
factor in the parallel scalability of the algorithm. In the next
section, the number of histories communicated in a given problem will
be quantified analytically for a simple model problem.

\clearpage

%%---------------------------------------------------------------------------%%
\section{Analytic Framework for Domain-Decomposed Monte Carlo\ }
\label{sec:analytic_framework}
To date, parallel Neumann-Ulam methods have been limited to full
domain replication with parallelism exploited through individual
histories \citep{alexandrov_efficient_1998} and in this chapter we
will exploit particle transport algorithms to alleviate this. To
accomplish this, we recognize from the literature that stochastic
histories must be transported from domain to domain as the simulation
progresses and they transition to states that are not in the local
domain. Because we have chosen a domain decomposition strategy in a
parallel environment, this means that communication of these histories
must occur between compute nodes owning neighboring pieces of the
global domain. We wish to characterize this communication not only
because communication is in general expensive, but also because these
nearest-neighbor communication sequences, specifically, have poor
algorithmic strong scaling \citep{gropp_high-performance_2001} in much
the same way as a parallel matrix-vector multiply
operation. Therefore, we desire a framework to provide a simple,
analytic theory based on the properties of the transport system that
will allow for estimates of the domain decomposed behavior of the
Neumann-Ulam method in terms of the amount of information that must be
communicated.

When solving problems where the linear operator is symmetric, a host
of analytic techniques exist based on the eigenvalue spectrum of the
operator that characterize their behavior in the context of
deterministic linear solvers. Using past work, these techniques are
adapted to the domain decomposed Neumann-Ulam method using the
one-speed, two-dimensional neutron diffusion equation and spatial
discretization presented in Appendix~\ref{chap:diffusion_problem} as a
model transport problem. Using the linear system generated by the
discretization of the model problem, we use a spectral analysis to
generate analytic relations for the eigenvalues of the operator based
on system parameters. Using the eigenvalue spectra, we then build
relationships to characterize the transport of stochastic histories in
a decomposed domain and the fraction of histories that leak from a
domain and will therefore have to be communicated. Finally, we compare
these analytic results to numerical experiments conducted with the
model transport problem.

\clearpage

\subsection{Spectral Analysis }
\label{subsec:spectral_analysis}
The convergence of the Neumann series in
Eq~(\ref{eq:adjoint_neumann_series}) approximated by the Monte Carlo
solver is dependent on the eigenvalues of the iteration matrix. We
will compute these eigenvalues by assuming eigenfunctions of the form
\citep{leveque_finite_2007}:
\begin{equation}
  \Phi_{p,q}(x,y) = e^{2 \pi \imath p x} e^{2 \pi \imath q y}\:,
  \label{eq:eigenfunction_form}
\end{equation}
where different combinations of $p$ and $q$ represent the different
eigenmodes of the solution. As these are valid forms of the solution,
then the action of the linear operator on these eigenfunctions will
yield the eigenvalues of the matrix as they exist on the unit circle
in the complex plane.

For the model problem, we first compute the eigenvalues for the
diffusion operator $\ve{D}$ by applying the operator to the
eigenfunctions and noting that $x=ih$ and $y=jh$:
\begin{multline}
  \ve{D}\Phi_{p,q}(x,y) = \lambda_{p,q}(\ve{D})
  =\\ -\frac{D}{6h^2}\Big[4 e^{-2 \pi \imath p h} + 4 e^{2 \pi \imath
      p h} + 4 e^{-2 \pi \imath q h} + 4 e^{2 \pi \imath q h} + e^{-2
      \pi \imath p h} e^{-2 \pi \imath q h} \\ + e^{-2 \pi \imath p h}
    e^{2 \pi \imath q h} + e^{2 \pi \imath p h} e^{-2 \pi \imath q h}
    + e^{2 \pi \imath p h} e^{2 \pi \imath q h} - 20\Big] + \Sigma_a
  \:.
  \label{eq:deriv_diff_1}
\end{multline}
Using Euler's formula, we can collapse the exponentials to
trigonometric functions:
\begin{equation}
  \lambda_{p,q}(\ve{D}) = -\frac{D}{6h^2}[ 8 \cos(\pi p h) + 8
    \cos(\pi q h) + 4 \cos(\pi p h) \cos(\pi q h) - 20] + \Sigma_a\:.
  \label{eq:deriv_diff_2}
\end{equation}

As Eq~(\ref{eq:diffusion_eq}) is diagonally dominant, point Jacobi
preconditioning as outlined in
\S~\ref{subsec:stochastic_preconditioning} is sufficient to reduce the
spectral radius of the iteration matrix below unity and therefore
ensure convergence of the Neumann series. Applying this
preconditioner, we are then solving the following diffusion system:
\begin{equation}
  \ve{M}^{-1} \ve{D} \boldsymbol{\phi} = \ve{M}^{-1} \ve{s}\:.
  \label{eq:precond_diffsion}
\end{equation}
The operator $\ve{M}^{-1} \ve{D}$ is merely the original diffusion
operator with each row scaled by the diagonal component. As we have
defined a homogeneous domain, the scaling factor, $\alpha$, is the
same for all rows in the operator and defined as the $\phi_{i,j}$
coefficient from Eq~(\ref{eq:fd_system}):
\begin{equation}
  \alpha = \Bigg[\frac{10 D}{3 h^2} + \Sigma_a\Bigg]^{-1}\:.
  \label{eq:jacobi_scaling}
\end{equation}
Using this coefficient, we then have the following spectrum of
preconditioned eigenvalues:
\begin{equation}
  \lambda_{p,q}(\ve{M}^{-1} \ve{D}) = \alpha \lambda_{p,q}(\ve{D})\:.
  \label{eq:preconditioned_eigenvalues}
\end{equation}

The spectral radius of the iteration matrix is obtained by seeking its
largest eigenvalue. As with the diffusion operator, we can use the
same analysis techniques to find the eigenvalues for the iteration
matrix. We use a few simplifications by noting that if the Jacobi
preconditioned iteration matrix is $\ve{H} = \ve{I} -
\ve{M}^{-1}\ve{D}$, the we except all terms on the diagonal of the
iteration matrix to be zero such that we have the following stencil:
\begin{multline}
  \ve{H}\boldsymbol{\phi} = \frac{\alpha D}{6h^2}\Big[4 \phi_{i-1,j} +
    4 \phi_{i+1,j} + 4 \phi_{i,j-1} + 4 \phi_{i,j+1}
    +\\ \phi_{i-1,j-1} + \phi_{i-1,j+1} + \phi_{i+1,j-1} +
    \phi_{i+1,j+1}\Big]\:.
  \label{eq:iteration_stencil}
\end{multline}
Inserting the eigenfunctions defined by
Eq~(\ref{eq:eigenfunction_form}) we get:
\begin{multline}
  \lambda_{p,q}(\ve{H}) = \frac{\alpha D}{6h^2}\Big[4 e^{-2 \pi \imath p
      h} + 4 e^{2 \pi \imath p h} + 4 e^{-2 \pi \imath q h} + 4 e^{2
      \pi \imath q h} + e^{-2 \pi \imath p h} e^{-2 \pi \imath q h}
    \\ + e^{-2 \pi \imath p h} e^{2 \pi \imath q q} + e^{2 \pi \imath
      p h} e^{-2 \pi \imath q h} + e^{2 \pi \imath p h} e^{2 \pi
      \imath q h}\Big]\:,
  \label{eq:iteration_deriv}
\end{multline}
which simplifies to:
\begin{equation}
  \lambda_{p,q}(\ve{H}) = \frac{\alpha D}{6h^2}[ 8 \cos(\pi p h) + 8
    \cos(\pi q h) + 4 \cos(\pi p h) \cos(\pi q h)]\:,
  \label{eq:iteration_spectrum}
\end{equation}
giving the eigenvalue spectrum for the Jacobi preconditioned iteration
matrix. To find the maxium eigenvalue,
Eq~(\ref{eq:preconditioned_eigenvalues}) is plotted as a function of
$p$ with $p=q$ in Figure~\ref{fig:diffusion_spectrum} for various
values of $\Sigma_a$.
\begin{figure}[t!]
  \begin{center}
    \includegraphics[width=5in,clip]{chapters/parallel_mc/diffusion_spectrum.png}
  \end{center}
  \caption{\textbf{Eigenvalue spectra for the diffusion equation.}}
  \label{fig:diffusion_spectrum}
\end{figure}
We find that the maximum eigenvalue exists when $p=q=0$, giving the
following for the spectral radius of the Jacobi preconditioned
iteration matrix:
\begin{equation}
  \rho(\ve{H}) = \frac{10 \alpha D}{3 h^2}\:.
  \label{eq:iteration_radius}
\end{equation}

\clearpage 

\subsection{Neumann Series Convergence }
\label{subsec:neumann_convergence}
As outlined in \S\ref{sec:mc_preliminaries}, the adjoint Monte Carlo
method is effectively an approximation to a stationary method. In the
adjoint Neumann-Ulam method, $k$ iterations, equivalent to $k$
applications of the iteration matrix, are approximated by a random
walk of average length $k$ to yield the summation in
Eq~(\ref{eq:adjoint_neumann_solution})
\citep{dimov_new_1998,danilov_asymptotic_2000}. This random walk
length, or the number of transitions before the termination of a
history (either by the weight cutoff, absorption, or exiting the
global domain) is therefore approximately the number of stationary
iterations required to converge to the specified tolerance. In the
case of the adjoint Neumann-Ulam method, no such tolerance exists,
however, we have specified a weight cutoff, $W_c$, that determines
when low-weight histories will be prematurely terminated as their
contributions are deemed minute. After $k$ iterations, a stationary
method is terminated as the error has reached some fraction,
$\epsilon$, of the initial error:
\begin{equation}
  ||\ve{e}^{k}||_2 = \epsilon ||\ve{e}^0||_2\:.
  \label{eq:linear_k_iter_norm4}
\end{equation}
Per Eq~(\ref{eq:linear_k_iter_norm3}), we see that this fraction is
equivalent to $\epsilon = \rho(\ve{H})^k$. For the adjoint
Neumann-Ulam method, if we take this fraction to be the weight cutoff,
a measure of how accurately the contributions of a particular history
to the solution are tallied, we then have the following relationship
for $k$:
\begin{equation}
  k = \frac{ \log(W_c) }{ \log( \rho(\ve{H}) ) }\:.
  \label{eq:analytic_k}
\end{equation}
This then gives us a means to estimate the length of the random walks
that will be generated from a particular linear operator based on the
eigenvalues of its iteration matrix (independent of the linear
operator splitting chosen) and based on the weight cutoff parameter
used in the Neumann-Ulam method.

\subsection{Domain Leakage Approximations }
\label{subsec:domain_leak_approx}
In a domain decomposed situation, not all histories will remain within
the domain they started in and must instead be communicated. This
communication, expected to be expensive, was analyzed by Siegel and
colleagues for idealized, load balanced situations for full nuclear
reactor core Monte Carlo neutral particle simulations
\citep{siegel_analysis_2012}.  To quantify the number of particles
that leak out of the local domain they define a leakage fraction,
$\Lambda$, as:
\begin{equation}
  \Lambda = \frac{average\ \#\ of\ particles\ leaving\ local\ domain}
          {total\ of\ \#\ of\ particles\ starting\ in\ local\ domain}\:.
          \label{eq:leakage_fraction}
\end{equation}
For their studies, Siegel and colleagues assumed that the value of
$\Lambda$ was dependent on the total cross section of the system via
the Wigner rational approximation. Outlined more thoroughly by Hwang's
chapter in \citep{azmy_nuclear_2010}, we will use both the Wigner
rational approximation and the mean chord approximation as a means to
estimate the leakage fraction.

In the case of domain decomposed linear systems, we can use diffusion
theory to estimate the optical thickness of a domain in the
decomposition and the corresponding leakage fraction in terms of
properties of the linear operator and the discretization. To begin we
must first calculate the mean distance a Monte Carlo history will move
in the grid by computing the mean squared distance of its movement
along the chord of length $l$ defined across the domain. After a
single transition a history will have moved a mean squared distance
of:
\begin{equation}
  \langle \bar{r_1^2} \rangle = (n_s h)^2\:,
  \label{eq:step_1_length}
\end{equation}
where $h$ is the size of the discrete grid elements along the chord
and $n_s$ is the number of grid elements a history will move on
average every transition. For our diffusion model problem, $n_s$ would
equate to the expected number of states in the $i$ (or $j$ as the problem is
symmetric) direction that a history will move in a single
transition and is dependent on the stencil used for the
discretization. After $k$ transitions in the random walk, the history
will have moved a mean squared distance of:
\begin{equation}
  \langle \bar{r_k^2} \rangle = k (n_s h)^2\:.
  \label{eq:step_k_length}
\end{equation}
If our chord is of length $l$ and there are $n_i$ grid elements (or
states to which a history may transition) along that chord, then $h =
l / n_i$ giving:
\begin{equation}
  \langle \bar{r_k^2} \rangle = k \Bigg(\frac{n_s l}{n_i}\Bigg)^2\:.
  \label{eq:step_k_length_sub}
\end{equation}
From diffusion theory, we expect the average number of interactions
along the chord to be:
\begin{equation}
  \tau = \frac{l}{2 d \sqrt{\langle \bar{r_k^2} \rangle}}\:,
  \label{eq:optical_thickness_1}
\end{equation}
where $d$ is the dimensionality of the problem and $\sqrt{\langle
  \bar{r_k^2} \rangle}$ is effectively the mean free path of the Monte
Carlo history in the domain. We can readily interpret $\tau$ to be the
\textit{effective optical thickness} of a domain of length
$l$. Inserting Eq~(\ref{eq:step_k_length_sub}) we arrive at:
\begin{equation}
  \tau = \frac{n_i}{2 d n_s \sqrt{k}}\:,
  \label{eq:optical_thickness_2}
\end{equation}
which if expanded with Eq~(\ref{eq:analytic_k}) gives us the final
relation for the effective optical thickness:
\begin{equation}
  \tau = \frac{n_i}{2 d n_s}
  \sqrt{\frac{\log(\rho(\ve{H}))}{\log(W_c)}}\:.
  \label{eq:optical_thickness_3}
\end{equation}

For optically thin domains, we expect that most histories will be
communicated, while optically thick domains will leak the fraction of
histories that did not interact within. Using the optical thickness
defined in Eq~(\ref{eq:optical_thickness_3}), we can then complete the
leakage approximations by defining the bounds of $\tau \rightarrow 0,
\Lambda \rightarrow 1$ and $\tau \rightarrow \infty, \Lambda
\rightarrow \tau^{-1}$.  With these bounds we can then define the
leakage fraction out of a domain for the adjoint Neumann-Ulam method
using the Wigner rational approximation:
\begin{equation}
  \Lambda = \frac{1}{1+\tau}\:,
  \label{eq:wigner_domain_leakage}
\end{equation}
and using the mean-chord approximation:
\begin{equation}
  \Lambda = \frac{1-e^{-\tau}}{\tau}\:.
  \label{eq:mean_chord_domain_leakage}
\end{equation}
Here, the leakage fraction is explicitly bound to the eigenvalues of
the iteration matrix, the size of the domain, the content of the
discretization stencil, and the weight cutoff selected to terminate
low weight histories.

\subsection{Numerical Experiments }
\label{subsec:numerical_experiments}
To test the relationships developed by the spectral analysis, we form
two simple numerical experiments using the diffusion model problem:
one to measure the length of the random walks as a function of the
iteration matrix eigenvalues, and one to measure the domain leakage
fraction as a function of the iteration matrix eigenvalues and the
discretization properties. Before doing this, we verify our
computation of the spectral radius of the iteration matrix by
numerically computing the largest eigenvalue of the diffusion operator
using an iterative eigenvalue solver. For this verification, a $100
\times 100$ square grid with $h=0.01$, $h=0.1$, and $h=1.0$ and the
absorption cross varied from 0 to 100 while the scattering cross
section was fixed at unity. Figure~\ref{fig:measured_spec_rad} gives
the measured spectral radius of the iteration matrix and the computed
spectral radius for the preconditioned diffusion operator using
Eq~(\ref{eq:iteration_radius}) as function of the absorption to
scattering ratio $(\Sigma_a / \Sigma_s)$. Excellent agreement was
observed between the analytic and numerical results with all data
points computed within the tolerance of the iterative eigenvalue
solver.
\begin{figure}[t!]
  \begin{spacing}{1.0}
    \begin{center}
      \includegraphics[width=5in,clip]{chapters/parallel_mc/measured_spec_rad.png}
    \end{center}
    \caption{\textbf{Measured and analytic preconditioned diffusion
        operator spectral radius as a function of the absorption cross
        section to scattering cross section ratio.} \textit{Values of
        $h=0.01$, $h=0.1$, and $h=1.0$ were used. The red data was
        computed numerically by an eigensolver while the black dashed
        data was generated by Eq~(\ref{eq:iteration_radius}).}}
    \label{fig:measured_spec_rad}
  \end{spacing}
\end{figure}

\subsubsection{Random Walk Length }
\label{subsubsec:walk_length}
With the eigenvalue derivations verified, we can go about setting up
an experiment to measure the length of the random walks generated by
the adjoint Neumann-Ulam solver. To do this, we again use a $100
\times 100$ square grid with $h=0.1$ and the absorption cross varied
from 0 to 100 while the scattering cross section was fixed at
unity. Three weight cutoff values of \sn{1}{-2}, \sn{1}{-4}, and
\sn{1}{-8} were used with 10,000 histories generated by a point source
of strength 1 in the center of the domain. For each of the histories,
the number of transitions made was tallied to provide an effective
value of $k$ for each history. This value was then averaged over all
histories to get a measured value of $k$ for the particular
operator. On the left, Figure~\ref{fig:measured_length} presents these
measurements as well as the analytic result computed by
Eq~(\ref{eq:analytic_k}) as a function of the iteration matrix
spectral radius, $\rho(\ve{H})$. On the right,
Figure~\ref{fig:measured_length} gives the relative difference between the
predicted and observed results. We note good qualitative agreement
between the measured and analytic results. However, we observe a
larger relative difference for both long and short random walks.
\begin{figure}[t!]
  \begin{spacing}{1.0}
    \begin{center}
      \includegraphics[width=6.0in,clip]{chapters/parallel_mc/measured_length_2.pdf}
    \end{center}
    \caption{\textbf{Measured and analytic random walk length as a
        function of the iteration matrix spectral radius.} \textit{The
        weight cutoff was varied with \sn{1}{-2}, \sn{1}{-4}, and
        \sn{1}{-8}. In the left plot, the red data was computed
        numerically by an adjoint Neumann-Ulam implementation while
        the black dashed data was generated by
        Eq~(\ref{eq:analytic_k}). In the right plot, the relative
        difference between the predicted and measured results is
        presented for each weight cutoff.}}
    \label{fig:measured_length}
  \end{spacing}
\end{figure}

\subsubsection{Domain Leakage }
\label{subsubsec:domain_leakage}
Finally, we seek to measure the leakage from a domain in a domain
decomposed Monte Carlo calculation and assess the quality of our
analytic relation for the optical thickness of a domain and the
associated leakage approximations. For this experiment, a square grid
with $h=0.1$ was decomposed into 9 square domains, 3 in each cardinal
direction with measurements occurring in the central domain without
boundary grid points. For the cross sections, the absorption cross
section was varied from 1 to 100 while the scattering cross
section was set to zero to create a purely absorbing environment
with weight cutoff of \sn{1}{-4}. The optical thickness of these
domains will vary as a function of the absorption cross section if
the other parameters are fixed. 

To compute the optical thickness, along with the spectral radius as
given by Eq~(\ref{eq:iteration_radius}), we also need the parameters
$n_i$ and $n_s$ which respectively describe the typical domain length
and the average number of states moved along that typical length per
history transition. For our grid above, the domains are varied in size
with $50 \times 50$, $100 \times 100$, and $200 \times 200$ cells
giving $n_i=50$, $n_i=100$, and $n_i=200$ grid points or states along
the typical length of the domain respectively. Looking at the
Laplacian stencil in Eq~(\ref{eq:nine_point_stencil}), we see that all
history transitions will only move a single state in either the $i$ or
$j$ directions due to the symmetry of the problem. Furthermore, if we
choose the $i$ direction, not all states we will transition to will
move the history in that direction. Therefore, we look to the
definition of the iteration matrix in Eq~(\ref{eq:iteration_stencil})
and the definition of the adjoint probability matrix in
Eq~(\ref{eq:adjoint_probability}) to estimate the $n_s$ parameter. For
a particular transition starting at state $(i,j)$, 6 of the 8 possible
new states in the stencil move the history in $i$ direction with
relative coefficients of 4 for moving in the $(\pm i,0)$ direction and
of 1 for moving in the $(\pm i,\pm j)$. These coefficients dictate the
frequency those states are visited relative to the others. For those 6
states we can visit along the typical length, their sum is 12 out of
the total 20 for the coefficients for all possible states with their
ratio giving $n_s = \frac{3}{5}$.

To compute the leakage fraction numerically, \sn{3}{5} histories were
sampled from a uniform source of strength unity over the global
domain. At the start of a stage of histories, the number of histories
starting in the center domain was computed and as the stage
progressed, the number of histories that exited that domain was
tallied with the ratio of the two numbers providing a numerical
measure for the leakage fraction. Figure~\ref{fig:measured_leakage}
gives the domain leakage measurements for the domain in the center of
the global grid as well as the analytic result computed by
Eqs~(\ref{eq:wigner_domain_leakage}) and
(\ref{eq:mean_chord_domain_leakage}) as a function of the iteration
matrix spectral radius.
\begin{figure}[t!]
  \begin{spacing}{1.0}
    \begin{center}
      \includegraphics[width=6.0in,clip]{chapters/parallel_mc/leakage_variation_2.pdf}
    \end{center}
    \caption{\textbf{Measured and analytic domain leakage as a
        function of the iteration matrix spectral radius.} \textit{To
        test the behavior with respect to domain size, $n_i=50$,
        $n_i=100$,and $n_i=200$ were used. The red data was computed
        numerically by a domain-decomposed adjoint Neumann-Ulam
        implementation, the black dashed data was generated by
        Eq~(\ref{eq:mean_chord_domain_leakage}) using the mean-chord
        approximation, and the dashed-dotted black data was generated
        by Eq~(\ref{eq:wigner_domain_leakage}) using the Wigner
        rational approximation.}}
    \label{fig:measured_leakage}
  \end{spacing}
\end{figure}
Again, we note good qualitative agreement between the measured and
analytic quantities but we begin to see the limits of the leakage
approximations. 

To compare the quality of the two approximations, the absolute
difference between the computed leakage fraction and that generated by
the Wigner rational and mean chord approximations is plotted in
Figure~\ref{fig:leakage_error} for all domain sizes tested.
\begin{figure}[t!]
  \begin{spacing}{1.0}
    \begin{center}
      \includegraphics[width=6.0in,clip]{chapters/parallel_mc/leakage_error_2.pdf}
    \end{center}
    \caption{\textbf{Measured and analytic domain leakage absolute
        difference as a function of the iteration matrix spectral radius.}
      \textit{To test the behavior with respect to domain size,
        $n_i=50$ (green), $n_i=100$ (blue), and $n_i=200$ (red) were
        used. The dashed lines represent the difference using the Wigner
        rational approximation while the solid lines represent the
        difference using the mean-chord approximation.}}
    \label{fig:leakage_error}
  \end{spacing}
\end{figure}
From these difference results, the mean chord approximation is shown to
have a lower difference for ill-conditioned systems as compared to the
Wigner approximation while the Wigner approximation produces less
difference for more well-conditioned systems. We also note that for the
optically thick domains, the difference is likely corresponded to that
observed in Figure~\ref{fig:measured_length} for the $k$ parameter
while the large relative difference in $k$ for optically thin domains does
not affect the approximation significantly. In general, the mean chord
approximation is a better choice to estimate the leakage fraction in a
domain from the adjoint Neumann-Ulam method and except for a single
data point with $n_i=50$, the mean chord approximation yielded leakage
fractions within 0.05 of the measured results. As the domain becomes
more optically thick (with both increasing $n_i$ and decreasing
$\rho(\ve{H})$), the approximations are more accurate.

\clearpage

%%---------------------------------------------------------------------------%%
\section{Domain Decomposed Neumann-Ulam Algorithm\ }
\label{sec:asynchronous_algorithm}
In the context of radiation transport, in 2009 Brunner and colleagues
provided a fully asynchronous domain decomposed parallel aglorithm as
implemented in production implicit Monte Carlo codes
\citep{brunner_efficient_2009}. We will adapt their algorithm and
directly apply it to a parallel formulation of the Neumann-Ulam
method. Direct analogs can be derived from their works by noting that
the primary difference between solving a linear transport system with
Monte Carlo methods and traditional fixed source Monte Carlo transport
problems is the content of the Markov chains that are generated. The
transitions represented by these chains are bound by probabilities and
weights and are initiated by the sampling of a discrete source. In the
context of transport problems, those transitions represent events such
as particle scattering and absorption with probabilities that are
determined by physical data in the form of cross sections.

For stochastic matrix inversion, those transitions represent moving
between the equations of the linear system (and therefore the
components of phase space which they represent) and their
probabilities are defined by the coefficients of those
equations. Ultimately, we tally the contributions to generate
expectation values in the desired states as we progress through the
chains. Therefore, parallel methods for Monte Carlo radiation
transport can be abstracted and we can use those concepts that apply
to matrix inversion methods as an initial means of developing a
parallel Neumann-Ulam-type solver. 

In this section Brunner and Brantley's fully asynchronous algorithm is
presented along with its application to the Neumann-Ulam method. For
considerably more detail, the algorithm presented can be found in
\citep{brunner_efficient_2009} and was effectively implemented
verbatim for this work. In their work they identify two data sets that
are required to be communicated: the sharing of particles that are
transported from one domain to another and therefore from one
processor to another and a global communication that signals if
particle transport has been completed on all processors. Both of these
communication sequences will be addressed at a high level along with
how the Monte Carlo data structures they require are constructed in
parallel.

\subsection{Parallel Transport Domain and Source }
\label{subsec:domain_generation}
To utilize a parallel transport algorithm, we must generate the
required data with the correct parallel decomposition. For the
Neumann-Ulam method, the transport domain consists of all states in
the system that are local, and the probabilities and weights for all
state transitions possible in the local domain. At the core of this
representation is the Neumann-Ulam decomposition of the linear
operator as given by Eq~\ref{eq:neumann_ulam_decomposition}. Given an
input decomposition for the linear operator, by definition the
Neumann-Ulam decomposition will have the same parallel
decomposition. In addition, any modification made to the linear
operator through preconditioning or relaxation parameters will also
modify the Neumann-Ulam decomposition. All possible transitions are
described by the local graph of the sparse input matrix. In addition,
any left preconditioning or relaxation parameters will modify the
right hand side of the linear system and therefore the fixed source in
the Monte Carlo calculation. This source vector will have the same
parallel decomposition as the input matrix and therefore all of the
birth states for the histories will exist for at least a single
transition event within the local domain.

Compared to the serial construction of the Neumann-Ulam decomposition,
data for all states that is required to be on process must be
collected in parallel. For the pure domain decomposition case, given
an input parallel decomposition for the Neumann-Ulam decomposition,
for all local states $m$ we require access to $P_{mn}$ and $W_{mn}$
(or $P^T_{mn}$ and $W^T_{mn}$) for the adjoint method for all possible
states $n$. Given by Figure~\ref{fig:diffusion_graph}, consider the
adjacency graph of a single state in the neutron diffusion matrix for
the model problem presented in Appendix~\ref{chap:diffusion_problem}.
\begin{figure}[htpb!]
  \begin{center}
    \scalebox{1.5}{ \input{chapters/parallel_mc/stencil_graph.pdftex_t} }
  \end{center}
  \caption{\textbf{Neutron diffusion equation state adjacency graph.}
    \textit{Adjacent boundary states are collected by traversing the
      graph one step for all local boundary states and collecting the
      equations for those states that are not local. For a given
      transition in the random walk for the diffusion problem, a
      history at mesh point $(i,j)$ may transition to all adjacent
      mesh points including itself, corresponding to a global state
      transition from $m$ to $n$ in the linear system.}}
  \label{fig:diffusion_graph}
\end{figure}

In this example, we currently reside in state $m$ of the system,
directly correlating to physical location $(i,j)$ in the mesh (the
center node). The discretization stencil of the diffusion problem
dictates the structure of this graph and the states to which a history
may transition. If we play the Monte Carlo game to move to a new state
$n$, then we may move to any of the nodes in this graph, including the
node at which we started. If grid point $(i,j)$ in this example is on
the right of the local domain boundary and we transition to the right
to state $n$ corresponding to mesh point $(i+1,j)$, we are now in a
state that is owned by the adjacent domain. We must then gather the
data in the Neumann-Ulam decomposition from the neighboring process
that owns grid point $(i+1,j)$. Doing this provides us with the data
required by the estimators permitting $P_{mn}$ and $W_{mn}$ to be
computed locally for this particular boundary transition. In addition,
we also collect the identification number for the process that owns
this state such that we have an address to send all histories that
leave the local domain boundary for this state. For the parallel
source, these adjacent states are not required as histories may only
be born in states in the local domain. Once the local Neumann-Ulam
decomposition has been generated with the proper data collected from
adjacent domains, Monte Carlo transport may proceed in parallel.

\clearpage

\subsection{The Algorithm}
\label{subsec:parallel_mc_algorithm}

Here we present in detail Brunner and Brantley's 2009 algorithm in
detail and discuss how it is adapted to parallelize the Neumann Ulam
method. Presented in Algorithm~\ref{alg:parallel_mc_algorithm}, the
top level sequence performs the task of managing history transport
through the local domain, communication of histories to adjacent
domains, and the completion of transport. For each of these specific
tasks, additional algorithms, shown in bold in
Algorithm~\ref{alg:parallel_mc_algorithm}
(e.g. \textbf{LocalHistoryTransport()}), are presented for additional
detail in the same manner as Brunner and Brantley.

\begin{algorithm}[h!]
  \caption{Parallel Neumann-Ulam Algorithm}
  \label{alg:parallel_mc_algorithm}
  \begin{algorithmic}[1]
    \State get list of neighbor processors 
    \Comment{Each neighbor owns an adjacent subdomain}
    \ForAll{neighbors}
    \State post nonblocking receive for maximum history buffer size
    \State allocate history buffer
    \EndFor
    \State \textit{historiesCompleted} = 0
    \Comment{local+child finished histories}
    \State \textit{localProcessed} = 0
    \Comment{local trajectories computed (not necessarily finished)}
    \State calculate parent and children processor ID numbers in
    binary tree
    \ForAll{child processes}
    \State post nonblocking receive for \textit{historiesCompleted} tally
    \EndFor
    \State post nonblocking receive for stop message from parent
    \While{stop flag not set}
    \If{any local histories in source or stack}
    \State \textbf{LocalHistoryTransport()}
    \State ++\textit{localProcessed}
    \EndIf
    \If{message check period == \textit{localProcessed} || no local histories}
    \State \textbf{ProcessMessages()}
    \State \textit{localProcessed} = 0
    \EndIf
    \If{no local histories}
    \State \textbf{ControlTermination()}
    \EndIf
    \EndWhile
    \State cancel outstanding nonblocking requests
    \State free all history buffers
  \end{algorithmic}
\end{algorithm}

Successful execution of this algorithm requires construction of the
parallel Neumann-Ulam decomposition as described in the preceeding
section. This data is used to begin
Algorithm~\ref{alg:parallel_mc_algorithm} where lines 1-5 use the
collected list of neighboring processors generated while building the
Neumann-Ulam decompostion to setup the set of asynchronous messages
required for domain-to-domain communication. Again, consider this
communication patter for the 9 subdomain example given by
Figure~\ref{fig:nearest_neighbor_comm}. In this pattern, each domain
has two neighbors with which they will communicate parallel histories
and this communication goes both ways as represented by the adjacent
arrows in the figure. For each set of neighbors, a non-blocking send
and receive is required with data buffers allocated with a
user-defined size prepared for incoming and outgoing histories. This
nonblocking structure is critical to the performance of the algorithm
in that it permits local history transport to receive while new work
to do is being collected. When a given process is ready to do more
work, it can check these data buffers for incoming histories requiring
further transport. In this way there is a maximum amount of overlap
between communication and transport of histories.

\begin{figure}[htpb!]
  \begin{center}
    \scalebox{1.5}{ \input{chapters/parallel_mc/domain_to_domain.pdftex_t} }
  \end{center}
  \caption{\textbf{Nearest neighbor history communication sequence.}
    \textit{Each subdomain in the system has a set of nearest
      neighbors determined by the parallel adjacency graph of the
      input matrix. The subdomains are indexed by an integer.}}
  \label{fig:nearest_neighbor_comm}
\end{figure}

Once the nearest-neighbor communication sequence has been prepared,
the completion of transport sequence is readied in lines 6 and 8-12 in
Algorithm~\ref{alg:parallel_mc_algorithm}. In these lines we are
setting up an asynchronous binary communication tree as presented for
the same 9 subdomain example in Figure~\ref{fig:binary_comm_tree}. In
this communication pattern, each process has one parent process
(except for the MASTER process 0) to which it will nonblocking send
the number of histories that terminated their transport procedure by
weight cutoff in the local domain. Equivalently, each process has up
to two child processes from which it will receive their completed
number of histories with a nonblocking receive operation. Setting up a
tree in this manner lets the completed history tally
(\textit{historiesCompleted} in the algorithm) be updated
incrementally and funnelled to the root process. Because we are
solving fixed source problems with the Neumann-Ulam algorithm without
any type of variance reduction that may generate more histories that
we started with, once the root process completion sum tallies to the
number of histories in the source, transport is complete. Once this
occurs, the root process nonblocking sends a process to its children
and each process nonblocking receives a stop message from its parent
as shown in Figure~\ref{fig:binary_comm_tree}. The stop message is
then propagated up the tree in the same manner.

\begin{figure}[htpb!]
  \begin{center}
    \scalebox{1.25}{
      \input{chapters/parallel_mc/binary_comm_tree.pdftex_t} }
    \caption{\textbf{Binary communication tree for coordinating the
        end of a parallel Neumann-Ulam solve.} \textit{Each child
        process reports to its parents how many histories completed
        within its domain. When the root process sums all complete
        histories, it forwards the stop signal to its children which
        in turn forward the message up the tree. The subdomains are
        indexed by an integer.}}
  \end{center}
  \label{fig:binary_comm_tree}
\end{figure}

With these communication structures prepared, we can now enter the
main transport loop at line 13 of
Algorithm~\ref{alg:parallel_mc_algorithm}. In this loop, a set of
mutually exclusive tasks enabled by the fully asynchronous
communication patterns are executed until the stop signal is received
from the parent process in the binary tree. In this case, mutually
exclusivivity permits all local operations to occur independently of
other processes in the system and their current state. For each
process, there are two data structures from which histories may be
obtained for the transport procedure. The first is the local source
and the second is a LIFO (last in first out) stack of histories
transported to the local domain from the adjacent domain. In lines
14-17, if histories exist in either of these data structures, then
they are transported through the local domain using
Algorithm~\ref{alg:local_history_transport}. If the history is
terminated by weight cutoff the \textit{historiesCompleted} tally is
updated. If it hits the boundary of the domain it is added to the
outgoing history buffer for the neighboring domain and if the buffer
is full, it is sent to the neighboring domain with a nonblocking
operation and the memory associated with that buffer reallocated. In
all instances that Algorithm~\ref{alg:local_history_transport} is
executed, \textit{localProcessed} is incremented to account for
histories that have been processed locally.

\begin{algorithm}[h!]
  \caption{\textbf{LocalHistoryTransport()}}
  \label{alg:local_history_transport}
  \begin{algorithmic}[1]
    \State transport history through the domain until termination
    \If{history is in neighbor boundary state}
    \State add history to neighbor buffer
    \If{neighbor buffer is full}
    \State nonblocking send history buffer to neighbor
    \State allocate new history buffer for neighbor
    \EndIf
    \Else
    \If{history terminated by weight cutoff}
    \State post-process history
    \State ++\textit{historiesCompleted}
    \EndIf
    \EndIf
  \end{algorithmic}
\end{algorithm}

Continuing in the transport loop, if there are no local histories to
transport or the \textit{localProcessed} count has reached some
user-define check frequency, lines 18-21 in
Algorithm~\ref{alg:local_history_transport} check for more histories
to transport in the incoming data buffers by calling
Algorithm~\ref{alg:process_messages}. For each neighboring domain in
the problem, if there are histories in those data buffers then they
are added to the stack from processing and the nonblocking receive
operation reinstantiated. In addition, the terminated histories count
is updated with those values received from the child processes.

\begin{algorithm}[h!]
  \caption{\textbf{ProcessMessages()}}
  \label{alg:process_messages}
  \begin{algorithmic}[1]
    \ForAll{received history buffers from neighbor}
    \State unpack number of histories in buffer
    \State add the histories to the running stack
    \State repost nonblocking receive with neighbor
    \EndFor
    \ForAll{\textit{historiesCompleted} messages from children}
    \State \textit{historiesCompleted} += message value
    \State repost nonblocking receive with child
    \EndFor
  \end{algorithmic}
\end{algorithm}

Finally, the transport loop finishes in lines 22-24 of
Algorithm~\ref{alg:local_history_transport} by calling
Algorithm~\ref{alg:control_termination} if there are no local
histories to transport in the stack or the source. In
Algorithm~\ref{alg:control_termination}, we continue to forward the
terminated history tally down to the parent in the binary tree and
send all history buffers to our neighbors, even if they are not
full. If Algorithm~\ref{alg:control_termination} is called by the
MASTER process, it checks for completion of the problem and if it is
complete, forwards to stop flag onto its children as in
Figure~\ref{fig:binary_comm_tree}. If the process is not the MASTER,
then we check for a stop signal from the parent and forward to the
child processes if transport is complete.

\begin{algorithm}[h!]
  \caption{\textbf{ControlTermination()}}
  \label{alg:control_termination}
  \begin{algorithmic}[1]
    \State nonblocking send an partially full history buffers
    \ForAll{\textit{historiesCompleted} messages from children}
    \State \textit{historiesCompleted} += message \textit{historiesCompleted}
    \State repost nonblocking receive with child
    \EndFor
    \If{MASTER processor}
    \If{\textit{historiesCompleted} == global \# of source histories}
    \State set stop flag
    \ForAll{children}
    \State nonblocking send stop message to child
    \EndFor
    \EndIf
    \Else
    \State nonblocking send \textit{historiesCompleted} to parent
    \State \textit{historiesCompleted} = 0
    \State check for stop signal from parent
    \If{stop signal from parent}
    \ForAll{children}
    \State nonblocking send stop message to child
    \EndFor
    \EndIf
    \EndIf
  \end{algorithmic}
\end{algorithm}

In this manner the transport loop continues until all process have
received the stop signal from their parents. Older versions of this
algorithm, particularly some of those presented in
\citep{brunner_comparison_2006}, use a master/slave approach as given
by Figure~\ref{fig:master_comm_tree} for the completion of a transport
stage instead of the binary tree scheme. In this approach, the root
process still manages the final summation of the completion tally, it
directly receives completed tally results from all other processes in
the problem instead of from just its children. Although this
implementation is potentially simpler to understand than the binary
tree approach, the authors of \citep{brunner_comparison_2006} observed
that this type of termination pattern, even when implemented in a
fully asynchronous manner, was a major scalability bottleneck. This
bottleneck is due to the fact that the master process falls behind the
others, creating a load imbalance even with the addition of a few
integer addition operations. We will explicitly demonstrate in scaling
studies how this bottleneck also occurs within a Neumann-Ulam
implementation of this algorithm thus requiring the implementation of
a binary communication tree.

\begin{figure}[htpb!]
  \begin{center}
    \scalebox{0.75}{
      \input{chapters/parallel_mc/master_comm_tree.pdftex_t} }
    \caption{\textbf{Master/slave scheme for coordinating the end of a
        parallel Neumann-Ulam solve.} \textit{Each slave process
        reports to the master how many histories completed within its
        domain. When the master process sums all complete histories,
        it sends the stop signal to all slave processes. The
        subdomains are indexed by an integer.}}
  \end{center}
  \label{fig:master_comm_tree}
\end{figure}

There is an additional advantage to Brunner and Brantley's work that
although not immediately applicable to this work has the potential to
provide value in the future. In addition to a robust fully
asynchronous communication pattern, this algorith may also be modified
to account for situations where the total number of histories in a
given stage are not known before starting transport. From a physics
perspective, we might expect this for situations where perhaps an
(n,2n) interaction occurs in a neutronics problem. In this case, the
algorithm is modified to account for both histories terminated and
created and several mechanisms are introduced to determine completion
of the transport stage. For future variations of this work, certain
variance reduction techniques which create histories, such as
splitting, have the potential to be successfully employed as a means
of accelerating the time to solution for a given problem using
MCSA. The parallel Neumann-Ulam algorithm presented here may be
adapted to account for these additional techniques.

\clearpage

%%---------------------------------------------------------------------------%%
\section{Multiple-Set Overlapping-Domain Algorithm\ }
\label{subsec:msod}
Although the implementation presented in the previous section was
observed by Brunner and Brantley to be robust and allowed for scaling
to large numbers of processors, performance issues were still noted
with parallel efficiency improvements needed in both the weak and
strong scaling cases for unbalanced problems. These results led them
to conclude that a combination of domain decomposition and domain
replication could be used to solve some of these issues. In 2010,
Wagner and colleagues developed the \textit{multiple-set
  overlapping-domain} (MSOD) decomposition for parallel Monte Carlo
applications for full-core light water reactor analysis
\citep{wagner_hybrid_2010}. In their work, an extension of Brunner's,
their scheme employed similar parallel algorithms for particle
transport but a certain amount of overlap between adjacent domains was
used to decrease the number of particles leaving the local domain. In
addition, Wagner utilized a level of replication of the domain such
that the domain was only decomposed on $O(100)$ processors and if
replicated $O(1,000)$ times potentially achieves efficient simulation
on $O(100,000)$ processors, thus providing both spatial and particle
parallelism.

Each collection of processors that constitutes a representation of the
entire domain is referred to as a set, and within a set overlap occurs
among its sub-domains. The original motivation was to decompose the
domain in a way that it remained in a physical cabinet in a large
distributed machine, thus reducing latency costs during
communication. A multiple set scheme is also motivated by the fact
that communication during particle transport only occurs within a set,
limiting communications during the transport procedure to a group of
$O(100)$ processors, a number that was shown to have excellent
parallel efficiencies in Brunner's work and therefore will scale well
in this algorithm. The overlapping domains within each set also
demonstrated reduced communication costs. On each processor, the
source is sampled in the local domain that would exist if no overlap
was used while tallies can be made over the entire overlapping domain.

To demonstrate this, consider the example adapted from Mervin's work
with Wagner and others in the same area \citep{mervin_variance_2012}
and presented in Figure~\ref{fig:msod_example}.
\begin{figure}[t!]
  \begin{center}
    \scalebox{1.5}{
      \input{chapters/parallel_mc/msod_example.pdftex_t} }
  \end{center}
  \caption{\textbf{Overlapping domain example illustrating how domain
      overlap can reduce communication costs.}
    \textit{All particles start in the blue region of interest. The
      dashed line represents 0.5 domain overlap between domains.}}
  \label{fig:msod_example}
\end{figure}
In this example, 3 particle histories are presented emanating from the
blue region of interest. Starting with particle A, if no domain
overlap is used then the only the blue domain exists on the starting
processor. Particle A is then transported through 3 other domains
before the history ends, therefore requiring three communications to
occur in Brunner's algorithm. If a 0.5 domain overlap is permitted as
shown by the dashed line, then the starting process owns enough of the
domain such that no communications must occur in order to complete the
particle A transport process. Using 0.5 domain overlap also easily
eliminates cases such as that represented by the path of particle
C. In this case, particle C is scattering between two adjacent
domains, incurring a large latency cost for a single
particle. Finally, with particle B we observe that 0.5 domain overlap
will still not eliminate all communications. However, if 1 domain
overlap were used, the entire geometry shown in
Figure~\ref{fig:msod_example} would be contained on the source
processor and therefore transport of all 3 particles without
communication would occur.

Wagner and colleagues used this methodology for a 2-dimensional
calculation of a pressurized water reactor core and varied the domain
overlap from 0 to 3 domain overlap (a $7 \times 7$ box in the context
of our example) where a domain contained an entire fuel assembly. For
the fully domain decomposed case, they observed that 76.12\% of all
source particles leave the domain. At 1.5 domain overlap, the
percentage of source particles born in the center assembly leaving the
processor domain dropped to 1.05\% and even further for 0.02\% for the
3 domain overlap. Based on their results, we hypothesize that the
overlap approach coupled with the multiple sets paradigm that will
enhance the scalability of the pure domain-decomposition algorithm for
Neumann-Ulam presented in the previous section.

\subsection{Generating the MSOD Decomposition }
\label{subsec:msod_generation}
In \S~\ref{subsec:domain_generation} we discussed how we generated the
parallel transport domain from the Neumann-Ulam decomposition of a
domain decomposed linear operator. We can readily adapt those data
structures to account for the extra information required by an
implementation of the MSOD algorithm. First, we consider the
generation of overlap. Conveniently, this is identical to the
neighboring states discovery problem discussed in conjuction with
Figure~\ref{fig:diffusion_graph}. To generate the boundary for the
local transport domain, we needed to gather all data from the
immediately adjacent states in the system that did not already exist
in the local domain. We solved this problem by travsersing the graph
of the matrix for each of the boundary states and determining which
processes owned those adjacent states. 

To generate overlap, we use the identical algorithm but in this case
we perform as many graph traversals as the specified amount of
overlap. For example, consider the analytic relations derived in
\S~\ref{sec:analytic_framework} for the length of random walks in a
Neumann-Ulam method and the amount of leakage from a domain
considering a neutron diffusion problem. From these relations we might
determine that for our particular problem, if we can grow the domain
by 10 additional discrete state transitions, we can reduce the number
of histories that must be comunicated by a significant fraction. We
determine the information we must gather from neighboring domains to
build the overlap by traversing the graph of the matrix outward from
the boundary 10 times. At each step, we find the data in the adjacent
domain that we require and make that data local, incrementing the size
of the overap. Once the overlap has been gathered, one extra graph
traversal on the boundary states is performed to get the new
neighboring states and their owning process.

\begin{figure}[htpb!]
  \begin{center}
    \scalebox{0.6}{ \input{chapters/parallel_mc/msod_construction.pdftex_t} }
  \end{center}
  \caption{\textbf{MSOD construction.} \textit{The linear system is
      used to construct the Monte Carlo transport domain on the
      primary set. The Monte Carlo data structures are then broadcast
      amongst the blocks to all sets in the system.}}
  \label{fig:msod_construction}
\end{figure}

\subsection{MSOD Neumann Ulam Algorithm }
\label{subsec:msod_algorithm}

\begin{figure}[htpb!]
  \begin{center}
    \scalebox{0.6}{ \input{chapters/parallel_mc/msod_tally.pdftex_t} }
  \end{center}
  \caption{\textbf{MSOD tally reduction.} \textit{The tally computed
      independently in each set is reduced across the blocks to the
      primary set. The linear system vector is updated on the primary
      set. In this example the tally vector in the block containing
      upper-left subdomains is combined. Identical reduction
      operations occur for all other blocks simultaneously.}}
  \label{fig:msod_tally_reduction}
\end{figure}

\clearpage

%%---------------------------------------------------------------------------%%
\section{Parallel MCSA\ }
\label{sec:parallel_mcsa}
With the parallel adjoint Neumann-Ulam solver implementation described
aboive, the parallel implementation of the MCSA method is
trivial. Recall the MCSA iteration procedure outlined in
Eq~(\ref{eq:mcsa}). In \S\ref{sec:parallel_krylov_methods} we discuss
parallel matrix and vector operations as utilized in conventional
Krylov methods. We utilize these here for the parallel MCSA
implementation. In the first step, a parallel matrix-vector multiply
is used to apply the split operator to the previous iterate's
solution. A parallel vector update is then performed with the source
vector to arrive at the initial iteration guess. In the next step, the
residual is computed by the same operations where now the operator is
applied to the solution guess with a parallel matrix-vector multiply
and then a parallel vector update with the source vector is
performed. Once the correction is computed with a parallel adjoint
Neumann-Ulam solve, this correction is applied to the guess with a
parallel vector update to get the new iteration
solution. Additionally, as given by
Eq~(\ref{eq:mcsa_stopping_criteria}), 2 parallel vector reductions
will be required to check the stopping criteria: one initially to
compute the infinity norm of the source vector, and another at every
iteration to compute the infinity norm of the residual vector. For
this implementation, all of the issues that will be potentially
generated by the parallel adjoint solver implementation will manifest
themselves here as the quality of the correction will be of intense
study.

%%---------------------------------------------------------------------------%%
\section{Parallel MCSA Verification\ }
\label{sec:parallel_verification}

%%---------------------------------------------------------------------------%%
\section{Parallel Performance Metrics\ }
\label{sec:parallel_performance_metrics}

How effectively the presented algorithms parallelize MCSA will be
determined by several performance measures. For this work, these
measures come directly from Keyes' primer on the scalability of domain
decomposed methods and specifically his discussion on how to measure
their parallel performance \citep{keyes_how_1999}. All of these
metrics are derived from two measured quantities: the number of
iterations required for convergence and the total wall time required
for convergence. Each of these quantities are then measured in varying
parameter studies by changing the number of processes in a simulation
and by varying the local or global problem size. Using the notation of
Keyes, we will define \textit{parallel efficiency} as $\eta(N,P)$
where the efficiency is a function of the global problem size, $N$,
and the number of processes in the calculation, $P$. There are several
types of efficiencies that will be useful in our analysis of
scalability with different definitions depending on the type of
parametric scaling study performed. For all efficiencies, a value of 1
is considered perfect while a value of 0 signals the worst possible
performance. Often, as was observed in this work, it is the case that
efficiencies above 1 are computed in certain scaling studies. For the
data presented in the following sections, an explanation of each
observed case of superunitary efficiencies will be provided.

Of primary interest to the nuclear engineer is the \textit{strong
  scaling} performance of an algorithm
\citep{siegel_analysis_2012}. In this type of scaling study, the
global problem size is fixed and the local problem size changes with
the number of processes in the calculation. In this case, as $P$
increases, $N$ decreases, thus reducing performance by having a larger
ratio of communication to computational work. This type of scaling is
important because typically the global problem size is fixed due to
the problem of interest. Consider the desire for a high fidelity
neutronics simulation of the entire core of a nuclear reactor. In this
case, the global problem is known from the geometry definition
including the number of fuel assemblies and the geometry of those
assemblies all the way down to the resolution of the space, angle, and
energy discretizations required in order to capture physical phenomena
of interest. Because the global problem size is known a priori, strong
scaling is an important measure of how useful a solution technique is
when larger and larger values of $P$ are used to solve the
problem. Ideally, the time to solution for the the problem will
decrease linearly as a function of $P$, permitting a faster time to
solution using the maximum available resources thus making sure all
$P$ are positively affecting the runtime of the problem.

For strong scaling, the objective is to then decrease the runtime of a
fixed size global problem as a linear function of $P$. We can then
define the strong scaling \textit{absolute efficiency} as:
\begin{equation}
  \eta_{strong}(N,P) = \frac{1}{P} \frac{T(N,1)}{T(N,P)}\:,
  \label{eq:strong_scaling_absolute}
\end{equation}
where $T(N,P)$ is the wall time for a computation of global size $N$
using $P$ processes. Note here that this particular definition is
measured with respect to a serial computation. On leadership class
machines, it is often the case that a problem size large enough to
effectively execute on $O(10,000)$ cores or larger is significantly
larger than any problem that may be solved by a single core due to
memory restrictions. Therefore, we consider absolute scaling for a
base case where $Q$ processes are used instead of a serial
computation:
\begin{equation}
  \eta_{strong}(N,P|Q) = \frac{Q}{P} \frac{T(N,Q)}{T(N,P)}\:.
  \label{eq:strong_scaling_absolute_ref}
\end{equation}
The absolute efficiency measure presented here only considers the
total time to solution. Often, that total time to solution may be
changing for an iterative method due to the fact that the number of
iterations required to converge may be changing as a function of
$P$. This will often happen in cases where modifying $N$ changes the
spectral character of the problem, often making it stiffer and
therefore more difficult to solve when $N$ increases. Therefore we
also consider \textit{algorithmic efficiency} from Keyes' work which
considers this potential change in iteration count:
\begin{equation}
  \eta_{alg}(N,P|Q) = \frac{I(N,Q)}{I(N,P)}\:,
  \label{eq:algorithmic_efficiency}
\end{equation}
where $I(N,P)$ is the number of iterations required to converge a
problem of global size $N$ computed using $P$ processes. We can then
adjust the absolute efficiency using this new algorithmic efficiency
in order to compute the parallel scalability of a single iteration
such that algorithmic effects may be neglected. We define the
\textit{implementation efficiency} as:
\begin{equation}
  \eta_{impl}(N,P|Q) = \frac{\eta}{\eta_{alg}}\:,
  \label{eq:implementation efficiency}
\end{equation}
using the absolute efficiency in the numerator, giving an effective
measure of scalability of a single iteration of the algorithm.

In addition to strong scaling, in certain cases it is useful to
consider the \textit{weak scaling} of an iterative method. In this
case, the ratio $N/P$ is fixed such that the local problem size
remains the same for all values of $P$. In this case, the objective is
to maintain a constant wall time while both $N$ and $P$ grow as a
fixed ratio. In this case, we define the absolute efficiency as:
\begin{equation}
  \eta_{weak}(M|N,P|Q) = \frac{T(M,Q)}{T(N,P)}\:,
  \label{eq:weak_scaling_absolute}
\end{equation}
subject to the constraint $M/Q = N/P$. The scaling is perfect if the
runtime is static for all $P$ when $N/P$ is fixed. Often, weak scaling
is of interest to the nuclear engineer due to the fact that the
computational resources available are fixed and may not be able to
contain the entire global problem of interest. In this case, an
algorithm with good weak scaling performance permits all of the
computational resources available to be used effectively to allow for
the highest fidelity problem possible to be solved. In addition, for
the resources available, the memory to processor ratio on a
distributed machine is often fixed, giving an exercise in fixed $N/P$
to solve the highest fidelity problem possible. Therefore, good weak
scaling should not be viewed as an enhancement of time to solution for
the engineer but rather permitting a higher fidelity problem to be
solved within a given time constraint. For problems where increasing
fidelity increases the stiffness of the system, there is often a
trade-off between the absolute efficiencies computed for a given
method and the degrading algorithmic efficiencies that arise when
increasing the fidelity. Both time and hardware constraints should be
considered when preparing a parallel calculation.

%%---------------------------------------------------------------------------%%
\section{Leadership-Class Parallel Scaling Studies\ }
\label{sec:leadership_scaling_studies}
For all cases, we explore scaling for just the Monte Carlo transport
kernel and MCSA as a whole iterative scheme. This will be necessary to
determine if the initial relaxation step required at each iteration is
a limiting factor in convergence. For each case, it will be
constructive to study the scaling performance as a function of
spectral radius - dictated by the cross sections and grid size to
diffusion length ratio. In addition, all of these studies should
hopefully have data from both the cluster and Titan and should all be
compared to Aztec GMRES and CG implementations with appropriate
preconditioning.

\subsection{Titan }
\label{subsec:titan}

\subsection{Communication Parameters}
\label{subsec:comm_parameters}

\begin{figure}[htpb!]
  \begin{center}
    \includegraphics[width=6in]{chapters/parallel_mc/titan_comm_parameters.pdf}
  \end{center}
  \caption{\textbf{Neumann Ulam parallel algorithm free parameter
      sensitivity.}  \textit{All runtimes were within 6.5\% of the
      fastest observed case.}}
  \label{fig:titan_comm_parameters}
\end{figure}

\clearpage

\subsection{Pure Domain Decomposition}
\label{subsec:pure_domain_decomp}

\subsubsection{Strong Scaling}
\label{subsubsec:pure_strong}

\begin{figure}[htpb!]
  \begin{center}
    \includegraphics[width=6in]{chapters/parallel_mc/titan_pure_strong.pdf}
  \end{center}
  \caption{\textbf{Pure domain decomposition strong scaling absolute
      efficiency.}  \textit{Super-linear speed-up from memory
      thrashing in base case. MCLS is an order of magnitude slower
      arithmetically, causing the higher efficiency.}}
  \label{fig:titan_pure_strong}
\end{figure}

\begin{figure}[htpb!]
  \begin{center}
    \includegraphics[width=6in]{chapters/parallel_mc/titan_strong_bvsm.pdf}
  \end{center}
  \caption{\textbf{Binary tree vs. master/slave communication scheme
      absolute efficiency.} \textit{Pure domain decomposition absolute
      strong scaling. The master/slave scheme will not scale beyond
      $O(1,000)$ cores due to load imbalance.}}
  \label{fig:titan_strong_bvsm}
\end{figure}

\clearpage

\subsubsection{Weak Scaling}
\label{subsubsec:pure_weak}

\begin{figure}[htpb!]
  \begin{center}
    \includegraphics[width=6in]{chapters/parallel_mc/titan_weak_absolute.pdf}
  \end{center}
  \caption{\textbf{Pure domain decomposition weak scaling absolute
      efficiency.}}
  \label{fig:titan_weak_absolute}
\end{figure}

\begin{figure}[htpb!]
  \begin{center}
    \includegraphics[width=6in]{chapters/parallel_mc/titan_weak_alg_eff.pdf}
  \end{center}
  \caption{\textbf{Pure domain decomposition weak scaling algorithmic
      efficiency.}}
  \label{fig:titan_weak_algorithmic}
\end{figure}

\begin{figure}[htpb!]
  \begin{center}
    \includegraphics[width=6in]{chapters/parallel_mc/titan_weak_implementation.pdf}
  \end{center}
  \caption{\textbf{Pure domain decomposition weak scaling
      implementation efficiency.}}
  \label{fig:titan_weak_implementation}
\end{figure}

\begin{figure}[htpb!]
  \begin{center}
    \includegraphics[width=6in]{chapters/parallel_mc/titan_weak_bvsm.pdf}
  \end{center}
  \caption{\textbf{Binary tree vs. master/slave communication scheme
      absolute efficiency.}  \textit{Pure domain decomposition
      absolute weak scaling. The master/slave scheme will not scale
      beyond $O(1,000)$ cores due to load imbalance.}}
  \label{fig:titan_weak_bvsm}
\end{figure}

\clearpage

\subsection{Overlapping Domain Decomposition}
\label{subsec:overlapping_domain_decomp}

\subsubsection{Strong Scaling}
\label{subsubsec:overlapping_strong}

\clearpage

\subsubsection{Weak Scaling}
\label{subsubsec:overlapping_weak}

\clearpage

\subsection{Multiple Set Decomposition}
\label{subsec:msod_decomposition}

\subsubsection{Strong Scaling}
\label{subsubsec:msod_strong}

\begin{figure}[htpb!]
  \begin{center}
    \includegraphics[width=6in]{chapters/parallel_mc/titan_strong_ms_time.pdf}
  \end{center}
  \caption{\textbf{Wall time in seconds to solution for each case.}}
  \label{fig:titan_strong_ms_time}
\end{figure}

\begin{figure}[htpb!]
  \begin{center}
    \includegraphics[width=6in]{chapters/parallel_mc/titan_strong_ms_eff.pdf}
  \end{center}
  \caption{\textbf{Absolute parallel efficiency relative to 16-core
      1-set base case.}}
  \label{fig:titan_strong_ms_eff}
\end{figure}

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{lcc}\hline\hline
      \multicolumn{1}{c}{Case}& 
      \multicolumn{1}{c}{Iterations}&
      \multicolumn{1}{c}{$\eta_{alg}$} \\\hline
      1 & 22 & 1.0 \\
      2-split & 22 & 1.0 \\
      2-replicated & 16 & 1.375 \\
      4-split & 22 & 1.0 \\
      4-replicated & 13 & 1.692 \\
      %%
      \hline\hline
    \end{tabular}
  \end{center}
  \caption{\textbf{Strong scaling algorithmic efficiency using
      multiple sets.}}
  \label{tab:ms_strong_alg_eff}
\end{table}

\begin{figure}[htpb!]
  \begin{center}
    \includegraphics[width=6in]{chapters/parallel_mc/titan_strong_ms_impeff.pdf}
  \end{center}
  \caption{\textbf{Strong scaling implementation efficiency relative
      to 16-core 1-set base case.}}
  \label{fig:titan_strong_ms_impeff}
\end{figure}

\clearpage

\subsubsection{Weak Scaling}
\label{subsubsec:msod_weak}

\begin{figure}[htpb!]
  \begin{center}
    \includegraphics[width=6in]{chapters/parallel_mc/titan_weak_ms_time.pdf}
  \end{center}
  \caption{\textbf{Wall time in seconds to solution for each case.}}
  \label{fig:titan_weak_ms_time}
\end{figure}

For a multiple set analysis of the weak scaling results, we have to
consider that adding sets to the problem is actually a form of strong
scaling. Consider a single set problem ($S=1$) for a given $P$ and
$N$. When that problem is now solved using 2 sets we have $S=2$, $N$
and $2P$. For any number of sets we then have a global problem size of
$N$ while the number of processors is $SP$ where $P$ is the number of
processors in the single set case. Therefore, $N/P$ is no longer fixed
in the weak scaling study but rather $N/(SP)$ is fixed. Therefore, we
modify the weak scaling absolute efficiency relationship to account
for this fact:
\begin{equation}
\eta_{weak}(M|N,P|Q,S) = \frac{T(M,Q)}{S \times T(N,P)}\:,
  \label{eq:ms_weak_efficiency}
\end{equation}
where now the absolute efficiency is a function of the number of sets
in the problem. For problems where adding sets improves time to
solution, this new weak scaling formulation accounts for how
efficiently those extra resources are incorporated into the
problem. Without such a modification, the largest case run for the
split 4 set case would have a computed weak scaling efficiency of
nearly 100\% using the standard formula for absolute
efficiency. However, using 4 times as many cores as the single set
case increases the runtime for a fixed global problem size and
therefore we actually expect to compute a very low efficiency that
shows this very poor use of resources.

\begin{figure}[htpb!]
  \begin{center}
    \includegraphics[width=6in]{chapters/parallel_mc/titan_weak_ms_eff.pdf}
  \end{center}
  \caption{\textbf{Weak scaling absolute efficiency relative to
      16-core 1-set base case.}}
  \label{fig:titan_weak_ms_eff}
\end{figure}

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{lcc}\hline\hline
      \multicolumn{1}{c}{Case}& 
      \multicolumn{1}{c}{Iterations}&
      \multicolumn{1}{c}{$\eta_{alg}$} \\\hline
      1 & 22 & 1.0 \\
      2-split & 22 & 1.0 \\
      2-replicated & 16 & 1.375 \\
      4-split & 22 & 1.0 \\
      4-replicated & 13 & 1.692 \\
      %%
      \hline\hline
    \end{tabular}
  \end{center}
  \caption{\textbf{Weak scaling algorithmic efficiency using multiple
      sets.}}
  \label{tab:ms_weak_alg_eff}
\end{table}

\begin{figure}[htpb!]
  \begin{center}
    \includegraphics[width=6in]{chapters/parallel_mc/titan_weak_ms_impeff.pdf}
  \end{center}
  \caption{\textbf{Implementation parallel efficiency relative to 16-core
      1-set base case.}}
  \label{fig:titan_weak_ms_impeff}
\end{figure}

\clearpage

\subsection{Subdomain Neumann-Ulam}
\label{subsec:full_clip}

Full-clip actually uses the solver as a true subdomain solver, just
like you would in a Schwarz method. In fact, this is a stochastic
realization of a Richardson iteration - they are very similar in those
regards.

Overlap is not effective because histories apparently dump most of
their useful information within the first few steps (which is why we
can use such a large weight cutoff and there is an insensitivity to
weight cutoff). So keeping them around to do extra work is not
improving iterative performance - thus increasing compute times. This
is true even for ill-conditioned cases where adding overlap might keep
a number of histories within the domain for significantly longer.

\begin{figure}[htpb!]
  \begin{center}
    \includegraphics[width=6in]{chapters/parallel_mc/titan_strong_subdomain.pdf}
  \end{center}
  \caption{\textbf{Strong scaling absolute efficiency for pure domain
      decomposition.} \textit{The subdomain Monte Carlo solver
      improves the parallel efficiency and therefore time to solution
      by eliminating all parallel communication during the Monte Carlo
      solve.}}
  \label{fig:titan_strong_subdomain}
\end{figure}

\begin{figure}[htpb!]
  \begin{center}
    \includegraphics[width=6in]{chapters/parallel_mc/titan_weak_subdomain.pdf}
  \end{center}
  \caption{\textbf{Weak scaling absolute efficiency for pure domain
      decomposition.} \textit{The subdomain Monte Carlo solver
      improves the parallel efficiency and therefore time to solution
      by eliminating all parallel communication during the Monte Carlo
      solve.}}
  \label{fig:titan_weak_subdomain}
\end{figure}

\subsubsection{Strong Scaling}
\label{subsubsec:full_clip_strong}

\clearpage

\subsubsection{Weak Scaling}
\label{subsubsec:full_clip_weak}





