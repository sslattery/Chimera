\chapter{Conclusions and Analysis\ }
\label{ch:conclusion}

For high fidelity simulations of nuclear reactors, physics solutions
are required with speed and accuracy. When the problems become large
enough, such as the simulation of an entire reactor core,
leadership-class computing facilities must be leveraged on the grounds
of requirements for both time to solution and the amount of memory
required to contain the description of the entire problem and the
solution. Looking forward to the next generation of machines,
concurrency will go up and available memory will go down, putting
pressure on physics application developers to research and develop
solution techniques that can efficiently leverage this hardware. In
nuclear reactor simulation, both neutron transport and fluid flow
calculations consume a vast amount of computational time in order to
properly characterize both steady state operational characteristics
and transient accident scenarios. In both cases, we desire solution
techniques that are aware of coming changes in hardware.

In this work, we have presented Monte Carlo Synthetic Acceleration as
a viable solution technique for both neutron transport and fluid flow
problems on future hardware. To do this, we carried out three research
and development activities. First, we applied MCSA to the $SP_N$ form
of the neutron transport equation and analyzed the preconditioning
requirements and subsequent performance of the method when compared to
conventional practices. Second, we used the knowledge gained from the
research on neutron transport to develop the FANM method for nonlinear
problems and studied its performance on three different benchmarks for
the Navier-Stokes equations to demonstrate its applicability. Finally,
we parallelized MCSA such that it may be applied to physics problems
on leadership-class computing platforms.

In this chapter, we review the work presented in this document and
describe how this work met the goals of the research. We then
reiterate the important issues discovered during the course of this
work and develop a strategy for future work to potentially alleviate
them. Finally, we close with some final remarks regarding Monte Carlo
Synthetic Acceleration.

%%---------------------------------------------------------------------------%%
\section{Monte Carlo Synthetic Acceleration Solutions for the $SP_N$ Equations\ }
\label{sec:spn_conclusion}

This work demonstrates the first application of MCSA to the neutron
transport problem. To do this, we used the $SP_N$ form of the
Boltzmann transport equation. In this form, the transport equation
takes on a diffusion-like form where the angular character of the flux
is accounted for in moment terms much like those found in the $P_N$
equations. To meet the goals of this work and drive the research and
development of MCSA for application to light water reactor problems, a
difficult nuclear fuel assembly criticality calculation was used in a
set of numerical experiments. These calculations were enabled by
incorporating MCSA as a solution scheme into the k-eigenvalue solver
in the Exnihilo production neutronics code base at Oak Ridge National
Laboratory.

When applying the new technique to the fuel assembly problem, it was
initially found that MCSA could not converge the problem with basic
Jacobi-based preconditioning. This was discovered to be due to the
fact that the transport operator generated in the eigenvalue
calculation was very ill-conditioned due to the large amount of
neutron scattering in the light water moderator. The more scattering
present in the system, the longer a random walk will take and thus a
spectral radius approaching unity was observed. A simpler neutron
diffusion problem was used to demonstrate the breakdown of MCSA when
spectral radii this large are generated.

As a result of the breakdown observation, a suite of advanced
preconditioning techniques was applied to MCSA for the fuel assembly
problem in order to reduce the spectral radius to a level below where
breakdown occurs. From the results of this analysis, ILUT
preconditioning was chosen for subsequent investigations. Using this
preconditioning, it was verified that MCSA is indeed general enough to
solve the asymmetric system generated by the $SP_N$
equations. However, the explicit MCSA preconditioning strategy
developed for this work generated very dense systems that consumed
tremendous amounts of memory to store and compute time to
generate. The memory problem was partially alleviated by applying the
reduced domain approximation, however a significant memory overhead
remained along with prohibitive time for construction.

Using the developed preconditioners and the reduced domain
approximation, a set of $SP_N$ calculations using up to 4 energy
groups was enabled for the fuel assembly problem. To verify MCSA, the
fuel assembly problem was solved with different energy groups using
both MCSA and two production Krylov methods. MCSA was observed to
produce the same scalar flux in all groups at all spatial locations
and generate the same k-eigenvalue in the same number of eigenvalue
iterations when used in conjuction with an eigenvalue solver.

For the same calculations, MCSA had qualitatively similar iterative
performance to the production Krylov methods, showing improved
performance over GMRES while a Conjugate Gradient-based method
performed better using the same preconditioning. Improved performance
over GMRES meets the goal of improving iterative performance to a
certain extent but this was not the true for the Conjugate
Gradient-based method. However, although the $SP_N$ equations are
asymmetric for multigroup calculations, they are symmetric at the
block level and therefore we expect the Conjugate Gradient-based
method to perform better as this structure can be more readily
leveraged to improve performance than with the more general GMRES and
MCSA methods. In terms of CPU time, MCSA was observed to perform
$O(100)$ times slower than the Krylov methods due to both lack of
optimization in the random walk sequence and the explicit
preconditioning strategy.

%%---------------------------------------------------------------------------%%
\section{Monte Carlo Synthetic Acceleration Solutions for Navier-Stokes Equations\ }
\label{sec:nonlinear_conclusions}

This work presents the FANM method, a new nonlinear solution scheme
based on an inexact Newton method. To meet the goals of this work the
FANM method was characterized by applying it to the Navier-Stokes
equations. This research was enabled by implementing the FANM method
within the nonlinear solver sequence leveraged by the Drekar production
multiphysics code base being developed at Sandia National
Laboratories. In both convection and driven flow regimes we solved
three difficult benchmark problems for the Navier-Stokes
equations. Using the work in preconditioning from the research on the
$SP_N$ equations, it was found that the same preconditioning strategy
could be used to achieve convergence of the linear models generated at
each FANM iteration by applying an algebraic multigrid method.

For each benchmark problem, FANM solutions were verified against
Newton-Krylov solutions leveraging GMRES as the linear solver. The
FANM solutions were observed to be numerically identical to those
generated by the Newton-Krylov method. In addition, the benchmark
problems also served as a vehicle to compare the performance of FANM
with the Newton-Krylov method. For the thermal convection cavity
problem, FANM was observed to converge in fewer linear solver
iterations than Newton-Krylov at all Rayleigh numbers tested when the
same preconditioning was applied to both methods. This means that FANM
demonstrated superior iterative performance for problems dominated by
natural convection, meeting the goal of improved iterative performance
in this case.

Iterative performance for the problems dominated by inertial driven
flow varied depending on the situation. For the lid driven cavity
problem, FANM converged in more linear solver iterations for 3 out of
4 cases. However, for each of these cases it was observed that the
extra MCSA iterations were generated by a smaller forcing term at each
Newton iteration when compared to the Newton-Krylov solver. Given that
a smaller forcing term is equivalent to requesting convergence of the
linear solver to a smaller residual, we expect these extra
iterations. For the lid driven cavity case where FANM converged in
fewer iterations, it was observed that increasing the number of
stochastic histories used at every MCSA iteration enabled convergence
in fewer iterations, again meeting the goal of improved iterative
performance.

The backward facing step problem at low Reynolds number gave better
performance with FANM converging in fewer linear solver
iterations. However, as the Reynolds number was increased so did the
number of MCSA iterations required to converge the linear model at
each FANM iteration. Unlike the lid driven cavity problem, this was
discovered to occur due to the inability of MCSA to quickly converge
the ill-conditioned linear model rather than the introduction of
smaller forcing terms.

Timing performance for all benchmarks favored the Newton-Krylov solver
with FANM observed to be $O(100)$ slower for the thermal convection
cavity and lid driven cavity problems and $O(1,000)$ times for the
backward facing step problem. For the first two cases, the timing
differences were observed to be qualitatively the same as those
observed for the $SP_N$ performance analysis. Again, this comes from
both lack of optimization of the Monte Carlo sequence and the
introduction of explicit preconditioning. For the backward facing step
problem, the ill-conditioning of the system adds the extra order of
magnitude slow down.

%%---------------------------------------------------------------------------%%
\section{Parallel Monte Carlo Synthetic Acceleration\ }
\label{sec:parallel_mc_conclusions}

A new parallel MCSA algorithm was developed for this work to meet the
goal of demonstrating the improved scalability of MCSA on
leadership-class hardware. It was found the MCSA can indeed be
effectively parallelized using the multiple-set overlapping-domain
decomposition algorithm borrowed from the reactor physics
community. Using a neutron diffusion problem, the parallel algorithm
was verified to produce the same results as two production Krylov
methods.

The new algorithm was tested in a wide variety of parallel scaling
studies on the Titan Cray XK7 machine at the Oak Ridge Leadership
Computing Facility. To test the algorithm at high levels of
concurrency, up to 65,356 cores were used in strong scaling exercises
and 131,072 cores used in weak scaling exercises using the neutron
diffusion problem. In general, the new parallel MCSA algorithm was
observed to produce better parallel scaling results when compared to a
production GMRES and Conjugate Gradient method. We note here that
these scaling studies should be reconsidered if arithmetic
optimization of the code has been completed.

It was found that the leakage of histories from domain to domain in
the parallel Monte Carlo algorithm could in fact be quantified
analytically using the algebraic properties of the system. These
relationships were then used to determine the amount of overlap one
may require in the parallel algorithm to reduce communication costs
and increase parallel efficiencies. Scaling studies showed that in the
strong case overlap in small quantities on the order of the
mean-free-path of a stochastic history in the simulation could boost
parallel efficiencies by a few percent. However, it was found that
this additional overlap was not effective in boosting strong scaling
efficiencies. It was found that in general overlap was not very
effective due to the fact that the parallel communication saved during
the Monte Carlo transport sequence is simply deferred until after
transport is complete when it manifests itself as an overlapping
parallel vector reduction operation.

Applying multiple sets in the parallel algorithm was found to not
enhance the weak scaling of the problem as an additional parallel
overhead is introduced when the calculations from the set are combined
in superposition. For the strong scaling case, improvements were not
noted until after the strong scaling wall was hit. At this point,
multiple sets were observed to increase parallel efficiencies from
38\% to 58\% at 16,384 cores. Perhaps more important here is the fact
that although the parallel efficiency was reduced, multiple sets were
observed to actually improve the time to solution. Unlike a
traditional Krylov method that we might apply to solve a neutron
transport problem, using MCSA means that we can actually make a
physical copy of the problem on the machine and can combine separate
Monte Carlo solutions for each copy through superposition to enhance
time to solution. Time to solution is then improved because fewer
histories were run in each copy and therefore each MCSA iteration is
faster or more global histories are computed and fewer MCSA iterations
are required to converge.

Finally, given that overlap was not very constructive in boosting
parallel efficiencies, it was postulated that very little
domain-to-domain communication of histories was occurring in the first
place and then subsequently verified using the analytic relationships
for domain leakage. Using this idea, we implemented a subdomain
Neumann-Ulam method with MCSA such that MCSA now takes the form of a
stochastic realization of an additive Schwarz method. Scaling studies
using this technique showed significant improvements in parallel
scalability even when compared to the MCSA results when
domain-to-domain communication was present, further enhancing the
method and meeting the goal of improved scalability.

%%---------------------------------------------------------------------------%%
\section{Future Work\ }
\label{sec:future_work}

Throughout this work, three salient issues were observed over all
others. First, the performance of the MCSA implementation developed
for this work was severely lacking in terms of time to solution when
compared to production Krylov methods. In all cases, this difference
was several orders of magnitude. Second, the preconditioning strategy
developed by this work, although often successful in achieving
convergence for difficult problems, creates many obstacles to a
production implementation. These obstacles all primarily stem from the
dense composite operators that are required to form the probabilities
and weights for the Monte Carlo game. Third, perhaps the most
restrictive and unattractive piece of MCSA as a whole is the simple
fact that the spectral radius of the physics operator iteration matrix
be less than one. This was demonstrated to prohibit convergence for
cases where the preconditioning was considered aggressive and Krylov
solvers exhibited excellent convergence properties. We will address
each one of these problems in turn in this section and provide
suggestions for their solution.

\subsection{Performance}
\label{subsec:future_performance}

When issues arising from preconditioning are not considered, there are
two primary issues that can be resolved in the MCSA implementation
developed by this work that will likely result in performance gains
for serial computations. First, all random walks in the implementation
have a state which changes as the random walk proceeds. These states
are those given by Eq~(\ref{eq:mc_walk_permutation}). For random walks
that are traversing a domain which has been decomposed in a parallel
environment, there are two sets of these states: one that represents
the states over the entire global domain and one that represents them
only over the local subdomain. Each state in the system gets a unique
index in both of these sets that act as absolute coordinates in the
random walk and permit the construction of the Monte Carlo estimates
and other data needed for the solution. 

A fundamental flaw of the implementation developed for this work is
that those random walk states given by
Eq~(\ref{eq:mc_walk_permutation}) were formulated such that they were
always represented using the indices from the global set. However, all
data in the system (e.g. weights, probabilities, tallies, etc.) are
stored such that they are accessed using the local indexing
scheme. Because of this, each time a random walk wants to sample its
current probability distribution function, get a weight for its
transition, or add its estimate to a tally, a call must first be made
to translate the current global state of the random walk into its
local state\footnote{This is a look-up in a hash table.}. What this
then means is that we are performing orders of magnitude more
operations during the Monte Carlo sequence than if the random walk
sequence operated in the local index space rather than the global
index space. Preliminary profiling of the implementation has shown
that indeed a vast majority of the time in the Monte Carlo sequence is
spent doing this index translation. Therefore, future work should
perform the significant refactoring of the implementation required to
achieve this.

A second mode of optimization to improve performance regards how
discrete probability distribution functions are sampled in the
implementation. At each transition in the random walk sequence, the
discrete probability distribution function generated by the current
row of the physics operator in which a random walk resides is sampled
to get the next state in the sequence. This operation is also used
when the right hand side of the system is sampled to acquire the
starting state for each random walk in the problem through
Eq~(\ref{eq:adjoint_source_probability}). The current implementation
uses the inverse transform method for sampling where the a random
number is generated and the cumulative distribution function is
queried with a binary search to find the next discrete state. This
algorithm has $O(log(N))$ time complexity for sampling and $O(N)$ time
complexity for construction. The alias sampling method is suggested
instead as it produces $O(1)$ complexity for sampling and $O(N)$ time
complexity for construction \cite{smith_analysis_2005}. Sabelfeld
outlines the use of this sampling technique in his Monte Carlo
algorithms \cite{sabelfeld_sparsified_2009}. Profiling of the
implementation shows that this improvement could also provide quality
reductions in run time.

For the parallel MCSA algorithm, overall parallel scaling results
indicated that the implementation was of good quality in both the
strong and weak scaling cases. However, for the multiple set cases in
the weak scaling analysis, it was noted that there was a growing
parallel overhead as a function of core count. This was
counter-intuitive in that the size of the parallel group over which
the set reductions were occurring was fixed at all core counts and
therefore we do not expect the addition of such an operation to
increase parallel run times. It did, however, and therefore future
research should explore why the implementation for multiple sets
generated parallel overhead that increased with core count instead of
remaining constant.

Nonlinear performance may also be improved by a closer relationship
between MCSA and the Newton iteration. We saw in our research how the
forcing term selected very tightly controls the convergence of the
linear model in FANM. For MCSA, in addition to the residual tolerance
that is dictated by this forcing term, convergence may also be
controlled by adjusting the number of stochastic histories computed at
each MCSA iteration. As the nonlinear iteration evolves, we may find
it useful to adjust the number of histories used at each MCSA
iteration in the same way as we adjust the convergence tolerance of
the linear model through the forcing term. Future research is
suggested to analyze the effects of the number of stochastic histories
on the convergence of the linear and nonlinear models and how this may
be controlled by the forcing term or another parameter.

With these implementation improvements perhaps MCSA and FANM timing
may approach that of production Krylov methods for equally conditioned
problems. Next we discuss potential research in the area of
preconditioning which may enable performance research for MCSA in the
area of memory consumption.

\subsection{Preconditioning}
\label{subsec:future_preconditioning}

In general, preconditioning developments in this work enabled much of
the desired research to be performed but also prevented some of it. Of
primary concern is the large memory footprint induced by the explicit
algebraic preconditioning strategy outlined in
\S~\ref{subsubsec:general_mcsa_preconditioning}. By preconditioning in
this way, we were able to leverage modern algebraic preconditioning
strategies used in current physics applications and enable solutions
for both neutronics and fluid flow simulations. The downside is that
in order to generate weights and probabilities with which to play the
Monte Carlo game we must first invert each of the preconditioners
which in many cases means applying their inverse over each row in the
system and then perform matrix-matrix multiply operations to build the
composite physics operator.

These operations were observed to substantially increase runtimes of
MCSA and consume unacceptable amounts of memory, mostly due to the
fact that inverting the preconditioners created dense
matrices. Although a good fraction of the elements in each row of the
composite operator could be eliminated through the reduced domain
approximation developed by this work, memory costs still prevented
higher fidelity simulations from being performed. In addition, the
reduced domain approximation does not prevent the large run times
incurred for inverting the preconditioners. We therefore propose the
following three developments in preconditioning for Monte Carlo
methods:

\paragraph{\textbf{Research and develop alternative sampling schemes for
  the general left/right preconditioned Monte Carlo method not relying
  on composite matrix generation.}} Developments that break away from
matrix-matrix multiplications within the preconditioned solver
framework improve scalability. This means avoiding the generation of
the composite operator and instead playing the Monte Carlo game with
weights and probabilities formed by considering the original linear
operator and the preconditioners individually. This individual
consideration may come from an alternative algebraic formulation of
the general preconditioned MCSA method in
Eq~(\ref{eq:left_right_mcsa}) or from an alternative random sampling
strategy for the Monte Carlo game.
\paragraph{\textbf{Develop Monte Carlo variance reduction based
  preconditioning strategies that do not rely on an algebraic
  preconditioner.}} Constructing the complete inverse of the
  preconditioner is the core time and scalability constraint when
  modern algebraic preconditioners are used. Building the inverse for
  preconditioners where a sparsity pattern is not enforced typically
  results in a dense matrix that destroys scalability for domain
  decomposed parallel computations. Alleviating both of these issues
  by stochastic preconditioning methods should be studied. Decades of
  research and development in Monte Carlo variance reduction for
  particle transport \cite{booth_1994} may be leveraged to form
  similar strategies for linear systems. In the same way that this
  work has been able to adapt modern particle transport strategies for
  parallelism and apply them to linear systems, the same strategy may
  possibly be applied to variance reduction techniques to improve MCSA
  convergence by modifying the weights and probabilities of the Monte
  Carlo game as an effective means of preconditioning.
\paragraph{\textbf{Study physics or PDE-based preconditioners for linear
  systems that produce a reduced-order model of the original system to
  which the Monte Carlo method may be applied.}} As the Monte Carlo
method serves to construct the solution correction vector in the MCSA
method, the Monte Carlo method may be applied to a reduced-order
physics or PDE-based system that captures the essence of the original
system and still allows for acceleration of the iterative
procedure. By reducing the order of the system, we are effectively
generating a Monte Carlo problem within MCSA in which the linear
operator is either more sparse, has a better-conditioned eigenvalue
spectrum, or both. The reduced domain approximation has shown that a
purely algebraic strategy for reducing the order of the Monte Carlo
problem can result in improved overall timing performance due to
increased sparsity at the cost of iterative performance as a result of
loss of information. A more well informed strategy for order reduction
via either an intelligent simplification of the PDEs that describe the
system or a simplification of the physics involved may prove to be a
more effective strategy.

\subsection{Breaking Away from $\rho(H) < 1$}
\label{subsec:future_spec_rad}

All numerical methods for solving linear problems require quality
preconditioning. Even a Krylov method may be effectively useful
without it. However, these methods typically converge eventually
whereas quality preconditioning may not be enough to guarantee
convergence of MCSA. We saw in our work that in many cases very
aggressive preconditioning was not enough to condition the eigenvalues
of the system into a regime where MCSA would apply. The limitation
that the spectral radius of the physics operator iteration matrix must
be less than one is by far the most prohibitive component of MCSA. The
fact that the Monte Carlo sequence is based on the Richardson
iteration means that without sufficient preconditioning, even if this
condition is achieved convergence may be incredibly slow. Therefore,
we propose two elements of future work that aim to move away from this
restriction, both fundamentally based on sampling a sequence not based
on the Neumann series.

\subsubsection{Monte Carlo Methods of the Second Degree}
\label{subsubsec:2_degree_mc}

Iterative methods of the $n^{th}$ degree produce solutions at each
iteration of the following form:
\begin{equation}
  \ve{x}^{k+1} = F(\ve{A},\ve{b},\ve{x}^{k},\ve{x}^{k-1},\dots,\ve{x}^{k-n+1})\:,
\label{eq:nth_degree_iter}
\end{equation}
where $\ve{A}$ is the linear operator and $\ve{b}$ is the right hand
side as defined by Eq~(\ref{eq:linear_problem}). Richardson's
iteration is an iterative method of the first degree such that
$\ve{x}^{k+1} = F(\ve{A},\ve{b},\ve{x}^{k})$. To move the Monte Carlo
method away from the spectral radius limitation we therefore we seek
higher order methods as outlined by Hallett
\cite{hughes_hallett_second-order_1984} that have guaranteed
convergence at all eigenvalue spectra. In his work, Hallett builds a
second order method such that $\ve{x}^{k+1} =
F(\ve{A},\ve{b},\ve{x}^{k},\ve{x}^{k-1})$. Such a method results from
mapping the original Richardson iteration onto a different set of
eigenvalues in the complex plane that have a spectral radius less than
one, thus guaranteeing convergence. Derived as expansions of the
iteration sequence in Chebyshev polynomials, Hallett arrives at simple
formulation for the second degree method that requires knowledge of
the bounding curve in the complex plane of the eigenvalue spectra for
good convergence properties.

As it turns out, Sabelfeld has already applied such a technique to the
Monte Carlo method but arrived at the same conclusions in a different
way \cite{sabelfeld_sparsified_2009}\footnote{Although the technique
  is outlined in this document, Sabelfeld cites his previous work in
  this area in 1982.} with this work effectively re-developed later by
Dimov without citation of Sabelfeld
\cite{dimov_parallel_2001,dimov_new_1998}. In his work, Sabelfeld
formulated the random walk sequence by instead building the Neumann
series from the \textit{resolvent} of the operator. The resolvent
operator is defined as that which spans all of the complex plane but
has singularities at the points that are eigenvalues of the original
operator. By building the Monte Carlo sequence from this operator,
Sabelfeld arrived at a Monte Carlo method which in fact was equivalent
to a fixed point method of the second degree and utilized effectively
the same spectral mapping procedure as Hallett. Just like the
deterministic case, some knowledge of the eigenvalue spectrum of the
system was required to ensure convergence. These methods should be
researched and developed further in the context of MCSA as a potential
means of always guaranteeing convergence of the algorithm regardless
of the eigenvalue spectrum.

\subsubsection{Stochastic Projection Methods}
\label{subsubsec:stochastic_projection_methods}

As a final means of improvement we consider stochastic projection
methods as developed by Sabelfeld \cite{hutchison_stochastic_2011}. In
his work, Sabelfeld claims his method is 'beyond Markov chains and von
Neumann-Ulam scheme'. To a certain extent this is true; his method is
based on a projection method and should always converge. In his work,
he utilizes a Kaczmarz iteration:
\begin{equation}
  \ve{x}^{k+1} = \ve{x}^k + \lambda_k \frac{b_i - \langle \ve{a}_i
    \ve{x}^k \rangle}{||\ve{a}_i||}\ve{a}_i\:,
  \label{eq:kaczmarz_iteration}
\end{equation}
where $\ve{a}_i$ is the $i^{th}$ row vector of the operator and
$\lambda_k$ is a relaxation parameter.

The stochastic nature of the algorithm comes from effectively
modifying the iteration sequence such that the value of $i$ is now
selected at random. This is interesting but arguably does not serve
our purposes at well. We have an interest in a fully stochastic
process like a random walk sequence because of its potential benefits
in terms of resiliency and the ability to use superposition to combine
results from replicated problems to enhance performance. In
Sabelfeld's work, the randomness is used to select which deterministic
iteration step will be performed and does not meet either of these
requirements. However, that does not mean there is nothing to learn
from this work. If one writes Eq~\ref{eq:kaczmarz_iteration} in terms
of applications of projection operators as that outlined in
\cite{tanabe_projection_1971} we see that this actually takes a
similar form as the Neumann series and could therefore be used in a
random walk sequence. In addition, perhaps the above algorithm can be
modified such that the randomness of the projection sequence permits
improved results via the superposition principle. Research studying
random walks based on projection sequences is therefore suggested as a
means of moving beyond the spectral radius limitation.

%%---------------------------------------------------------------------------%%
\section{Closing Remarks\ }
\label{sec:closing}

This work was performed with the goal of investigating new solution
techniques based on Monte Carlo Synthetic Acceleration and how they
perform when applied to real problems in nuclear engineering. These
methods aim to make improvements over conventional methods when
looking towards the advanced computer architectures of the near
future. Parallelism at high levels of concurrency, a potential
reduction in memory footprint, and a natural element of resiliency
deriving from the stochastic nature of the algorithm motivated us to
research their strengths and weaknesses and develop solutions to
further their applicability.

Before this work was performed, MCSA had yet to be applied to the
neutron transport problem, asymmetric systems, and nonlinear problems
including fluid flow systems. Techniques were developed by this work
that enabled solutions for all of these problems using MCSA. In many
cases, it was found that the iterative performance of the MCSA-based
methods were superior to that of conventional solution techniques
based on Krylov methods. Furthermore, with improved implementation
performance, future problems may be solved in less time and more
efficiently using MCSA due to this demonstrated improved iterative
performance.

When this work began, the Neumann-Ulam sequence and MCSA in general
had yet to be parallelized in a domain decomposed framework. This
research developed a new parallel algorithm based on Monte Carlo
particle transport that permitted MCSA solutions to be generated on
$O(100,000)$ cores on a leadership class computing platform. The new
algorithm was also demonstrated to achieve better strong and weak
scaling than conventional Krylov methods at high levels of
concurrency. If future work can improve the time to solution using
MCSA through optimization, then this improved scaling demonstrated by
the algorithm can potentially improve the time to solution for the
engineer and more efficiently leverage high performance computing
hardware for analysis.

With this work, a new foundation for Monte Carlo Synthetic Acceleration
methods has been laid with new theory, algorithms, and numerical
experiments that demonstrate both their potential to make great
contributions to nuclear engineering calculations in the future and
where the research must go in order to achieve this. By advancing
these techniques, we look forward to future work that will enable the
improved design and analysis of nuclear systems.
