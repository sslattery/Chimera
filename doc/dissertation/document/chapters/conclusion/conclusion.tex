\chapter{Conclusion\ }
\label{ch:conclusion}

For high fidelity simulations of nuclear reactors, physics solutions
are required with speed and accuracy. When the problems become large
enough, such as the simulation of an entire reactor core,
leadership-class computing facilities must be leveraged on the grounds
of requirements for both time to solution and the amount of memory
required to contain the description of the entire problem. Looking
forward to the next generation of machines, concurrency will go up and
available memory will go down, putting pressure on physics application
developers to research and develop solution techniques that can
efficiently leverage this hardware. In nuclear reactor simulation,
both neutron transport and fluid flow calculations consume a vast
amount of computational time in order to properly characterize both
steady state operational characteristics and transient accident
scenarios. In both cases, we desire solution techniques that are aware
of coming changes in hardware.

In this work, we have presented Monte Carlo Synthetic Acceleration as
a viable solution technique for both neutron transport and fluid flow
problems on future hardware. To do this, we carried out three research
and development activities. First, we applied MCSA to the $SP_N$ form
of the neutron transport equation and analyzed the preconditioning
requirements and subsequent performance of the method when compared to
conventional practices. Second, we used the knowledge gained from the
research on neutron transport to develop the FANM method for nonlinear
problems and studied its performance on three different benchmarks for
the Navier-Stokes equations to demonstrate its applicability. Finally,
we parallelized MCSA such that it may be applied to physics problems
on leadership-class computing platforms.

In this chapter, we review the work presented in this document and
describe how this work met the goals of the research. We then
reiterate the important issues discovered during the course of this
work and develop a strategy for future work to potentially alleviate
them. Finally, we close with some final remarks regarding Monte Carlo
Synthetic Acceleration.

%%---------------------------------------------------------------------------%%
\section{Monte Carlo Synthetic Acceleration Methods for the $SP_N$ Equations\ }
\label{sec:spn_conclusion}
This work demonstrates the first application of MCSA to the neutron
transport problem. To do this, we used the $SP_N$ form of the
Boltzmann transport equation. In this form, the transport equation
takes on a diffusion-like form where the angular character of the flux
is accounted for in moment terms much like those found in the $P_N$
equations. To meet the goals of this work and drive the research and
development of MCSA for application to light water reactor problems, a
difficult nuclear fuel assembly criticality calculation was used in a
set of numerical experiments. These calculations were enabled by
incorporating MCSA as a solution scheme into the k-eigenvalue solver
in the Exnihilo production neutronics code base at Oak Ridge National
Laboratory.

When applying the new technique to the fuel assembly problem, it was
initially found that MCSA could not converge the problem with basic
Jacobi-based preconditioning. This was discovered to be due to the
fact that the transport operator generated in the eigenvalue
calculation was very ill-conditioned as a result of the large amount
of neutron scattering in the light water moderator. The more
scattering present in the system, the longer a random walk will take
and thus a spectral radius approaching unity was observed. A simpler
neutron diffusion problem was used to demonstrate the breakdown of
MCSA when spectral radii this large are generated.

As a result of the breakdown observation, a suite of advanced
preconditioning techniques was applied to MCSA for the fuel assembly
problem in order to reduce the spectral radius to a level below where
breakdown occurs. From the results of this analysis, ILUT
preconditioning was chosen for subsequent investigations. Using this
preconditioning, it was verified that MCSA is indeed general enough to
solve the asymmetric system generated by the $SP_N$
equations. However, the explicit MCSA preconditioning strategy
developed for this work generated very dense systems that consumed
tremendous amounts of memory and compute time. The memory problem was
partially alleviated by developing and applying the reduced domain
approximation, however, a significant memory overhead remained along
with prohibitive time for construction.

To verify MCSA, the fuel assembly problem was solved with different
energy groups using both MCSA and two production Krylov methods. MCSA
was observed to produce the same k-eigenvalue in the same number of
eigenvalue iterations when used in conjunction with an eigenvalue
solver. For the same calculations, MCSA had qualitatively similar
iterative performance to the production Krylov methods, showing
improved performance over GMRES using the same preconditioning,
meeting the goal of demonstrating improved iterative performance. In
terms of CPU time, MCSA was observed to perform $O(100)$ times slower
than the Krylov methods due to both lack of optimization in the random
walk sequence and the explicit preconditioning strategy.

%%---------------------------------------------------------------------------%%
\section{Monte Carlo Synthetic Acceleration Methods for the Navier-Stokes Equations\ }
\label{sec:nonlinear_conclusions}
This work presents the FANM method, a new nonlinear solution scheme
based on an inexact Newton method. To meet the goals of this work the
FANM method was characterized by applying it to the Navier-Stokes
equations. This research was enabled by implementing the FANM method
within the nonlinear solver sequence leveraged by the Drekar
production multiphysics code base being developed at Sandia National
Laboratories. In both convection and driven flow regimes we solved
three difficult benchmark problems for the Navier-Stokes
equations. Using the work in preconditioning from the research on the
$SP_N$ equations, it was found that the same preconditioning strategy
could be used to achieve convergence of the linear models generated at
each FANM iteration by applying an algebraic multigrid method.

For each benchmark problem, FANM solutions were verified against
Newton-Krylov solutions leveraging GMRES as the linear solver. The
FANM solutions were observed to be numerically identical to those
generated by the Newton-Krylov method. For the thermal convection
cavity problem, FANM was observed to converge in fewer linear solver
iterations than Newton-Krylov at all Rayleigh numbers tested when the
same preconditioning was applied to both methods. This means that FANM
demonstrated superior iterative performance for problems dominated by
natural convection, meeting the goal of improved iterative performance
in this case.

Iterative performance for the problems dominated by inertial driven
flow varied depending on the situation. For the lid driven cavity
problem, FANM converged in more linear solver iterations for 3 out of
4 cases. However, for each of these cases it was observed that the
extra MCSA iterations were generated by a smaller forcing term at each
Newton iteration when compared to the Newton-Krylov solver. Given that
a smaller forcing term is equivalent to requesting convergence of the
linear solver to a smaller residual, we expect these extra
iterations. For the lid driven cavity case where FANM converged in
fewer iterations, it was observed that increasing the number of
stochastic histories used at every MCSA iteration enabled convergence
in fewer iterations, again meeting the goal of improved iterative
performance.

The backward facing step problem at low Reynolds numbers gave better
performance with FANM converging in fewer linear solver
iterations. However, as the Reynolds number was increased so did the
number of MCSA iterations required to converge the linear model at
each FANM iteration. Unlike the lid driven cavity problem, this was
discovered to occur due to the inability of MCSA to quickly converge
the ill-conditioned linear model rather than the introduction of
smaller forcing terms.

Timing performance for all benchmarks favored the Newton-Krylov solver
with FANM observed to be $O(100)$ slower for the thermal convection
cavity and lid driven cavity problems and $O(1,000)$ times for the
backward facing step problem. For the first two cases, the timing
differences were observed to be qualitatively the same as those
observed for the $SP_N$ performance analysis. Again, this comes from
both lack of optimization of the Monte Carlo sequence and the
introduction of explicit preconditioning. For the backward facing step
problem, the ill-conditioning of the system adds the extra order of
magnitude slow down.

%%---------------------------------------------------------------------------%%
\section{Parallel Monte Carlo Synthetic Acceleration Methods\ }
\label{sec:parallel_mc_conclusions}
A new parallel MCSA algorithm was developed for this work to meet the
goal of demonstrating the improved scalability on leadership-class
hardware. It was found the MCSA can indeed be effectively parallelized
using the multiple-set overlapping-domain decomposition algorithm
borrowed from the reactor physics community. Using a neutron diffusion
problem, the parallel algorithm was verified to produce the same
results as two production Krylov methods.

The new algorithm was tested in a wide variety of parallel scaling
studies on the Titan Cray XK7 machine at the Oak Ridge Leadership
Computing Facility. To test the algorithm at high levels of
concurrency, up to 65,356 cores were used in strong scaling exercises
and 131,072 cores used in weak scaling exercises using the neutron
diffusion problem. In general, the new parallel MCSA algorithm was
observed to produce better parallel scaling results when compared to a
production GMRES and Conjugate Gradient method. We note here, however,
that these scaling studies should be reconsidered if arithmetic
optimization of the code has been completed.

Outlined in Appendix~\ref{chap:parallel_theory}, it was found that the
leakage of histories from domain to domain in the parallel Monte Carlo
algorithm could in fact be quantified analytically using the algebraic
properties of the system. These relationships were then used to
determine the amount of overlap one may require in the parallel
algorithm to reduce communication costs and increase parallel
efficiencies. Scaling studies showed that in the strong case overlap
in small quantities on the order of the mean-free-path of a stochastic
history in the simulation could boost parallel efficiencies by up to
10\% in isolated cases. However, it was found that this additional
overlap was not effective in boosting weak scaling efficiencies. In
general, overlap was not very effective due to the fact that the
parallel communication saved during the Monte Carlo transport sequence
is simply deferred until after transport is complete when it manifests
itself as an overlapping parallel vector reduction operation. As
compared to transport calculations where this overlap procedure was
very effective, in the context of MCSA the Monte Carlo calculations
are significantly shorter with $O(10,000)$ histories used in this
work. These shorter calculations and more frequent overlapping tally
vector reductions create an overhead that is not observed in the
literature for transport calculations.

Applying multiple sets in the parallel algorithm was found to not
enhance the weak scaling of the problem as an additional parallel
overhead is introduced when the calculations from the set are combined
in superposition. For the strong scaling case, improvements were not
noted until after the strong scaling wall was hit. At this point,
multiple sets were observed to increase parallel efficiencies from
38\% to 58\% at 16,384 cores. Perhaps more important here is the fact
that although the parallel efficiency was reduced, multiple sets were
observed to actually improve the time to solution. Unlike a
traditional Krylov method that we might apply to solve a neutron
transport problem, using MCSA means that we can actually make a
physical copy of the problem on the machine and can combine separate
Monte Carlo solutions for each copy through superposition. Time to
solution is then improved because fewer histories were run in each
copy and therefore each MCSA iteration is faster or more global
histories are computed and fewer MCSA iterations are required to
converge.

Finally, given that overlap was not very constructive in boosting
parallel efficiencies, it was postulated that very little
domain-to-domain communication of histories was occurring in the first
place. Motivated by this idea, we implemented a subdomain Neumann-Ulam
method with MCSA such that MCSA now takes the form of a stochastic
realization of an additive Schwarz method. Scaling studies using this
technique showed significant improvements in parallel scalability even
when compared to the MCSA results when domain-to-domain communication
was present, further enhancing the method and meeting the goal of
improved scalability.

%%---------------------------------------------------------------------------%%
\section{Future Work\ }
\label{sec:future_work}

Throughout this work, three salient issues were observed. First, the
performance of the MCSA implementation developed for this work was
severely lacking in terms of time to solution when compared to
production Krylov methods. In all cases, this difference was several
orders of magnitude. Second, the preconditioning strategy developed by
this work, although often successful in achieving convergence for
difficult problems, creates many obstacles to a production
implementation. These obstacles all primarily stem from the dense
composite operators that are required to form the probabilities and
weights for the Monte Carlo game. Third, perhaps the most restrictive
and unattractive piece of MCSA as a whole is the simple fact that the
spectral radius of the physics operator iteration matrix be less than
one. This was demonstrated to prohibit convergence for cases where the
preconditioning was considered aggressive and Krylov solvers exhibited
excellent convergence properties. We will address each one of these
problems in turn in this section and provide suggestions for their
solution.

\subsection{Performance}
\label{subsec:future_performance}
When issues arising from preconditioning are not considered, there are
two primary issues that can be resolved in the MCSA implementation
developed by this work that will likely result in performance gains
for serial computations. First, all random walks in the implementation
have a state which changes as the random walk proceeds. These states
are those given by Eq~(\ref{eq:mc_walk_permutation}). For random walks
that are traversing a domain which has been decomposed, there are two
sets of these states: one that represents the states over the entire
global domain and one that represents them only over the local
subdomain. Each state in the system gets a unique index in both of
these sets that act as absolute coordinates in the random walk and
permit the construction of the Monte Carlo estimates and other data
needed for the solution.

A fundamental flaw of the implementation developed for this work is
that those random walk states given by
Eq~(\ref{eq:mc_walk_permutation}) were formulated such that they were
always represented using the indices from the global set. However, all
data in the system (e.g. weights, probabilities, tallies, etc.) are
stored such that they are accessed using the local indexing
scheme. Because of this, each time a random walk wants to sample its
current probability distribution function, get a weight for its
transition, or add its estimate to a tally, a call must first be made
to translate the current global state of the random walk into its
local state\footnote{This is a look-up in a hash table.}. What this
then means is that we are performing orders of magnitude more
operations during the Monte Carlo sequence than if the random walk
sequence operated in the local index space rather than the global
index space. Preliminary profiling of the implementation has shown
that indeed a vast majority of the time in the Monte Carlo sequence is
spent doing this index translation. Therefore, future work should
perform the significant refactoring of the implementation required to
achieve this.

A second mode of optimization to improve the performance of how
discrete probability distribution functions are sampled in the
implementation. At each transition in the random walk sequence, the
discrete probability distribution function generated by the current
row of the physics operator in which a random walk resides is sampled
to get the next state in the sequence. This operation is also used
when the right hand side of the system is sampled to acquire the
starting state for each random walk in the problem through
Eq~(\ref{eq:adjoint_source_probability}). The current implementation
uses the inverse transform method for sampling where the a random
number is generated and the cumulative distribution function is
queried with a binary search to find the next discrete state. This
algorithm has $O(log(N))$ time complexity for sampling and $O(N)$ time
complexity for construction. The alias sampling method is suggested
instead as it produces $O(1)$ complexity for sampling and $O(N)$ time
complexity for construction \cite{smith_analysis_2005}. Sabelfeld
outlines the use of this sampling technique in his Monte Carlo
algorithms \cite{sabelfeld_sparsified_2009}. Profiling of the
implementation shows that this improvement could also provide quality
reductions in run time.

For the parallel MCSA algorithm, overall parallel scaling results
indicated that the implementation was of good quality in both the
strong and weak scaling cases. However, for the multiple set cases in
the weak scaling analysis, it was noted that there was a growing
parallel overhead as a function of core count. This was
counter-intuitive in that the size of the parallel group over which
the set reductions were occurring was fixed at all core counts and
therefore we do not expect the addition of such an operation to
increase parallel run times. It did, however, and therefore future
research should explore why the implementation for multiple sets
generated parallel overhead that increased with core count instead of
remaining constant.

\subsection{Preconditioning}
\label{subsec:future_preconditioning}
In general, preconditioning developments in this work enabled much of
the desired research to be performed but also prevented some of it. Of
primary concern is the large memory footprint induced by the explicit
algebraic preconditioning strategy outlined in
\S~\ref{subsubsec:general_mcsa_preconditioning}. By preconditioning in
this way, we were able to leverage modern algebraic preconditioning
strategies used in current physics applications and enable solutions
for both neutronics and fluid flow simulations. The downside is that
in order to generate weights and probabilities with which to play the
Monte Carlo game we must first invert each of the preconditioners
which in many cases means applying their inverse over each row in the
system and then perform matrix-matrix multiply operations to build the
composite physics operator.

These operations were observed to substantially increase run-times of
MCSA and consume unacceptable amounts of memory, mostly due to the
fact that inverting the preconditioners created dense
matrices. Although a good fraction of the elements in each row of the
composite operator could be eliminated through the reduced domain
approximation developed by this work, memory costs still prevented
higher fidelity simulations from being performed. In addition, the
reduced domain approximation does not prevent the large run times
incurred for inverting the preconditioners. We therefore propose the
following two developments in preconditioning for Monte Carlo
methods.

\subsubsection{Variance Reduction-Based Preconditioning}
Traditional Monte Carlo variance reduction techniques instead of
preconditioning strategies that rely on algebraic preconditioners
should be investigated. Constructing the complete inverse of the
preconditioner is the core time and scalability constraint when modern
algebraic preconditioners are used. Building the inverse for
preconditioners where a sparsity pattern is not enforced typically
results in a dense matrix that destroys scalability for domain
decomposed parallel computations. Alleviating both of these issues by
stochastic preconditioning methods should be studied. Decades of
research and development in Monte Carlo variance reduction for
particle transport \cite{booth_1994} may be leveraged to form similar
strategies for linear systems. In the same way that this work has been
able to adapt modern particle transport strategies for parallelism and
apply them to linear systems, the same strategy may possibly be
applied to variance reduction techniques to improve MCSA convergence
by modifying the weights and probabilities of the Monte Carlo game,
effectively serving as a form of preconditioning by modifying the
Neumann-Ulam decomposition and subsequently the physics operator.

\subsubsection{Physics and PDE-based Preconditioners}
Studies should be considered for physics or PDE-based preconditioners
for linear systems that produce a reduced-order model of the original
system to which the Monte Carlo method may be applied. As the Monte
Carlo method serves to construct the solution correction vector in the
MCSA method, the Monte Carlo method may be applied to a reduced-order
physics or PDE-based system that captures the essence of the original
system and still allows for acceleration of the iterative
procedure. By reducing the order of the system, we are effectively
generating a Monte Carlo problem within MCSA in which the linear
operator is either more sparse, has a better-conditioned eigenvalue
spectrum, or both. The reduced domain approximation has shown that a
purely algebraic strategy for reducing the order of the Monte Carlo
problem can result in improved overall timing performance due to
increased sparsity at the cost of iterative performance as a result of
loss of information. A more well informed strategy for order reduction
via either an intelligent simplification of the PDEs that describe the
system or a simplification of the physics involved may prove to be a
more effective strategy.

\subsection{Breaking Away from $\rho(H) < 1$}
\label{subsec:future_spec_rad}
All numerical methods for solving linear problems require quality
preconditioning. Even a Krylov method may be effectively useless
without it. However, these methods typically converge eventually
whereas quality preconditioning may not be enough to guarantee
convergence of MCSA. We saw in our work that in many cases very
aggressive preconditioning was not enough to condition the eigenvalues
of the system into a regime where MCSA would apply. The limitation
that the spectral radius of the physics operator iteration matrix must
be less than one is by far the most prohibitive component of MCSA. The
fact that the Monte Carlo sequence is based on the Richardson
iteration means that without sufficient preconditioning, even if this
condition is achieved convergence may be incredibly slow. Therefore,
we propose future work that aims to move away from this restriction
that is fundamentally based on sampling a sequence not based on the
Neumann series.

\subsubsection{Monte Carlo Methods of the Second Degree}
\label{subsubsec:2_degree_mc}

Iterative methods of the $n^{th}$ degree produce solutions at each
iteration of the following form:
\begin{equation}
  \ve{x}^{k+1} = F(\ve{A},\ve{b},\ve{x}^{k},\ve{x}^{k-1},\dots,\ve{x}^{k-n+1})\:,
\label{eq:nth_degree_iter}
\end{equation}
where $\ve{A}$ is the linear operator and $\ve{b}$ is the right hand
side as defined by Eq~(\ref{eq:linear_problem}). Richardson's
iteration is an iterative method of the first degree such that
$\ve{x}^{k+1} = F(\ve{A},\ve{b},\ve{x}^{k})$. To move the Monte Carlo
method away from the spectral radius limitation we therefore we seek
higher order methods as outlined by Hallett
\cite{hughes_hallett_second-order_1984} that have guaranteed
convergence at all eigenvalue spectra. In his work, Hallett builds a
second order method such that $\ve{x}^{k+1} =
F(\ve{A},\ve{b},\ve{x}^{k},\ve{x}^{k-1})$. Such a method results from
mapping the original Richardson iteration onto a different set of
eigenvalues in the complex plane that have a spectral radius less than
one, thus guaranteeing convergence. Derived as expansions of the
iteration sequence in Chebyshev polynomials, Hallett arrives at simple
formulation for the second degree method that requires knowledge of
the bounding curve in the complex plane of the eigenvalue spectra for
good convergence properties.

As it turns out, Sabelfeld has already applied such a technique to the
Monte Carlo method but arrived at the same conclusions in a different
way \cite{sabelfeld_sparsified_2009}\footnote{Although the technique
  is outlined in this document, Sabelfeld cites his previous work in
  this area in 1982.} with this work effectively re-developed later by
Dimov without citation of Sabelfeld
\cite{dimov_parallel_2001,dimov_new_1998}. In his work, Sabelfeld
formulated the random walk sequence by instead building the Neumann
series from the \textit{resolvent} of the operator. The resolvent
operator is defined as that which spans all of the complex plane but
has singularities at the points that are eigenvalues of the original
operator. By building the Monte Carlo sequence from this operator,
Sabelfeld arrived at a Monte Carlo method which in fact was equivalent
to a fixed point method of the second degree and utilized effectively
the same spectral mapping procedure as Hallett. Just like the
deterministic case, some knowledge of the eigenvalue spectrum of the
system was required to ensure convergence. These methods should be
researched and developed further in the context of MCSA as a potential
means of always guaranteeing convergence of the algorithm regardless
of the eigenvalue spectrum.

%%---------------------------------------------------------------------------%%
\section{Closing Remarks\ }
\label{sec:closing}

This work was performed with the goal of investigating new solution
techniques based on Monte Carlo Synthetic Acceleration and how they
perform when applied to real problems in nuclear engineering. These
methods aim to make improvements over conventional methods when
looking towards the advanced computer architectures of the near
future. Parallelism at high levels of concurrency, a potential
reduction in memory footprint, and a natural element of resiliency
deriving from the stochastic nature of the algorithm motivated us to
research their strengths and weaknesses and develop solutions to
further their applicability.

Before this work was performed, MCSA had yet to be applied to the
neutron transport problem, asymmetric systems, and nonlinear problems
including fluid flow systems. Techniques were developed by this work
that enabled solutions for all of these problems using MCSA. In many
cases, it was found that the iterative performance of the MCSA-based
methods were superior to that of conventional solution techniques
based on Krylov methods. Furthermore, with improved implementation
performance, future problems may be solved in less time and more
efficiently using MCSA due to this demonstrated improved iterative
performance.

When this work began, the Neumann-Ulam sequence and MCSA in general
had yet to be parallelized in a domain decomposed framework. This
research developed a new parallel algorithm based on Monte Carlo
particle transport that permitted MCSA solutions to be generated on
$O(100,000)$ cores on a leadership class computing platform. The new
algorithm was also demonstrated to achieve better strong and weak
scaling than conventional Krylov methods at high levels of
concurrency. If future work can improve the time to solution using
MCSA through optimization, then this improved scaling demonstrated by
the algorithm can potentially improve the time to solution for the
engineer and more efficiently leverage high performance computing
hardware for analysis.

With this work, a new foundation for Monte Carlo Synthetic Acceleration
methods has been laid with new theory, algorithms, and numerical
experiments that demonstrate both their potential to make great
contributions to nuclear engineering calculations in the future and
where the research must go in order to achieve this. By advancing
these techniques, we look forward to future work that will enable the
improved design and analysis of nuclear systems.
