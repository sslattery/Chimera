\chapter{Monte Carlo Solution Methods for the Simplified $P_N$ Equations}
\label{ch:mc_spn_solutions}
In this chapter we apply a preconditioned MCSA method as a solution
scheme for the $SP_N$ equations.

%%---------------------------------------------------------------------------%%
\section{Spectral Analysis of the $SP_N$ Equations}
\label{sec:spectral_analysis}
Now that we have an understanding of the linear system generated by
the $SP_N$ approximation to the neutron transport equation and the
various parameters in the system that may be adjusted, we can consider
various means of solution. As the solution is asymmetric, modern
iterative solvers should be considered for this task. The performance
of Krylov subspace methods are bound to various properties of the
eigenvalue spectrum of the linear operator while methods based on
stationary iterations have performance limits imposed by the spectral
radius of the sytem. In this section we will review the important
spectral properties to study for common iterative methods for
asymmetric systems. Based on these properties, we will then perform a
parameter-based spectral analysis for the linear operator generated by
the multigroup $SP_N$ equations. Preconditioning strategies for these
methods are not considered. Krylov subspace method restarts and
truncation will also not be considered.

\subsection{Stationary Methods for Linear Systems}
\label{subsec:stationary_solvers}
Stationary methods for linear systems arise from splitting the
operator in Eq~(\ref{eq:multigroup_matrix_system})
\cite{leveque_finite_2007}:
\begin{equation}
  \mathbb{L} = \mathbf{M} - \mathbf{N}\:,
  \label{eq:split_linear_operator}
\end{equation}
where the choice of $\mathbf{M}$ and $\mathbf{N}$ will be dictated by
the particular method chosen. Using this split definition of the
operator we can then write:
\begin{equation}
  \mathbf{M}\mathbb{U} - \mathbf{N}\mathbb{U} = \mathbb{Q}\:.
  \label{eq:linear_split_equation1}
\end{equation}
By rearranging, we can generate a form more useful for analysis:
\begin{equation}
  \mathbb{U} = \mathbf{H}\mathbb{U} + \mathbf{c}\:,
  \label{eq:linear_split_equation2}
\end{equation}
where $\mathbf{H}=\mathbf{M}^{-1}\mathbf{N}$ is defined as the
\textit{iteration matrix} and
$\mathbf{c}=\mathbf{M}^{-1}\mathbb{Q}$. With the solution vector on
both the left and right hand sides, an iterative method can then be
formed:
\begin{equation}
    \mathbb{U}^{k+1} = \mathbf{H}\mathbb{U}^k + \mathbf{c}\:,
  \label{eq:linear_iterative_method}
\end{equation}
with $k \in \mathbf{Z}^+$ defined as the \textit{iteration
  index}. Defining $\mathbf{e}^k = \mathbf{u}^k - \mathbf{u}$ as the
solution error at the $k^{th}$ iterate, we can subtract
Eq~(\ref{eq:linear_split_equation2}) from
Eq~(\ref{eq:linear_iterative_method}):
\begin{equation}
  \mathbf{e}^{k+1} = \mathbf{H}\mathbf{e}^k\:.
  \label{eq:linear_iterative_error}
\end{equation}
Our error after $k$ iterations is then:
\begin{equation}
  \mathbf{e}^{k} = \mathbf{H}^k\mathbf{e}^0\:.
  \label{eq:linear_k_iter_error}
\end{equation}
In other words, successive application of the iteration matrix is the
mechanism driving down the error in a stationary method. By assuming
$\mathbf{H}$ is diagonalizable \cite{saad_iterative_2003}, we then
have:
\begin{equation}
  \mathbf{e}^{k} =
  \mathbf{R}\boldsymbol{\Lambda}^k\mathbf{R}^{-1}\mathbf{e}^0\:,
  \label{eq:linear_k_iter_error_diag}
\end{equation}
where $\boldsymbol{\Lambda}$ contains the Eigenvalues of $\mathbf{H}$
on its diagonal and the columns of $\mathbf{R}$ contain the
Eigenvectors of $\mathbf{H}$. Computing the 2-norm of the above form
then gives:
\begin{equation}
  ||\mathbf{e}^{k}||_2 \leq ||\boldsymbol{\Lambda}^k||_2\ 
  ||\mathbf{R}||_2\ ||\mathbf{R}^{-1}||_2\ ||\mathbf{e}^0||_2\:,
  \label{eq:linear_k_iter_norm1}
\end{equation}
which gives:
\begin{equation}
  ||\mathbf{e}^{k}||_2 \leq \rho(\mathbf{H})^k \kappa(\mathbf{R})
  ||\mathbf{e}^0||_2\:.
  \label{eq:linear_k_iter_norm2}
\end{equation}
For iteration matrices where the Eigenvectors are orthogonal,
$\kappa(\mathbf{R})=1$ and the error bound reduces to:
\begin{equation}
  ||\mathbf{e}^{k}||_2 \leq \rho(\mathbf{H})^k
  ||\mathbf{e}^0||_2\:.
  \label{eq:linear_k_iter_norm3}
\end{equation}
We can now restrict $\mathbf{H}$ by asserting that $\rho(\mathbf{H}) <
1$ for a stationary method to converge such that $k$ applications of
the iteration matrix will not cause the error to grow in
Eq~(\ref{eq:linear_k_iter_norm3}). In addition, the performance of the
stationary method is dictated by $\rho(\mathbf{H})$ such that
performance of the method decreases as it approaches to unity.

\subsection{Krylov Subspace Methods for Linear Systems}
\label{subsec:subspace_solvers}
The performance of different Krylov subspace solvers can depend on
different properties of the linear system. As a means to study these
differences, we use a 1992 paper by Nachtigal and colleagues
\cite{nachtigal_how_1992} that performs and in-depth analysis of three
Krylov subspace methods that are still used today in modern physics
applications: conjugate gradient iteration for the normal equations
(CGN), generalized minimum residual iteration (GMRES), and a
biconjugate gradient-based method (CGS). For our work, we will
consider only their analysis of GMRES as it currently leveraged in
Denovo for $SP_N$ solutions and as it also applies to CGS for most
matrices.

For GMRES, at each iteration we are extracting a correction for the
solution from the Krylov subspace:
\begin{equation}
  \mathbb{U}^k \in \mathbb{U}^0 + \langle \mathbf{r_0}, \mathbb{L}
  \mathbf{r_0}, \cdots, \mathbb{L}^{k-1} \mathbf{r_0} \rangle\:,
  \label{eq:gmres_iteration}
\end{equation}
where the extraction is constrained by ensuring orthogonality of the new
residual with the Krylov space:
\begin{equation}
  \mathbf{r}^k \bot \langle \mathbb{L} \mathbf{r_0}, \mathbb{L}^2
  \mathbf{r_0}, \cdots, \mathbb{L}^{k} \mathbf{r_0} \rangle\:,
  \label{eq:gmres_constraint}
\end{equation}
with the residual at the $k^{th}$ iteration defined as:
\begin{equation}
  \mathbf{r}^k = \mathbb{Q} - \mathbb{L}\mathbb{U}^k\:.
  \label{eq:residual}
\end{equation}
From \cite{nachtigal_how_1992}, we then have the following error at
each iteration:
\begin{equation}
  \mathbf{e}^k = p_k(\mathbb{L})\mathbf{e}^0\:,
  \label{eq:gmres_iteration_error}
\end{equation}
where $p_k(\mathbb{L})$ is an arbitrary $k^{th}$ degree polynomial and
$p_k(0)=1$. This is a very similar form for the iteration error as
observed for stationary methods in Eq~(\ref{eq:linear_k_iter_norm3})
where now as these $p_k$ polynomials are constructed as the iterations
progress, a rapid decrease in $||p_k(\mathbb{L})||$ means a rapid rate
of convergence. For matrices $\mathbb{L}$ that are approximately
normal (which was exactly the assumption we made to arrive at
Eq~(\ref{eq:linear_k_iter_norm3})), then we can write the polynomial
norm as:
\begin{equation}
  ||p_k(\mathbb{L})|| = \sup_{z\in\Lambda(\mathbb{L})}|p_k(z)|\:,
  \label{eq:gmres_polynomial}
\end{equation}
where $\Lambda(\mathbb{L})$ is the eigenvalue spectrum of the normal
matrix $\mathbb{L}$. So if we could obtain the polynomials, the
largest eigenvalue of $\mathbb{L}$ could be used to calculate
convergence rates. Next we consider a choice of polynomials to use for
the convergence rate approximation. They should of course be
orthogonal to reflect the orthogonal projection mechanism by which the
residual is reduced. In Nachtigal's work, Chebyshev polynomials were
used and defined on the interval of Eigenvalues,
$[\lambda_{min},\lambda_{max}]$, and then normalized via the
constraint $p_k(0)=1$. Thus the rate of convergence for these
polynomials is an approximation to the rate of convergence of the
method.

We can check the validity of the normal
approximation by computing the condition number of $\mathbb{L}$:
\begin{equation}
  \kappa(\mathbb{L}) = ||\mathbb{L}||\ ||\mathbb{L}^{-1}||
  \label{eq:operator_condition_number}
\end{equation}
The closer $\kappa(\mathbb{L})$ is to one, the more normal the matrix
and the better the above approximation. The same check can also be
performed to verify the assumption used to generate
Eq~(\ref{eq:linear_k_iter_norm3}) where $\kappa(\mathbf{H})$ can be
computed. For our analysis, we will compute $\kappa_2$ where the
matrix norms are induced by the 2-norm of a vector such that:
\begin{equation}
  ||\mathbb{L}||_2 = \sqrt{\rho(\mathbb{L}^T\mathbb{L})}\:.
  \label{eq:matrix_2_norm}
\end{equation}


\subsection{Organization of the Spectral Analysis}
\label{subsec:spectral_organization}
For the spectral analysis we will perform a numerical parameter study
for the multigroup $SP_N$ equations. Based on the above information
for iterative methods, several key quantities should be computed for
the $SP_N$ system:
\begin{itemize}
\item Eigenvalue spectrum for the linear operator $\mathbb{L}$
\item Largest eigenvalue for the linear operator $\mathbb{L}$
\item Smallest eigenvalue for the linear operator $\mathbb{L}$
\item Condition number for the linear operator $\mathbb{L}$
\item For each stationary method:
  \begin{itemize}
  \item Eigenvalue spectrum for the iteration matrix $\mathbf{H}$
  \item Largest eigenvalue for the iteration matrix $\mathbf{H}$
  \item Smallest eigenvalue for the iteration matrix $\mathbf{H}$
  \item Condition number for the iteration matrix $\mathbf{H}$
  \end{itemize}
\item For GMRES:
  \begin{itemize}
  \item Chebyshev polynomial norm using eigenvalues of $\mathbb{L}$
    for a fixed number of iterations determined by stationary method
    convergence
  \end{itemize}
\end{itemize}
In addition, the following parameters may be varied in the $SP_N$
system in Denovo:
\begin{itemize}
  \item $SP_N$ (angular flux expansion) order
  \item $P_N$ (scattering cross section expansion) order
  \item Number of energy groups $N_g$
  \item Total cross sections $\sigma^g$
  \item Scattering cross section matrix $\sigma_{sn}^{gg'}$
  \item Boundary conditions (Marshak or reflecting)
  \item Finite volume mesh size ($\Delta$ in all directions)
  \item Number of mesh elements
  \item Source of neutrons (Uniform or Point)
\end{itemize}
We will restrict the problem description in Denovo to one similar to
Brantley and Larsen's $SP_N$ work where the spectral radius was
computed for convergence analysis of several proposed solution
methods. From this definition we choose a single material problem with
a uniform source of unity in all energy groups. For the boundary
conditions, we choose reflecting conditions on all six sides of the
cubic domain for an effectively homogenous infinite medium problem
which we will mesh with a $5\times 5\times 5$ grid with
$\Delta=0.1$. We are then left to vary the angular and energy
components of the system in our parameter studies: the $SP_N$ order,
the number of energy groups, and their cross sections of order
$P_N$. In Brantley and Larsen's work, a single energy group was used
and therefore we may not use their cross sections in our multigroup
computations. For our work, we will always choose a total cross
section of unity for all groups and all flux moments. For the
scattering matrix, two classes will be chosen: one with upscatter and
one without. For the downscatter only case, all groups may scatter
within the group and to all groups of lower energy (yielding a lower
triangular scattering matrix) with a cross section of 0.5. For the
upscatter case, all groups may scatter within the group and to all
groups of lower energy given a cross section of 0.5 and for upscatter,
all groups may scatter up to two groups higher in energy with a cross
section of 0.25. All cross section matrices will be equivalent for
each scattering moment. For these cross sections, the $P_N$ order will
be varied with $N=0,1,3$. Both 1 and 10 energy groups will be used
with the 10 energy group case using the scattering matrices specified
above. For the angular flux, the $SP_N$ order will be varied with
$N=1,3,5,7$. This gives a total of 36 linear system variations for the
spectral analysis.

\subsection{Spectral Analysis Results}
\label{subsec:spn_analysis_results}


\subsection{Preconditioning Strategy}
\label{subsec:spn_preconditioning}

%%---------------------------------------------------------------------------%%
\section{Monte Carlo Synthetic Acceleration for the $SP_N$ Equations}
\label{sec:monte_carlo}
