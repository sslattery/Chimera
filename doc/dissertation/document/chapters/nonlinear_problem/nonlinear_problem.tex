\chapter{Monte Carlo Solution Methods Applied to the Navier-Stokes Equations}
\label{ch:nonlinear_problem}
Nonlinear transport problems are a common occurrence in single and
multiple physics problems. Systems of partial differential equations
such as those that describe fluid flow or more general transport
processes when discretized by conventional methods yield discrete sets
of stiff equations with nonlinearities present in the
variables. Traditionally, such systems have been solved by linearizing
them in a form where the nonlinearities in the variables are
eliminated and more traditional linear methods can be used for
solutions. Often characterized as segregated methods where physics
operators are split and their action on the system approximated in
steps, such methods lack consistency and accuracy in resolving the
nonlinear component of the solution. In the last 30 years, fully
consistent nonlinear methods based on Newton's method have become more
popular and many advances have been made in the computational physics
field to employ these methods.

In the context of solving standalone linear systems, Monte Carlo
methods do not provide significant merit over Krylov methods due to
the fact that the linear operator must be explicitly formed. For many
applications, such a requirement is prohibitive and perhaps not even
feasible to implement. Therefore, a Monte Carlo solver is best suited
for situations in which not only are Krylov methods applicable, but
also in which the operator is readily, if not naturally,
formed. Modern nonlinear methods meet both of these requirements with
Newton methods used in conjunction with Krylov methods for a robust,
fully implicit solution strategy. Furthermore, modern techniques exist
that permit the automatic construction of the linear operator
generated within a Newton method based on the nonlinear residual
evaluations, providing all of the components necessary for a Monte
Carlo solver to provide value. We therefore devise a new nonlinear
method based on the MCSA algorithm and Newton's method and discuss its
potential benefits.

\section{Preliminaries}
\label{sec:nonlinear_preliminaries}
We formulate the \textit{nonlinear problem} as follows
\citep{knoll_jacobian-free_2004}:
\begin{equation}
  \ve{F}(\ve{u}) = \ve{0}\:,
  \label{eq:nonlinear_problem}
\end{equation}
where $\ve{u} \in \mathbb{R}^n$ is the solution vector and
$\ve{F}:\mathbb{R}^N \rightarrow \mathbb{R}^N$ is the function of
nonlinear residuals. We write the nonlinear system in this form so
that when an exact solution for $\ve{u}$ is achieved, all residuals
evaluate to zero. \textit{Newton's method} is a root finding algorithm
and therefore we can use it to solve Eq~(\ref{eq:nonlinear_problem})
if we interpret the exact solution $\ve{u}$ to be the roots of
$\ve{F}(\ve{u})$. Newton's method is also an iterative scheme, and we
can generate this procedure by building the Taylor expansion of the
$k+1$ iterate of the nonlinear residuals about the previous $k$
iterate:
\begin{equation}
  \ve{F}(\ve{u}^{k+1}) = \ve{F}(\ve{u}^{k}) +
  \ve{F}'(\ve{u}^{k})(\ve{u}^{k+1}-\ve{u}^{k}) +
  \frac{\ve{F}''(\ve{u}^{k})}{2}(\ve{u}^{k+1}-\ve{u}^{k})^2 + \cdots
  \:.
  \label{eq:newton_derivation_1}
\end{equation}
If we ignore the nonlinear terms in the expansion and assert that at
the $k+1$ iterate $\ve{u}^{k+1}$ is the exact solution such that
$\ve{F}(\ve{u}^{k+1}) = \ve{0}$, then we are left with the following
equality:
\begin{equation}
  -\ve{F}(\ve{u}^{k}) =
  \ve{F}'(\ve{u}^{k})(\ve{u}^{k+1}-\ve{u}^{k})\:.
  \label{eq:newton_derivation_2}
\end{equation}
We note two things of importance in
Eq~(\ref{eq:newton_derivation_2}). The first is that
$\ve{F}'(\ve{u}^{k})$ is in fact the \textit{Jacobian},
$\ve{J}(\ve{u})$, of the set of nonlinear residuals and is defined
element-wise as:
\begin{equation}
  J_{ij} = \frac{\partial F_i(\ve{u})}{\partial u_j}\:.
  \label{eq:jacobian_def}
\end{equation}
Second, we note that $(\ve{u}^{k+1}-\ve{u}^{k})$ is simply the
solution update from the $k$ iterate to the $k+1$ iterate. We will
define this update as the \textit{Newton correction} at the $k$
iterate, $\delta \ve{u}^k$. To finish, we can then rearrange
Eq~(\ref{eq:newton_derivation_2}) to define the Newton iteration
scheme for nonlinear problems:
\begin{subequations}
  \begin{gather}
    \ve{J}(\ve{u}) \delta \ve{u}^k = -\ve{F}(\ve{u}^{k})\\
    \ve{u}^{k+1} = \ve{u}^k + \delta \ve{u}^k\:.
  \end{gather}
  \label{eq:newton_iteration}
\end{subequations}
There are then three distinct steps to perform: evaluation of the
nonlinear residuals using the solution at the $k$ iterate, the
solution of a linear system to compute the Newton correction where the
Jacobian matrix of the nonlinear equation set is the linear operator,
and the application of the correction to the previous iterate's
solution to arrive at the next iterate's solution. In the asymptotic
limit, the iterations of Newton's method will converge the nonlinear
residual quadratically \citep{kelley_iterative_1995}. Convergence
criteria is set for stopping the iteration sequence based on the
nonlinear residual. Commonly, the following criteria is used:
\begin{equation}
  ||\ve{F}(\ve{u}^{k})|| < \epsilon ||\ve{F}(\ve{u}^{0})||\:,
  \label{eq:newton_stopping_criteria}
\end{equation}
where $\epsilon$ is a user defined tolerance parameter. Newton's
method is \textit{consistent} in that all components of the nonlinear
functions that describe the physics we are modeling are updated
simultaneously in the iteration sequence with respect to one
another. This is in comparison to \textit{inconsistent} strategies,
such as a pressure correction strategy for solving the Navier-Stokes
equations \citep{pletcher_computational_1997}, where the components of
$\ve{u}$ are updated in a staggered fashion depending on the
particular equations that they are associated with.

\section{Inexact Newton Methods}
\label{sec:newton_methods}
Inexact Newton methods arise when the Jacobian operator is not exactly
inverted, resulting in an inexact Newton correction as initially
described by Dembo and others \citep{dembo_inexact_1982}. For common
sparse nonlinear systems, which in turn yield a sparse Jacobian
matrix, this situation occurs when conventional iterative methods are
applied. In their definition, Dembo formulated inexact methods such
that they are independent of the linear method used to solve for the
Newton correction and therefore are amenable to use with any linear
solver. Furthermore, they bind the convergence of the outer nonlinear
iteration to the inner linear iteration such that:
\begin{equation}
  ||\ve{J}(\ve{u}^k)\delta \ve{u}^k + \ve{F}(\ve{u}^k)|| \leq \eta^k
  ||\ve{F}(\ve{u}^k)||\:,
  \label{eq:inexact_newton_forcing}
\end{equation}
where $\eta^k \in [0,1)$ is defined as the \textit{forcing term} at
  the $k$ iterate. Eq~(\ref{eq:inexact_newton_forcing}) then states
  that the residual generated by the linear solver is bound by the
  nonlinear residual and how tightly it is bound is defined by the
  forcing term. This is useful in that we can vary how tightly coupled
  the convergence of the linear iterations used to generate the Newton
  correction is to the nonlinear iteration by relaxing or tightening
  the convergence properties on the linear iterative method. As a
  result, strategies for determining the forcing term can vary
  depending on the problem type and can greatly affect the convergence
  of the method or even prohibit convergence
  \citep{eisenstat_choosing_1996}. In addition, \textit{globalization
    methods} may be used to modify the Newton correction in a more
  desire able direction such that convergence properties can be
  improved when the initial guess for $\ve{u}$ is poor
  \citep{pawlowski_globalization_2006}.

\subsection{Newton-Krylov Methods}
\label{subsec:newton_krylov_methods}
A form of inexact Newton methods, \textit{Newton-Krylov methods} are
nonlinear iterative methods that leverage a Krylov subspace method as
the linear solver for generating the Newton correction
\citep{kelley_iterative_1995}. As we investigated in
Chapter~\ref{ch:linear_problem}, Krylov methods are robust and enjoy
efficient parallel implementations on modern
architectures. Furthermore, their lack of explicit dependence on the
operator make them easier to implement than other
methods. Additionally, although many iterations can become memory
intensive due to the need to store the Krylov subspace for the
orthogonalization procedure, at each nonlinear iteration this cost is
reset as the Jacobian matrix will change due to its dependence on the
solution vector. This means that for every nonlinear iteration, a
completely new linear system is formed for generating the Newton
correction and we can modify the Krylov solver parameters accordingly
to accommodate this. In most nonlinear problems, the Jacobian operator
is generally non-symmetric and therefore either Krylov methods with
long recurrence relations that can handle non-symmetric systems must be
considered or the Newton correction system must be preconditioned such
that the operator is symmetric and short recurrence relation methods
can be potentially be used.

With many Krylov methods available, which to use with the Newton
method is dependent on many factors including convergence rates and
memory usage. Several studies have been performed to investigate this
\citep{mchugh_inexact_1993,knoll_newton-krylov_1995}. In their
numerical studies in 1995, Knoll and McHugh used the set of highly
nonlinear and stiff convection-diffusion-reaction equations to solve a
set of tokamak plasma problems with the goal of measuring solver
performance with Newton's method. They note several trade offs in using
Krylov methods with the Newton solver. The first is that the
optimization condition that results from the constraints (e.g. the
minimization of the GMRES residual over the Krylov space) can be
relaxed by restricting the size of the subspace such that only a fixed
number of subspace vectors may be maintained, thus reducing memory
requirements. We can also relax the optimization condition by instead
restarting the recurrence relation with a new set of vectors once a
certain number of vectors have been generated. The optimization
condition is maintained over that particular set of vectors, however,
Knoll and McHugh note that this ultimately slows the convergence rate
as compared to keeping all vectors as the new set of vectors is not
necessarily orthogonal to the previous set, and therefore not optimal
over the entire iteration procedure. The orthogonality condition can
be relaxed by using a recurrence relation that does not generate a
strictly orthonormal basis for the Krylov subspace such as the Lanzcos
biorthogonalization procedure, resulting in memory savings due to the
shorter Lanzcos recurrence relation.

As a comparison, Knoll and McHugh chose an Arnoldi-based GMRES with a
fixed vector basis approximately the size of the number of iterations
required to converge as the long recurrence relation solver and
conjugate gradients squared (CGS), bi-orthogonalized conjugate
gradient stabilized (Bi-CGSTAB), and transpose-free quasiminimal
residual (TFQMR) methods as Lanzcos-based short recurrence relation
solvers. All solvers were used to compute the right-preconditioned
Newton correction system. For standard implementations of Newton's
method where the Jacobian operator was explicitly formed using
difference equations, all methods exhibited roughly equivalent
iteration count performance for both the inner linear iterations and
the outer nonlinear iterations in terms of iterations required to
converge. Bi-CGSTAB typically performed the best for
implementations where the Jacobian was explicitly formed and GMRES
performing best for matrix-free implementations. However, upon
investigating the convergence of the inner iterations, it was observed
that the GMRES solver was significantly more robust, always generating
a monotonically decreasing residual as compared to the Lanzcos-based
methods which had the tendency to oscillate. Based on these results,
in all of their future work Knoll and McHugh tended to use GMRES as
the Krylov solver \citep{knoll_jacobian-free_2004}.

\subsubsection{Jacobian-Free Approximation}
\label{subsubsec:jacobian_free_approximation}
In most cases, the Jacobian is difficult to form from the difference
equations and costly to evaluate for large equation sets. For simple
nonlinear cases such as the Navier-Stokes equations, the derivatives
can be computed and coded, but due to the complexity of those
derivatives and the resulting difference equations this task can be
tedious, error prone, and must be repeated for every equation
set. Furthermore, in their 1995 work, Knoll and McHugh also noted that
a dominating part of their computation time was the evaluation of the
difference equations for building the Jacobian
\citep{knoll_newton-krylov_1995}. By recognizing that Krylov methods
only need the action of the operator on the vector instead of the
operator itself, the Jacobian can instead be approximated through
various numerical methods including a difference-based Jacobian-free
formulation. 

Jacobian-Free methods, and in particular \textit{Jacobian-Free
  Newton-Krylov} (JFNK) methods \citep{knoll_jacobian-free_2004}, rely
on forming the action of the Jacobian on a vector as required by the
Krylov solver through a forward difference scheme. In this case, the
action of the Jacobian on some vector $\ve{v}$ is given as:
\begin{equation}
  \ve{J}(\ve{u})\ve{v} = \frac{\ve{F}(\ve{u} + \epsilon \ve{v}) -
    \ve{F}(\ve{u})}{\epsilon}\:,
  \label{eq:jacobian_free_product}
\end{equation}
where $\epsilon$ is a small number typically on the order of machine
precision. Kelley \citep{kelley_iterative_1995} points out a potential
downfall of this formulation in that if the discretization error in
$\ve{F}(\ve{u})$ is on the order of the perturbation parameter
$\epsilon$, then the finite difference error from
Eq~(\ref{eq:jacobian_free_product}) pollutes the solution. In
addition, Knoll and McHugh noted that for preconditioning purposes,
part of the Jacobian must still explicitly be formed periodically and
that linear solver robustness issues were magnified by the matrix-free
approach due to the first-order approximation. This formation
frequency coupled with the numerous evaluations of the Jacobian
approximation create a situation where after so many nonlinear
iterations, it becomes cheaper to instead fully form the
Jacobian. For simple equation sets, this may only take 5-10 Newton
iterations to reach this point while over 30 may be required for
larger equations sets and therefore larger Jacobians.

\subsubsection{Automatic Differentiation for Jacobian Generation}
\label{subsubsec:automatic_differentiation}
If it is acceptable to store the actual Jacobian matrix, other methods
are available to construct it without requiring hand-coding and
evaluating derivatives, thus eliminating the associated issues. In
addition, if any additional equations are added to the system or a
higher order functional approximation is desired, it would be useful
to avoid regenerating and coding these derivatives. Becoming more
prominent in the 1990's, \textit{automatic differentiation} is a
mechanism by which the derivatives of a function can be generated
automatically by evaluating it. Automatic differentiation is built on
the concept that all functions discretely represented in a computer
are ultimately represented by elementary mathematical operations. If
the chain rule is applied to those elementary operations, then the
derivatives of those functions can be computed to the order of
accuracy of their original discretization in a completely automated
way \citep{averick_computing_1994}.

The work of Bartlett and others \citep{bartlett_automatic_2006}
extended initial Fortran-based work in the area of automatic
differentiation implementations to leverage the parametric type and
operator overloading features of C++ \citep{stroustrup_c++_1997}. They
formulate the differentiation problem from an element viewpoint by
assuming that a global Jacobian can be assembled from local element
function evaluations of $e_k : \mathbb{R}^{n_k} \rightarrow
\mathbb{R}^{m_k}$, similar to the finite element assembly procedure
as:
\begin{equation}
  \ve{J}(\ve{u}) = \sum_{i=1}^N \ve{Q}^T_i \ve{J}_k \ve{P}_i\:,
  \label{eq:fad_global_jacobian}
\end{equation}
where $\ve{J}_{k_i} = \partial e_{k_i} / \partial P_i u$ is the
$k^{th}$ element function Jacobian, $\ve{Q} \in \mathbb{R}^{n_{k_i}
  \times N}$ is a projector onto the element domain and $\ve{P} \in
\mathbb{R}^{m_{k_i} \times N}$ a projector onto the element range for
$\ve{F}(\ve{u}) \in \mathbb{R}^{N \times N}$. The Jacobian matrix for
each element will therefore have entirely local data in a dense
structure, eliminating the need for parallel communication and sparse
techniques during differentiation. Only when all local differentials
are computed does communication of the Jacobian occur through
gather/scatter operations in order to properly assembly it. Also of
benefit is the fact that element-level computations generally consist
of a smaller number of degrees of freedom, thus reducing memory
requirements during evaluation as compared to a global formulation of
the problem. Such a formulation is not limited to finite element
formulations and is amenable to any scheme where the system is
globally sparse with degrees of freedom coupled to local domains
including finite volume representations. The templating capabilities
of C++ were leveraged with the element-based evaluation and assembly
scheme as in Eq~(\ref{eq:fad_global_jacobian}) by templating element
function evaluation code on the evaluation type. If these functions
are instantiated with standard floating point types then the residual
is returned. If they are instead instantiated with the
operator-overloaded automatic differentiation types, both the residual
and Jacobian are returned.

Of interest to Bartlett, Averick, and the many others that have
researched automatic differentiation are measures of its performance
relative to hand-coded derivatives and capturing the Jacobian matrix
from matrix-free approximations. Given their element-based function
evaluation scheme, Bartlett's work varied the number of degrees of
freedom per element and compared both the floating point operation
count and CPU time for both the templated automatic differentiation
method and hand-coded derivatives for Jacobian evaluations. Although
they observed a 50\% increase in floating point operations in the
templated method over the hand-coded method, run times were observed
to be over 3 times faster for the templated method. They hypothesize
that this is due to the fact that the element-based formulation of the
templated method is causing better utilization of cache and therefore
faster data access. Furthermore, they observed linear scaling behavior
for automatic differentiation as the number of degrees of freedom per
element were increased up to a few hundred. Based on these results,
this type of automatic differentiation formulation was deemed
acceptable for use in large-scale, production physics codes.

\section{The FANM Method}
\label{sec:fanm}
In production physics codes based on nonlinear equations sets,
Newton-Krylov methods are the primary means of generating a fully
consistent solution scheme
\citep{evans_development_2006,evans_enhanced_2007,gaston_parallel_2009,godoy_parallel_2012}. Typically,
for large scale simulations these problems are memory limited due to
the subspaces generated by robust Krylov methods. Often, a matrix-free
approach is chosen to relax memory requirements over directly
generating the Jacobian matrix and facilitate the
implementation. However, as we observed in previous sections, these
matrix-free methods suffer from poorly scaled problems and the first
order error introduced by the Jacobian approximation. In addition, it
was observed that the savings induced by the matrix-free approach is
eventually amortized over a number of nonlinear iterations where it
becomes more efficient computationally to instead form the Jacobian.

In Chapter~\ref{ch:stochastic_methods}, we focused our efforts on
developing and improving Monte Carlo methods for inverting linear
systems. These methods, when used to accelerate a stationary method in
MCSA, enjoy a robust implementation and exponential convergence
rates. Further, the only storage required is that of the full linear
system including the linear operator so that we may generate
transition probabilities for the random walk sequence. Although this
requires more storage to represent the linear system than that of a
Krylov method where the operator is not required, we do not incur any
additional storage costs once the iteration sequence begins. In the
context of nonlinear problems, the Jacobian matrix that we are
required to generate for the Monte Carlo solvers may be generated at
will from the nonlinear functions in the Newton system using automatic
differentiation. Not only do we then have a simple and automated way to
generate the Jacobian, but we also enjoy a Jacobian of numerical
precision equivalent to that of our function evaluations. We therefore
propose the \textit{Forward-Automated Newton-MCSA} (FANM) method that
utilizes all of the above components. We hypothesize that such a
method will be competitive with Newton-Krylov methods not only from a
convergence and timing perspective, but also relax scaling
requirements of matrix-free methods and memory costs of both
matrix-free and fully formed Jacobian methods to allow the application
developer to solve problems of finer discretization and higher-order
functional representations while maintaining a robust and efficient
parallel implementation.

\subsection{Jacobian Storage vs. Subspace Storage and Restarts}
\label{subsec:fanm_storage}
To gauge the memory benefits of a FANM method over a Krylov-based
scheme, we must look at the storage requirements of the Jacobian
matrix as compared to the Krylov subspace vectors for sparse linear
problems. Saad's text provides us relations for both of these cases
\citep{saad_iterative_2003}. Beginning with the Jacobian storage, for
sparse problems production linear algebra library implementations
utilize \textit{compressed row storage} to efficiently store the
non-zero components of a sparse matrix while still allowing for all
standard parallel matrix computations to be performed. Per
\S~\ref{sec:parallel_krylov_methods}, recall that efficient parallel
matrix-vector multiplications rely on processors knowing which other
processors contain their neighboring matrix and vector
elements. Typically this information is represented by a graph which
must be stored along with the matrix elements themselves. To store the
elements, consider the following sparse matrix:
\[
  \ve{A} =
  \begin{bmatrix}
    2 & 0 & 8 & 0 & 0 & 0 \\ 
    4 & 5 & 0 & 1 & 0 & 0 \\ 
    0 & 2 & 1 & 0 & 1 & 0 \\ 
    0 & 0 & 3 & 7 & 0 & 2 \\ 
    0 & 0 & 0 & 4 & 9 & 0 \\
    0 & 0 & 0 & 0 & 9 & 1 \\
  \end{bmatrix}\:.
\]
The compressed form of \ve{A} would then be:
\[
  \begin{array}{lccccccccccccccc}
    \ve{A}\ values: & 2 & 8 & 4 & 5 & 1 & 2 & 1 & 1 & 3 & 7 & 2 & 4 & 9
    & 9 & 1 \\ column: & 1 & 3 & 1 & 2 & 4 & 2 & 3 & 5 & 3 & 4 & 6 & 4
    & 5 & 5 & 6 \\ row\ start: & 1 & 3 & 6 & 9 & 12 & 14 & 16 \\
  \end{array}
\]
where the first row contains the non-zero values of the matrix
$\ve{A}$ starts at, the second row contains the column index of those
values, and the third row contains the index of the A value and column
index rows that each matrix row starts at ending with the start of the
next row. For this simple example, we actually break even by storing
36 elements in the full matrix and 36 pieces of data in the compressed
format. However, for large, sparse matrices there is a significant
storage savings using the compressed format.

In his discussion on orthogonalization schemes for Krylov subspaces,
Saad provides us with space and time complexities for these
operations. To find a solution vector $\ve{x} \in \mathbb{R}^{N \times
  N}$ with $m$ Krylov iterations (and therefore $m+1$ subspace
vectors) the storage requirement is then $(m+1) N$ for most common
orthogonalization techniques. Using our simple example above (even
though we did not see any gains in storage over the full matrix
representation), if it takes 10 GMRES iterations using Arnoldi
orthogonalization to converge to a solution, then a total of 66 pieces
of data are required to be stored instead of 36 for the explicitly
formed matrix case (we do not consider graph storage here for
comparison as both methods will require the graph for parallel
operations). For larger sparse matrices, this disparity in memory
requirements will be even greater, especially for ill-conditioned
systems that require many GMRES iterations to converge.

\subsection{Parallel FANM Implementation}
\label{subsec:parallel_fanm}
Like the parallelization of the MCSA method described in
\S~\ref{sec:parallel_stochastic_methods}, a parallel FANM method
relies on a basic set of parallel matrix-vector operations as well as
the global residual and Jacobian assembly procedure described in
\S~\ref{subsubsec:automatic_differentiation}. Consider the Newton
iteration scheme in Eq~(\ref{eq:newton_iteration}). We must first
assemble the linear system in parallel through the element-wise
function evaluations to generate both the global Jacobian operator and
the global residual vector on the right hand side. Per Bartlett's
work, efficient and automated parallel mechanisms are available to do
this through a sequence of scatter/gather operations. With these tools
available for residual and Jacobian generation, the remainder of the
parallel procedure is simple, with the linear Newton correction system
solved using the parallel MCSA method as previously described and the
Newton correction applied to the previous iterate's solution through a
parallel vector update operation.

As Newton methods are formulated independent of the inner linear
solver generating the Newton corrections, the actual performance of
the nonlinear iterations using MCSA is expected to be similar to that
of traditional Newton-Krylov methods. Furthermore, we expect to
achieve numerically identical answers with a Newton-MCSA method as
other Newton methods and we should indeed verify this. The parallel
performance of such a method will inherently be bound to the parallel
Monte Carlo implementation of the linear solver as the parallel
operations at the nonlinear iteration level are identical to those
that you would perform with a Newton-Krylov method. Matrix-free
formulations will have different parallel performance than these
methods, and therefore we should compare FANM performance to JFNK-based
schemes. More important than performance in this situation is the
reduced memory pressure that a FANM implementation provides, as
discussed in \S~\ref{subsec:fanm_storage}, because a FANM method will
not generate a subspace in the linear solver and compressed storage
for sparse matrices are utilized, we expect significant memory savings
over Newton-Krylov methods. We must measure the memory utilization of
both of these methods in order to quantify their differences and
provide additional analysis of the FANM method's merits, or lack
thereof.

%%---------------------------------------------------------------------------%%
\section{Monte Carlo Solution Methods for the Navier-Stokes Equations}
\label{sec:mc_ns_solutions}

\subsection{FANM Verification}
\label{subsec:fanm_verification}
To verify the FANM method for nonlinear problems, we choose benchmark
solutions for the 2-dimensional, steady, incompressible Navier-Stokes
equations on a rectilinear grid in much the same way as Shadid and
Pawlowski's work on Newton-Krylov methods for the solution of these
equations \citep{shadid_inexact_1997,pawlowski_globalization_2006}. We
define these equations as follows:
\begin{subequations}
  \begin{gather}
    \rho \ve{u} \cdot \nabla \ve{u} - \nabla \cdot \ve{T} - \rho
    \ve{g} = \ve{0}
    \label{eq:ns_momentum}\\
    \nabla \cdot \ve{u} = 0
    \label{eq:ns_continuity}\\
    \rho C_p \ve{u} \cdot \nabla T + \nabla \cdot \ve{q} = 0\:,
    \label{eq:ns_energy}
  \end{gather}
  \label{eq:navier_stokes}
\end{subequations}
where $\rho$ is the fluid density, $\ve{u}$ is the fluid velocity,
$\ve{g}$ gravity, $C_p$ the specific heat capacity at constant
pressure of the fluid and $T$ the temperature of the
fluid. Eq~(\ref{eq:ns_momentum}) provides momentum transport,
Eq~(\ref{eq:ns_continuity}) provides the mass balance, and
Eq~(\ref{eq:ns_energy}) provides energy transport with viscous
dissipation effects neglected. In addition, we close the system with
the following equations:
\begin{subequations}
  \begin{gather}
    \ve{T} = -P \ve{I} + \mu[\nabla \ve{u} + \nabla \ve{u}^T]
    \label{eq:ns_stress_tensor}\\
    \ve{q} = - k \nabla T\:,
    \label{eq:ns_heat_flux}
  \end{gather}
  \label{eq:ns_closure}
\end{subequations}
where $\ve{T}$ is the stress tensor, $P$ is the hydrodynamic pressure,
$\mu$ is the dynamic viscosity of the fluid, $\ve{q}$ is the heat flux
in the fluid, and $k$ is the thermal conductivity of the fluid. This
set of strongly coupled equations possesses both the nonlinearities
and asymmetries that we are seeking for qualification of the FANM
method. Further, physical parameters within these equations can be
tuned to enhance the nonlinearities. We will then apply these
equations to the following three standard benchmark problems.

\subsubsection{Thermal Convection Cavity Problem}
\label{subsubsec:natural_convection_cavity}
In 1983 a benchmark solution for the natural convection of air in a
square cavity was published \citep{de_vahl_davis_natural_1983} as
shown in Figure~\ref{fig:natural_convection_cavity} for the solution
of the energy, mass, and momentum equations.
\begin{figure}[t!]
  \begin{center}
    \scalebox{1.5}{
      \input{chapters/navier_stokes_solutions/natural_convection_cavity.pdftex_t} }
  \end{center}
  \caption{\textbf{Problem setup for the natural convection cavity
      benchmark.} \textit{Dirichlet conditions are set for the
      temperature on the left and right while Neumann conditions are
      set on the top and bottom of the Cartesian grid. The temperature
      gradients will cause buoyancy-driven flow. Zero velocity
      Dirichlet conditions are set on each boundary. No thermal source
      was present.}}
  \label{fig:natural_convection_cavity}
\end{figure}
In this problem, a rectilinear grid is applied to the unit square. No
heat flow is allowed out of the top and bottom of the square with a
zero Neumann condition specified. Buoyancy driven flow is generated by
the temperature gradient from the cold and hot Dirichlet conditions on
the left and right boundaries of the box. By adjusting the Rayleigh
number of the fluid (and therefore adjusting the ratio of convective
to conductive heat transfer), we can adjust the influence of the
nonlinear convection term in Eq~(\ref{eq:ns_momentum}). In Shadid's
work, Rayleigh numbers of up to \sn{1}{6} were used for this benchmark
on a $100 \times 100$ square mesh grid.

\subsubsection{Lid Driven Cavity Problem}
\label{subsubsec:lid_driven_cavity}
As an extension of the convection problem, the second benchmark
problem given by Ghia \citep{ghia_high-re_1982} adds a driver for the
flow to introduce higher Reynolds numbers into the system, providing
more inertial force to overcome the viscous forces in the fluid. The
setup for this problem is equally simple, containing only the
Dirichlet conditions as given in Figure~\ref{fig:lid_driven_cavity}
and is only applied to the mass and momentum equations on the unit
square.
\begin{figure}[t!]
  \begin{center}
    \scalebox{1.5}{
      \input{chapters/navier_stokes_solutions/lid_driven_cavity.pdftex_t} }
  \end{center}
  \caption{\textbf{Problem setup for the lid driven cavity benchmark.}
    \textit{Dirichlet conditions of zero are set for the velocity on
      the left and right and bottom while the Dirichlet condition set
      on the top provides a driving force on the fluid.}}
  \label{fig:lid_driven_cavity}
\end{figure}
The top boundary condition will provide a driver for the flow and its
variation will in turn vary the Reynolds number of the fluid. An
increased velocity will generate more inertial forces in the fluid,
which will overcome the viscous forces and again increase the
influence of the nonlinear terms in Eq~(\ref{eq:ns_momentum}). Shadid
also used a $100 \times 100$ grid for this benchmark problem with
Reynolds numbers up to \sn{1}{4}.

\subsubsection{Backward-Facing Step Problem}
\label{subsubsec:backward_facing_step}
The third benchmark was generated by Gartling in 1990 and consists of
both flow over a backward step and an outflow boundary condition
\citep{gartling_test_1990}. Using the mass and momentum equations
while neglecting the energy equation, this problem utilizes a longer
domain with a 1/30 aspect ratio with the boundary conditions as shown
in Figure~\ref{fig:backward_facing_step}.
\begin{figure}[t!]
  \begin{center}
    \scalebox{1.3}{
      \input{chapters/navier_stokes_solutions/backward_facing_step.pdftex_t} }
  \end{center}
  \caption{\textbf{Problem setup for the backward facing step
      benchmark.} \textit{Zero velocity boundary conditions are
      applied at the top and bottom of the domain while the outflow
      boundary condition on the right boundary is represented by zero
      stress tensor components in the direction of the flow. For the
      inlet conditions, the left boundary is split such that the top
      half has a fully formed parabolic flow profile and the bottom
      half has a zero velocity condition, simulating flow over a
      step.}}
  \label{fig:backward_facing_step}
\end{figure}
In this problem, the inflow condition is specified by a fully-formed
parabolic flow profile over a zero velocity boundary representing a
step. The flow over this step will generate a recirculating backflow
under the inlet flow towards the step. As in the lid driven cavity
problem, the nonlinear behavior of this benchmark and the difficulty
in obtaining a solution is dictated by the Reynolds number of the
fluid. In Shadid's work, a $20 \times 400$ non-square rectilinear grid
was used to discretize the domain with Reynolds number up to
\sn{5}{2}.
