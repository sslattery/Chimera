\chapter{Conventional Solution Methods for Nonlinear Systems\ }
\label{chap:nonlinear_background}

\section{Newton-Krylov Methods\ }
\label{sec:newton_krylov_methods}
A form of inexact Newton methods, \textit{Newton-Krylov methods} are
nonlinear iterative methods that leverage a Krylov subspace method as
the linear solver for generating the Newton correction
\citep{kelley_iterative_1995}. As we investigated in
Chapter~\ref{ch:linear_problem}, Krylov methods are robust and enjoy
efficient parallel implementations on modern
architectures. Furthermore, their lack of explicit dependence on the
operator make them easier to implement than other
methods. Additionally, although many iterations can become memory
intensive due to the need to store the Krylov subspace for the
orthogonalization procedure, at each nonlinear iteration this cost is
reset as the Jacobian matrix will change due to its dependence on the
solution vector. This means that for every nonlinear iteration, a
completely new linear system is formed for generating the Newton
correction and we can modify the Krylov solver parameters accordingly
to accommodate this. In most nonlinear problems, the Jacobian operator
is generally non-symmetric and therefore either Krylov methods with
long recurrence relations that can handle non-symmetric systems must be
considered or the Newton correction system must be preconditioned such
that the operator is symmetric and short recurrence relation methods
can be potentially be used.

With many Krylov methods available, which to use with the Newton
method is dependent on many factors including convergence rates and
memory usage. Several studies have been performed to investigate this
\citep{mchugh_inexact_1993,knoll_newton-krylov_1995}. In their
numerical studies in 1995, Knoll and McHugh used the set of highly
nonlinear and stiff convection-diffusion-reaction equations to solve a
set of tokamak plasma problems with the goal of measuring solver
performance with Newton's method. They note several trade offs in using
Krylov methods with the Newton solver. The first is that the
optimization condition that results from the constraints (e.g. the
minimization of the GMRES residual over the Krylov space) can be
relaxed by restricting the size of the subspace such that only a fixed
number of subspace vectors may be maintained, thus reducing memory
requirements. We can also relax the optimization condition by instead
restarting the recurrence relation with a new set of vectors once a
certain number of vectors have been generated. The optimization
condition is maintained over that particular set of vectors, however,
Knoll and McHugh note that this ultimately slows the convergence rate
as compared to keeping all vectors as the new set of vectors is not
necessarily orthogonal to the previous set, and therefore not optimal
over the entire iteration procedure. The orthogonality condition can
be relaxed by using a recurrence relation that does not generate a
strictly orthonormal basis for the Krylov subspace such as the Lanzcos
biorthogonalization procedure, resulting in memory savings due to the
shorter Lanzcos recurrence relation.

As a comparison, Knoll and McHugh chose an Arnoldi-based GMRES with a
fixed vector basis approximately the size of the number of iterations
required to converge as the long recurrence relation solver and
conjugate gradients squared (CGS), bi-orthogonalized conjugate
gradient stabilized (Bi-CGSTAB), and transpose-free quasiminimal
residual (TFQMR) methods as Lanzcos-based short recurrence relation
solvers. All solvers were used to compute the right-preconditioned
Newton correction system. For standard implementations of Newton's
method where the Jacobian operator was explicitly formed using
difference equations, all methods exhibited roughly equivalent
iteration count performance for both the inner linear iterations and
the outer nonlinear iterations in terms of iterations required to
converge. Bi-CGSTAB typically performed the best for
implementations where the Jacobian was explicitly formed and GMRES
performing best for matrix-free implementations. However, upon
investigating the convergence of the inner iterations, it was observed
that the GMRES solver was significantly more robust, always generating
a monotonically decreasing residual as compared to the Lanzcos-based
methods which had the tendency to oscillate. Based on these results,
in all of their future work Knoll and McHugh tended to use GMRES as
the Krylov solver \citep{knoll_jacobian-free_2004}.

\subsection{Jacobian-Free Approximation}
\label{subsec:jacobian_free_approximation}
In most cases, the Jacobian is difficult to form from the difference
equations and costly to evaluate for large equation sets. For simple
nonlinear cases such as the Navier-Stokes equations, the derivatives
can be computed and coded, but due to the complexity of those
derivatives and the resulting difference equations this task can be
tedious, error prone, and must be repeated for every equation
set. Furthermore, in their 1995 work, Knoll and McHugh also noted that
a dominating part of their computation time was the evaluation of the
difference equations for building the Jacobian
\citep{knoll_newton-krylov_1995}. By recognizing that Krylov methods
only need the action of the operator on the vector instead of the
operator itself, the Jacobian can instead be approximated through
various numerical methods including a difference-based Jacobian-free
formulation. 

Jacobian-Free methods, and in particular \textit{Jacobian-Free
  Newton-Krylov} (JFNK) methods \citep{knoll_jacobian-free_2004}, rely
on forming the action of the Jacobian on a vector as required by the
Krylov solver through a forward difference scheme. In this case, the
action of the Jacobian on some vector $\ve{v}$ is given as:
\begin{equation}
  \ve{J}(\ve{u})\ve{v} = \frac{\ve{F}(\ve{u} + \epsilon \ve{v}) -
    \ve{F}(\ve{u})}{\epsilon}\:,
  \label{eq:jacobian_free_product}
\end{equation}
where $\epsilon$ is a small number typically on the order of machine
precision. Kelley \citep{kelley_iterative_1995} points out a potential
downfall of this formulation in that if the discretization error in
$\ve{F}(\ve{u})$ is on the order of the perturbation parameter
$\epsilon$, then the finite difference error from
Eq~(\ref{eq:jacobian_free_product}) pollutes the solution. In
addition, Knoll and McHugh noted that for preconditioning purposes,
part of the Jacobian must still explicitly be formed periodically and
that linear solver robustness issues were magnified by the matrix-free
approach due to the first-order approximation. This formation
frequency coupled with the numerous evaluations of the Jacobian
approximation create a situation where after so many nonlinear
iterations, it becomes cheaper to instead fully form the
Jacobian. For simple equation sets, this may only take 5-10 Newton
iterations to reach this point while over 30 may be required for
larger equations sets and therefore larger Jacobians.

\subsection{Automatic Differentiation for Jacobian Generation}
\label{subsec:automatic_differentiation}
If it is acceptable to store the actual Jacobian matrix, other methods
are available to construct it without requiring hand-coding and
evaluating derivatives, thus eliminating the associated issues. In
addition, if any additional equations are added to the system or a
higher order functional approximation is desired, it would be useful
to avoid regenerating and coding these derivatives. Becoming more
prominent in the 1990's, \textit{automatic differentiation} is a
mechanism by which the derivatives of a function can be generated
automatically by evaluating it. Automatic differentiation is built on
the concept that all functions discretely represented in a computer
are ultimately represented by elementary mathematical operations. If
the chain rule is applied to those elementary operations, then the
derivatives of those functions can be computed to the order of
accuracy of their original discretization in a completely automated
way \citep{averick_computing_1994}.

The work of Bartlett and others \citep{bartlett_automatic_2006}
extended initial Fortran-based work in the area of automatic
differentiation implementations to leverage the parametric type and
operator overloading features of C++ \citep{stroustrup_c++_1997}. They
formulate the differentiation problem from an element viewpoint by
assuming that a global Jacobian can be assembled from local element
function evaluations of $e_k : \mathbb{R}^{n_k} \rightarrow
\mathbb{R}^{m_k}$, similar to the finite element assembly procedure
as:
\begin{equation}
  \ve{J}(\ve{u}) = \sum_{i=1}^N \ve{Q}^T_i \ve{J}_k \ve{P}_i\:,
  \label{eq:fad_global_jacobian}
\end{equation}
where $\ve{J}_{k_i} = \partial e_{k_i} / \partial P_i u$ is the
$k^{th}$ element function Jacobian, $\ve{Q} \in \mathbb{R}^{n_{k_i}
  \times N}$ is a projector onto the element domain and $\ve{P} \in
\mathbb{R}^{m_{k_i} \times N}$ a projector onto the element range for
$\ve{F}(\ve{u}) \in \mathbb{R}^{N \times N}$. The Jacobian matrix for
each element will therefore have entirely local data in a dense
structure, eliminating the need for parallel communication and sparse
techniques during differentiation. Only when all local differentials
are computed does communication of the Jacobian occur through
gather/scatter operations in order to properly assembly it. Also of
benefit is the fact that element-level computations generally consist
of a smaller number of degrees of freedom, thus reducing memory
requirements during evaluation as compared to a global formulation of
the problem. Such a formulation is not limited to finite element
formulations and is amenable to any scheme where the system is
globally sparse with degrees of freedom coupled to local domains
including finite volume representations. The templating capabilities
of C++ were leveraged with the element-based evaluation and assembly
scheme as in Eq~(\ref{eq:fad_global_jacobian}) by templating element
function evaluation code on the evaluation type. If these functions
are instantiated with standard floating point types then the residual
is returned. If they are instead instantiated with the
operator-overloaded automatic differentiation types, both the residual
and Jacobian are returned.

Of interest to Bartlett, Averick, and the many others that have
researched automatic differentiation are measures of its performance
relative to hand-coded derivatives and capturing the Jacobian matrix
from matrix-free approximations. Given their element-based function
evaluation scheme, Bartlett's work varied the number of degrees of
freedom per element and compared both the floating point operation
count and CPU time for both the templated automatic differentiation
method and hand-coded derivatives for Jacobian evaluations. Although
they observed a 50\% increase in floating point operations in the
templated method over the hand-coded method, run times were observed
to be over 3 times faster for the templated method. They hypothesize
that this is due to the fact that the element-based formulation of the
templated method is causing better utilization of cache and therefore
faster data access. Furthermore, they observed linear scaling behavior
for automatic differentiation as the number of degrees of freedom per
element were increased up to a few hundred. Based on these results,
this type of automatic differentiation formulation was deemed
acceptable for use in large-scale, production physics codes.
